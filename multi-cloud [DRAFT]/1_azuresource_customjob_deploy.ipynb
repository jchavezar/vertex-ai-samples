{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93418941-0ac0-4953-a0d7-d1bb4b72434c",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f947c01b-dbef-4a99-845b-44fb896f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "TRAINING_IMAGE_URI = f'gcr.io/{PROJECT_ID}/demos-train-azure:latest'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'\n",
    "DATASET_NAME = 'fraud_detection.csv'\n",
    "MODEL_DIR = 'gs://vtx-models/azure/hpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc827c5-cafa-403f-a769-db6c3423d3ec",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539aaeec-025a-413b-85e2-a6c84b2aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"1_custom_train_job\"):\n",
    "    os.makedirs(\"1_custom_train_job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a2f7d-e852-428c-9488-146ac72fa66a",
   "metadata": {},
   "source": [
    "## Create Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbcd2d4-2e66-4979-82ba-1f19cffb190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/main.py\n",
    "\n",
    "# Extracting information from Azure blob storage\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "FILENAME = os.environ['FILE_NAME']\n",
    "connect_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\n",
    "print(FILENAME)\n",
    "print(connect_str)\n",
    "    \n",
    "# Create the BlobServiceClient object\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "blob_client_instance = blob_service_client.get_blob_client('vertex', FILENAME, snapshot=None)\n",
    "    \n",
    "with open(FILENAME, 'wb') as my_blob:\n",
    "    blob_data = blob_client_instance.download_blob()\n",
    "    blob_data.readinto(my_blob)\n",
    "    \n",
    "df = pd.read_csv(FILENAME)\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=1)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "X_train = train_df.iloc[:,:-1]\n",
    "X_test = test_df.iloc[:,:-1]\n",
    "y_train = train_df['Class'].astype(np.float32)\n",
    "y_test = test_df['Class'].astype(np.float32)\n",
    "\n",
    "# Standarization\n",
    "\n",
    "X_train_norm = (X_train-X_train.mean())/X_train.std()\n",
    "X_test_norm = (X_test-X_test.mean())/X_test.std()\n",
    "        \n",
    "# Model\n",
    "\n",
    "def create_model(my_learning_rate, ds_length):\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Dense(16, activation='relu', input_shape=[ds_length]),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "        \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(x, y, model, epochs,\n",
    "                batch_size=None, shuffle=True):\n",
    "    history = model.fit(x=x, y=y, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "ds_length = len(X_train_norm.keys())\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, ds_length)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(X_train_norm, y_train, my_model, epochs, \n",
    "                           batch_size)\n",
    "\n",
    "my_model.save(os.environ['AIP_MODEL_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e667f-004d-4bb3-9eec-18a0bd642d56",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5839f34a-7d36-470a-ba84-5f1b41dfac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/Dockerfile\n",
    "\n",
    "FROM tensorflow/tensorflow\n",
    "\n",
    "RUN pip install azure-storage-blob azure-identity pandas\n",
    "\n",
    "COPY main.py /main.py\n",
    "\n",
    "CMD [\"python\", \"/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b86159-88db-4a31-a11d-a09b97311114",
   "metadata": {},
   "source": [
    "## Create Docker Image with CloudBuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f82034e-165c-4d37-939a-bd6c76c5a109",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.9 KiB before compression.\n",
      "Uploading tarball of [1_custom_train_job/.] to [gs://jchavezar-demo_cloudbuild/source/1672238210.184197-7b5495b7ef7d408495d97698bddc063c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/e75ba77d-a8dc-456b-b58e-13d02db7d027].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/e75ba77d-a8dc-456b-b58e-13d02db7d027?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e75ba77d-a8dc-456b-b58e-13d02db7d027\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1672238210.184197-7b5495b7ef7d408495d97698bddc063c.tgz#1672238210393469\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1672238210.184197-7b5495b7ef7d408495d97698bddc063c.tgz#1672238210393469...\n",
      "/ [1 files][  1.4 KiB/  1.4 KiB]                                                \n",
      "Operation completed over 1 objects/1.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  5.632kB\n",
      "Step 1/4 : FROM tensorflow/tensorflow\n",
      "latest: Pulling from tensorflow/tensorflow\n",
      "eaead16dc43b: Pulling fs layer\n",
      "83bb66f4018d: Pulling fs layer\n",
      "a9d243755566: Pulling fs layer\n",
      "38d8f03945ed: Pulling fs layer\n",
      "0e62e78ef96b: Pulling fs layer\n",
      "311604e9ab28: Pulling fs layer\n",
      "584c5149ce07: Pulling fs layer\n",
      "3b5c5b94152b: Pulling fs layer\n",
      "0e62e78ef96b: Waiting\n",
      "311604e9ab28: Waiting\n",
      "584c5149ce07: Waiting\n",
      "3b5c5b94152b: Waiting\n",
      "38d8f03945ed: Waiting\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "83bb66f4018d: Verifying Checksum\n",
      "83bb66f4018d: Download complete\n",
      "0e62e78ef96b: Download complete\n",
      "38d8f03945ed: Verifying Checksum\n",
      "38d8f03945ed: Download complete\n",
      "584c5149ce07: Verifying Checksum\n",
      "584c5149ce07: Download complete\n",
      "3b5c5b94152b: Verifying Checksum\n",
      "3b5c5b94152b: Download complete\n",
      "a9d243755566: Verifying Checksum\n",
      "a9d243755566: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "311604e9ab28: Verifying Checksum\n",
      "311604e9ab28: Download complete\n",
      "83bb66f4018d: Pull complete\n",
      "a9d243755566: Pull complete\n",
      "38d8f03945ed: Pull complete\n",
      "0e62e78ef96b: Pull complete\n",
      "311604e9ab28: Pull complete\n",
      "584c5149ce07: Pull complete\n",
      "3b5c5b94152b: Pull complete\n",
      "Digest: sha256:eea5989852623037f354c49404b66761467516b79ab7af26e643b5ac7382c53f\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:latest\n",
      " ---> 82f1344ecd48\n",
      "Step 2/4 : RUN pip install azure-storage-blob azure-identity pandas\n",
      " ---> Running in 57afac16ee64\n",
      "Collecting azure-storage-blob\n",
      "  Downloading azure_storage_blob-12.14.1-py3-none-any.whl (383 kB)\n",
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.12.0-py3-none-any.whl (135 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Collecting msrest>=0.7.1\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "Collecting azure-core<2.0.0,>=1.24.2\n",
      "  Downloading azure_core-1.26.1-py3-none-any.whl (172 kB)\n",
      "Collecting cryptography>=2.1.4\n",
      "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "Collecting msal-extensions<2.0.0,>=0.3.0\n",
      "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting msal<2.0.0,>=1.12.0\n",
      "  Downloading msal-1.20.0-py2.py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from azure-identity) (1.16.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7-py2.py3-none-any.whl (499 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.4)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.7.1->azure-storage-blob) (1.3.1)\n",
      "Requirement already satisfied: requests~=2.16 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.7.1->azure-storage-blob) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.7.1->azure-storage-blob) (2022.9.24)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-core<2.0.0,>=1.24.2->azure-storage-blob) (4.4.0)\n",
      "Collecting cffi>=1.12\n",
      "  Downloading cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Collecting portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\"\n",
      "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.7.1->azure-storage-blob) (3.2.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.7.1->azure-storage-blob) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.7.1->azure-storage-blob) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.16->msrest>=0.7.1->azure-storage-blob) (2.1.1)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: azure-core, isodate, msrest, pycparser, cffi, cryptography, azure-storage-blob, portalocker, PyJWT, msal, msal-extensions, azure-identity, pytz, python-dateutil, pandas\n",
      "Successfully installed PyJWT-2.6.0 azure-core-1.26.1 azure-identity-1.12.0 azure-storage-blob-12.14.1 cffi-1.15.1 cryptography-38.0.4 isodate-0.6.1 msal-1.20.0 msal-extensions-1.0.0 msrest-0.7.1 pandas-1.5.2 portalocker-2.6.0 pycparser-2.21 python-dateutil-2.8.2 pytz-2022.7\n",
      "\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 57afac16ee64\n",
      " ---> 4661b4c7d3b6\n",
      "Step 3/4 : COPY main.py /main.py\n",
      " ---> ee6e94202887\n",
      "Step 4/4 : CMD [\"python\", \"/main.py\"]\n",
      " ---> Running in bc8e5f2a2e7f\n",
      "Removing intermediate container bc8e5f2a2e7f\n",
      " ---> d8485e0d6665\n",
      "Successfully built d8485e0d6665\n",
      "Successfully tagged gcr.io/jchavezar-demo/demos-train-azure:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jchavezar-demo/demos-train-azure:latest\n",
      "The push refers to repository [gcr.io/jchavezar-demo/demos-train-azure]\n",
      "92b80ef52ec0: Preparing\n",
      "24d3aac67210: Preparing\n",
      "d25f7660e6c7: Preparing\n",
      "582a80795eda: Preparing\n",
      "e552716cf5da: Preparing\n",
      "cf3e28d7ced8: Preparing\n",
      "1c75a2767473: Preparing\n",
      "55d30f8f8b35: Preparing\n",
      "c5429f2feab6: Preparing\n",
      "f4462d5b2da2: Preparing\n",
      "cf3e28d7ced8: Waiting\n",
      "55d30f8f8b35: Waiting\n",
      "c5429f2feab6: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "1c75a2767473: Waiting\n",
      "d25f7660e6c7: Layer already exists\n",
      "582a80795eda: Layer already exists\n",
      "e552716cf5da: Layer already exists\n",
      "1c75a2767473: Layer already exists\n",
      "cf3e28d7ced8: Layer already exists\n",
      "55d30f8f8b35: Layer already exists\n",
      "c5429f2feab6: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "92b80ef52ec0: Pushed\n",
      "24d3aac67210: Pushed\n",
      "latest: digest: sha256:d243a223c1ea2e4b582c9aef6ba029d04126b3ea684eae884d7cc735df624ecc size: 2420\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                             STATUS\n",
      "e75ba77d-a8dc-456b-b58e-13d02db7d027  2022-12-28T14:36:50+00:00  1M4S      gs://jchavezar-demo_cloudbuild/source/1672238210.184197-7b5495b7ef7d408495d97698bddc063c.tgz  gcr.io/jchavezar-demo/demos-train-azure (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAINING_IMAGE_URI 1_custom_train_job/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2de95d-2a56-4320-9f88-472a99770954",
   "metadata": {},
   "source": [
    "## Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0045c865-ef6e-42d4-80a9-d1dfa327f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import env\n",
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machineSpec': {\n",
    "            'machineType': 'n1-standard-4',\n",
    "    },\n",
    "        'replicaCount': 1,\n",
    "        'containerSpec': {\n",
    "            'image_uri': TRAINING_IMAGE_URI,\n",
    "            'env': [\n",
    "                {\n",
    "                    'name': 'FILE_NAME',\n",
    "                    'value': DATASET_NAME\n",
    "                },\n",
    "                {\n",
    "                    'name': 'AZURE_STORAGE_CONNECTION_STRING',\n",
    "                    'value': env.AZURE_STORAGE_CONNECTION_STRING\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@pipeline(name='azure-gcp-test')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    model_dir: str,\n",
    "    serving_image_uri: str\n",
    "):\n",
    "    custom_train_task = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name='custom_train_task',\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "        base_output_directory=model_dir\n",
    "    )\n",
    "    \n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=f'tf-kfp-prebuilt-model-upload-job',\n",
    "        artifact_uri=f'{model_dir}/model',\n",
    "        serving_container_image_uri=serving_image_uri,\n",
    "    ).after(custom_train_task)\n",
    "    \n",
    "    create_endpoint_task = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name='fraud_det'\n",
    "    ).after(model_upload_task)\n",
    "    \n",
    "    model_deploy_task = ModelDeployOp(\n",
    "        endpoint=create_endpoint_task.outputs[\"endpoint\"],\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_machine_type='n1-standard-4'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a94eb7-2b34-4ad1-88e7-f8ffd290e032",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236b2dff-6a3e-4d7a-9c27-6e714819cf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='azure_gcp_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea69d87-82fb-4a15-b856-29655e155874",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1d7eb3-d109-4867-89f7-3c806297211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/azure-gcp-test-20221228143801\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/azure-gcp-test-20221228143801')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/azure-gcp-test-20221228143801?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name='customjob-azure-gcp',\n",
    "    template_path='azure_gcp_test.json',\n",
    "    pipeline_root='gs://vtx-path-root',\n",
    "    parameter_values={\n",
    "        'project_id': 'jchavezar-demo',\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'serving_image_uri': SERVING_CONTAINER_IMAGE_URI\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d708276e-cb5c-4b56-a7ae-b99e4b049b55",
   "metadata": {},
   "source": [
    "![azure](../images/azure-pipe.png)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tf",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
