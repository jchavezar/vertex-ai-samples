{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e559f1-467e-4d72-a118-1065a718cdeb",
   "metadata": {},
   "source": [
    "# Vertex Tabular Binary Classification with .CustomJob().from_local_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe67ba-1a9e-459a-80da-d0d8e904c1e1",
   "metadata": {},
   "source": [
    "**Use case: predict if a customer will buy on return visit.**\n",
    "\n",
    "The ecommerce dataset has different training features:\n",
    "- latest_ecommerce_progress\n",
    "- bounces\n",
    "- time_on_site\n",
    "- pageviews\n",
    "- source\n",
    "- medium\n",
    "- channel_grouping\n",
    "- device_category\n",
    "- country\n",
    "\n",
    "The label: will_buy_on_return_visit\n",
    "\n",
    "Data is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61df73e-7eec-495d-aea7-bd7fb908c888",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4520f22-e76a-40ed-a86a-92cc2f7bd3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "DATASET_URI = 'gs://vtx-datasets-public/ecommerce/datasets.csv'\n",
    "MODEL_URI = 'gs://vtx-models/pytorch/ecommerce'\n",
    "MODEL_DISPLAY_NAME = 'pytorch-ecommerce'\n",
    "STAGING_URI = 'gs://vtx-staging/pytorch/ecommerce/'\n",
    "TRAIN_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-11:latest'\n",
    "PREDICTION_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6de79d-1e07-4b1f-89f3-af62fb4791a6",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7e95b-72e6-4b32-a827-5c40247f0d3e",
   "metadata": {},
   "source": [
    "```\n",
    "source\n",
    "     └─── trainer\n",
    "          |  train.py\n",
    "          |\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fa50e3-a932-493e-9c0b-8e2b7c0249ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr source\n",
    "!mkdir -p source/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa183da4-3d69-4176-8d10-78855ca2a8f6",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Below we have the code for the training, it was made with PyTorch by building a neural network with these components:\n",
    "\n",
    "- 2 types of features set: categorical and numerical.\n",
    "- Shape detection of embedding layer for categorical.\n",
    "- Drouput to avoid overfit during the training.\n",
    "- Batch Normalization to standarize the data.\n",
    "- 1 input layer, shape: 114x32: \n",
    "  - 114 is the number of total features (categorical and numerical) after the embedding.\n",
    "  - 32 is the number of the neurons.\n",
    "- Activation function applied to the last input layer to fix non-linearity.\n",
    "- 1 output layer, shape: 32x2.\n",
    "\n",
    "The following diagram shows the neural netowkr with steps ordered used during the Model building class: ShelterOutcomeModel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb9b09e-ff79-48c1-a05b-97d3df6716bc",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/04-pytorch-nn.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f41f6a6-0a2f-47ae-ae14-1d13724ccac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile source/trainer/train.py\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as torch_optim\n",
    "from google.cloud import storage\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    '--dataset_uri',\n",
    "    type = str,\n",
    "    help = 'Dataset uri in the format gs://[BUCKET]/*suffix*/file_name.extension')\n",
    "parser.add_argument(\n",
    "    '--project',\n",
    "    type = str,\n",
    "    help = 'This is the tenant or the Google Cloud project id name')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "def preprocessing_data(df):\n",
    "    target = 'will_buy_on_return_visit'\n",
    "    cat_columns = [i for i in df.columns if df[i].dtypes == 'object']\n",
    "    num_columns = [i for i in df.columns if df[i].dtypes == 'int64' or df[i].dtypes == 'float']\n",
    "    num_columns.remove(target)\n",
    "\n",
    "    cat_train_df = df[cat_columns]\n",
    "    num_train_df = df[num_columns]\n",
    "    label = df[target].to_numpy()\n",
    "    \n",
    "    labelencoder = defaultdict(LabelEncoder)\n",
    "    cat_train_df[cat_columns] = cat_train_df[cat_columns].apply(lambda x: labelencoder[x.name].fit_transform(x))\n",
    "    cat_train_df[cat_columns] = cat_train_df[cat_columns].astype('category')\n",
    "    \n",
    "    train_df = pd.concat([cat_train_df,num_train_df], axis=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_df, label, test_size=0.10, random_state=0)\n",
    "    \n",
    "    ## Numerical columns standarization\n",
    "    scaler = StandardScaler()\n",
    "    X_train[num_columns] = scaler.fit_transform(X_train[num_columns])\n",
    "    X_val[num_columns] = scaler.transform(X_val[num_columns])\n",
    "    \n",
    "    # Categorical Embedding\n",
    "    embedded_cols = {n: len(col.cat.categories) for n,col in X_train[cat_columns].items() if len(col.cat.categories) > 2}\n",
    "    embedded_col_names = embedded_cols.keys()\n",
    "    embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "    embedding_sizes = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "    pickle.dump(labelencoder, open('label.pkl', 'wb'))\n",
    "    pickle.dump(scaler, open('std_scaler.pkl', 'wb'))\n",
    "    pickle.dump(embedding_sizes, open('emb.pkl', 'wb'))\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, embedded_col_names, embedding_sizes\n",
    "\n",
    "df = pd.read_csv(args.dataset_uri)\n",
    "X_train, X_val, y_train, y_val, embedded_col_names, embedding_sizes = preprocessing_data(df)\n",
    "\n",
    "## PyTorch Dataset\n",
    "\n",
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "    \n",
    "## Train and Valid datasets\n",
    "\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "## CPU or GPU selection\n",
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_default_device()\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = embedding_sizes\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 2)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "model = ShelterOutcomeModel(embedding_sizes, 4)\n",
    "to_device(model, device)\n",
    "\n",
    "## Define Optimizer\n",
    "\n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "## Train Model\n",
    "\n",
    "def train_model(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total\n",
    "\n",
    "\n",
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl)\n",
    "        print(\"training loss: \", loss)\n",
    "        val_loss(model, valid_dl)\n",
    "        \n",
    "        \n",
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)\n",
    "\n",
    "\n",
    "train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "torch.save(model.state_dict(), \"state_model.pt\")\n",
    "\n",
    "bucket = os.environ['AIP_MODEL_DIR'].split('/')[2]\n",
    "blob_name = '/'.join(os.environ['AIP_MODEL_DIR'].split('/')[3:])\n",
    "\n",
    "storage_client = storage.Client(project=args.project)\n",
    "bucket = storage_client.bucket(bucket)\n",
    "\n",
    "files_to_upload = [\"label.pkl\", \"std_scaler.pkl\", \"state_model.pt\", \"emb.pkl\"]\n",
    "\n",
    "for i in files_to_upload:\n",
    "    blob = bucket.blob(blob_name+i)\n",
    "    blob.upload_from_filename(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511604d-3c13-43e2-b251-5ed504848116",
   "metadata": {},
   "source": [
    "## Training Job (CustomJob.from_local_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd16c2c-6796-46c7-811d-d159a5884d3b",
   "metadata": {},
   "source": [
    "To speed up the training a GPU NVIDIA Tesla T4 is used, it should take around 2 minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e477c-8ad3-49b7-b5ef-e456e73977d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "\n",
    "customJob = aip.CustomJob.from_local_script(\n",
    "    display_name = 'pytorch_tab_sa_ecommerce',\n",
    "    script_path = 'source/trainer/train.py',\n",
    "    container_uri = TRAIN_IMAGE_URI,\n",
    "    requirements = ['scikit-learn'],\n",
    "    args = [\n",
    "        '--dataset_uri', \n",
    "        DATASET_URI,\n",
    "        '--project',\n",
    "        PROJECT_ID\n",
    "    ],\n",
    "    replica_count = 1,\n",
    "    machine_type = 'n1-standard-4',\n",
    "    accelerator_type = 'NVIDIA_TESLA_T4',\n",
    "    accelerator_count = 1,\n",
    "    staging_bucket = STAGING_URI,\n",
    "    base_output_dir = MODEL_URI\n",
    ")\n",
    "\n",
    "customJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169928f-3d93-47ae-83dc-dbf10e923d29",
   "metadata": {},
   "source": [
    "## Creating Custom Container for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2bd063-39bb-435e-877f-ca6095f7d9d0",
   "metadata": {},
   "source": [
    "#### The method I'm using is called Custom Prediction Routines, where we specify load, preprocess and prediction methods and Vertex will do the rest for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ce845f0b-b8af-4d3a-a02d-4d4516d251c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_SRC_DIR = \"src_dir_pytorch\"  # @param {type:\"string\"}\n",
    "IMAGE_URI = \"us-central1-docker.pkg.dev/jchavezar-demo/custom-predictions/pytorch-ecommerce:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b6cabc-0dff-4104-afa2-c16236814489",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr $USER_SRC_DIR\n",
    "!mkdir $USER_SRC_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4e2a7b-daa8-41d9-870b-107ea75ea884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src_dir_pytorch/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $USER_SRC_DIR/requirements.txt\n",
    "fastapi\n",
    "uvicorn==0.17.6\n",
    "pandas\n",
    "torch\n",
    "scikit-learn\n",
    "google-cloud-storage>=1.26.0,<2.0.0dev\n",
    "google-cloud-aiplatform[prediction]>=1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c60eb-c3ab-4e0c-a310-f8b7d9588a72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r $USER_SRC_DIR/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34f41d67-01cf-429c-9fbe-cd720a6055e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vtx-models/pytorch/ecommerce/model/emb.pkl...\n",
      "Copying gs://vtx-models/pytorch/ecommerce/model/label.pkl...                    \n",
      "Copying gs://vtx-models/pytorch/ecommerce/model/state_model.pt...               \n",
      "Copying gs://vtx-models/pytorch/ecommerce/model/std_scaler.pkl...               \n",
      "/ [4 files][382.8 KiB/382.8 KiB]                                                \n",
      "Operation completed over 4 objects/382.8 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "## Copy all the Artifacts from Vertex Custom Training\n",
    "!gsutil cp $MODEL_URI/model/* $USER_SRC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c9c287-06fc-4586-a80a-1f89afaeebd1",
   "metadata": {},
   "source": [
    "#### PyTorch has issues with libraries so I highly recommend install their packages with conda:\n",
    "\n",
    "$ conda install pytorch torchvision torchaudio cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcf3f2c0-c09e-4052-8731-dd51509d03bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src_dir_pytorch/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $USER_SRC_DIR/predictor.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from typing import Dict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as torch_optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "\n",
    "\n",
    "class CustomPyTorchPredictor(Predictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedded_col_names = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "        self.columns = [ \"latest_ecommerce_progress\" , \"bounces\", \"time_on_site\", \"pageviews\", \"source\", \"medium\", \"channelGrouping\", \"deviceCategory\", \"country\"]\n",
    "\n",
    "            \n",
    "    def preprocess(self, prediction_input: Dict) -> torch.utils.data.dataloader.DataLoader:\n",
    "        instances = prediction_input[\"instances\"]\n",
    "        data = pd.DataFrame(instances, columns = self.columns)\n",
    "        ## Prepare Data        \n",
    "        embedded_col_names = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "        \n",
    "        def preprocessing_data(df):\n",
    "            import pickle\n",
    "            \n",
    "            standarization = pickle.load(open(\"std_scaler.pkl\", \"rb\"))\n",
    "            labelencoder = pickle.load(open(\"label.pkl\", \"rb\"))\n",
    "    \n",
    "            target = 'will_buy_on_return_visit'\n",
    "            cat_columns = [i for i in df.columns if df[i].dtypes == 'object']\n",
    "            num_columns = [i for i in df.columns if df[i].dtypes == 'int64' or df[i].dtypes == 'float']\n",
    "\n",
    "            cat_df = df[cat_columns]\n",
    "            num_df = df[num_columns]\n",
    "    \n",
    "            cat_df = cat_df.apply(lambda x: labelencoder[x.name].transform(x))\n",
    "            cat_df = cat_df.astype('category')\n",
    "    \n",
    "            df = pd.concat([cat_df, num_df], axis=1)\n",
    "            df[num_columns] = standarization.transform(df[num_columns])\n",
    "            \n",
    "            return df\n",
    "\n",
    "        class PredictData(Dataset):\n",
    "            def __init__(self, X):\n",
    "                embedded_col_names = ['source', 'medium', 'channelGrouping', 'deviceCategory', 'country']\n",
    "                self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64)\n",
    "                self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32)\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                return self.X1[index], self.X2[index]\n",
    "\n",
    "            def __len__ (self):\n",
    "                return len(self.X1)\n",
    "        \n",
    "        prep_df = DataLoader(PredictData(preprocessing_data(data)))\n",
    "        return prep_df\n",
    "    \n",
    "    def load(self, artifacts_uri: str):\n",
    "        \"\"\"Loads the model artifacts.\"\"\"\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        self.embeddings = pickle.load(open('emb.pkl', 'rb'))\n",
    "        class ShelterOutcomeModel(nn.Module):\n",
    "            def __init__(self, embedding_sizes, n_cont):\n",
    "                super().__init__()\n",
    "                self.embeddings = embedding_sizes\n",
    "                n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "                self.n_emb, self.n_cont = n_emb, 4\n",
    "                self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "                self.lin2 = nn.Linear(200, 70)\n",
    "                self.lin3 = nn.Linear(70, 2)\n",
    "                self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "                self.bn2 = nn.BatchNorm1d(200)\n",
    "                self.bn3 = nn.BatchNorm1d(70)\n",
    "                self.emb_drop = nn.Dropout(0.6)\n",
    "                self.drops = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "            def forward(self, x_cat, x_cont):\n",
    "                x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "                x = torch.cat(x, 1)\n",
    "                x = self.emb_drop(x)\n",
    "                x2 = self.bn1(x_cont)\n",
    "                x = torch.cat([x, x2], 1)\n",
    "                x = F.relu(self.lin1(x))\n",
    "                x = self.drops(x)\n",
    "                x = self.bn2(x)\n",
    "                x = F.relu(self.lin2(x))\n",
    "                x = self.drops(x)\n",
    "                x = self.bn3(x)\n",
    "                x = self.lin3(x)\n",
    "                return x\n",
    "            \n",
    "        device = torch.device('cpu')\n",
    "        self._model = ShelterOutcomeModel(self.embeddings, 4)\n",
    "        self._model.load_state_dict(torch.load(\"state_model.pt\", map_location=device))\n",
    "        \n",
    "    @torch.inference_mode()\n",
    "    def predict(self, instances: torch.utils.data.dataloader.DataLoader) -> list:\n",
    "        \"\"\"Performs prediction.\"\"\"\n",
    "        preds = []\n",
    "        self._model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x1,x2 in instances:\n",
    "                out = self._model(x1,x2)\n",
    "                prob = F.softmax(out, dim=1)\n",
    "                preds.append(prob)\n",
    "        final_probs = [item for sublist in preds for item in sublist]\n",
    "        predicted = [0 if t[0] > 0.5 else 1 for t in final_probs]\n",
    "        print(predicted)\n",
    "        return predicted\n",
    "\n",
    "    def postprocess(self, prediction_results: list) -> Dict:\n",
    "        return {\"predictions\": prediction_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7fafe-6348-4efa-96bd-b22fe7970392",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "The easiest way to handle AuthN/AuthZ for next steps is by login with application credentials, this method will store json credential locally here: **/home/jupyter/.config/gcloud/application_default_credentials.json**\n",
    "\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22334484-dd5b-40f5-9e03-ada467788e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS_FILE = \"/home/jupyter/.config/gcloud/application_default_credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebc51ec8-f52e-49a7-9646-7d56345f7ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from src_dir_pytorch.predictor import \\\n",
    "    CustomPyTorchPredictor  # Update this path as the variable $USER_SRC_DIR to import the custom predictor.\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    USER_SRC_DIR,\n",
    "    IMAGE_URI,\n",
    "    predictor=CustomPyTorchPredictor,\n",
    "    requirements_path=os.path.join(USER_SRC_DIR, \"requirements.txt\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d492b37-1ea8-47b9-9f5c-14a2d6098998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/jchavezar-demo/custom-predictions/pytorch-ecommerce:latest\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef1317-6736-43c4-a9a2-a05cbdfe83e0",
   "metadata": {},
   "source": [
    "##  Test Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a042807-064e-4207-901e-afa7534a8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"instances.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb6e39c6-7939-420e-b881-b2fc112f9ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting instances.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $INPUT_FILE\n",
    "{\n",
    "    \"instances\": [\n",
    "        [0, 0, 142, 5.0, \"(direct)\", \"(none)\", \"Direct\", \"mobile\", \"Argentina\"]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4760d5ca-43c8-4660-9106-1d6abf59d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"{MODEL_URI}/model\",\n",
    "    credential_path = CREDENTIALS_FILE,\n",
    ") as local_endpoint:\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request_file = INPUT_FILE,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a7b93-1e7f-473d-9c0a-57f0212ebc1e",
   "metadata": {},
   "source": [
    "## Deploy to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "410ba357-ae48-4f34-aeae-01e93bc161ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/envs/cpr/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb2b67-cebd-4c07-a152-ad25afadaec1",
   "metadata": {},
   "source": [
    "## Upload Model to Vertex Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8320140-5b45-4986-b787-261843137f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/569083142710/locations/us-central1/models/275372687176499200/operations/1071499163876720640\n",
      "Model created. Resource name: projects/569083142710/locations/us-central1/models/275372687176499200@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/569083142710/locations/us-central1/models/275372687176499200@1')\n"
     ]
    }
   ],
   "source": [
    "model = aip.Model.upload(\n",
    "    local_model=local_model,\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=f\"{MODEL_URI}/model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51a39c-3cd4-445f-b44b-33e8288ba5de",
   "metadata": {},
   "source": [
    "## Deploy Model using Vertex Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd5a22b2-56ea-4f9a-9670-e72e61593272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/569083142710/locations/us-central1/endpoints/2482173887983386624/operations/4245129526289367040\n",
      "Endpoint created. Resource name: projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/569083142710/locations/us-central1/endpoints/2482173887983386624')\n",
      "Deploying model to Endpoint : projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n",
      "Deploy Endpoint model backing LRO: projects/569083142710/locations/us-central1/endpoints/2482173887983386624/operations/4801324080269623296\n",
      "Endpoint model deployed. Resource name: projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0e919-79dc-4402-a9a8-d005da08a853",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b9d2b428-f9c0-4ec2-893d-57a48e04ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    0\n",
      "  ],\n",
      "  \"deployedModelId\": \"5720939319225483264\",\n",
      "  \"model\": \"projects/569083142710/locations/us-central1/models/275372687176499200\",\n",
      "  \"modelDisplayName\": \"pytorch-ecommerce\",\n",
      "  \"modelVersionId\": \"1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" https://us-central1-aiplatform.googleapis.com/v1/$endpoint.gca_resource.name:predict -d \"@$INPUT_FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3f4aa-a86e-40cf-aeee-a887b5e25ee8",
   "metadata": {},
   "source": [
    "## Destroy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "685c1af4-9504-4618-b785-e7f31705f58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n",
      "Undeploy Endpoint model backing LRO: projects/569083142710/locations/us-central1/endpoints/2482173887983386624/operations/2089031204685742080\n",
      "Endpoint model undeployed. Resource name: projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n"
     ]
    }
   ],
   "source": [
    "endpoint.undeploy(deployed_model_id=endpoint.gca_resource.deployed_models[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f06e5b71-b05b-4c00-9f61-c41ec37da923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Endpoint : projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n",
      "Delete Endpoint  backing LRO: projects/569083142710/locations/us-central1/operations/8368737935100477440\n",
      "Endpoint deleted. . Resource name: projects/569083142710/locations/us-central1/endpoints/2482173887983386624\n"
     ]
    }
   ],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "45f3654f-35bc-48ca-b8a7-cddd1e2408f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/569083142710/locations/us-central1/models/275372687176499200\n",
      "Delete Model  backing LRO: projects/569083142710/locations/us-central1/operations/7686442591553847296\n",
      "Model deleted. . Resource name: projects/569083142710/locations/us-central1/models/275372687176499200\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "84a7b65f-1407-452c-8dbe-4952516a831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr $USER_SRC_DIR\n",
    "!rm -fr instances.json\n",
    "!rm -fr source"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cpr",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "cpr",
   "language": "python",
   "name": "cpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
