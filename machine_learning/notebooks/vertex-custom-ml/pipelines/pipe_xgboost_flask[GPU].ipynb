{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a337da5c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This is just a bunch of snippets ordered to create a quick pipeline which demonstrates Vertex capabilities, the pipeline contains different technologies, frameworks, etc, like dask, rapids, docker for the training leveraging GPU and using Flask as webserver for online predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a784d-6402-442d-9a66-a47fa6a8f4ad",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0366ebbc-760e-4510-b176-c57a805ab6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "CUSTOM_TRAIN_NAME = 'gpu_custom_job'\n",
    "PIPELINE_ROOT_PATH = 'gs://vtx-root-path'\n",
    "MODEL_FILE_BUCKET = 'gs://vtx-pipe-models'\n",
    "TRAINING_REPOSITORY = 'trainings'\n",
    "IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{TRAINING_REPOSITORY}/train_xgb_gpu:latest'\n",
    "PREDICTION_REPOSITORY = 'predictions'\n",
    "PREDICTION_IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{PREDICTION_REPOSITORY}/pred_xgb_cpu:latest'\n",
    "ART_REG = IMAGE_URI.split('/')[0]\n",
    "DATASET_DISPLAY_NAME = 'covertype-4Mr'\n",
    "DATASET_SOURCE = 'gs://vtx-datasets-public/cover_type_4Mrows.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04690df7-d0e8-4a84-a118-b1c11c5c522e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d42af723-702e-41a3-8b86-a8bc8deb6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr training\n",
    "!rm -fr prediction\n",
    "!mkdir training\n",
    "!mkdir prediction\n",
    "!mkdir training/trainer\n",
    "!touch training/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2197d9e-6788-40d7-9ade-5a3495ba1004",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils for Storing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "ec3e6d22-d977-40da-8134-8947a37c5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/utils.py\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def save_model(args):\n",
    "    \"\"\"Saves the model to Google Cloud Storage or local file system\n",
    "    Args:\n",
    "      args: contains name for saved model.\n",
    "    \"\"\"\n",
    "    scheme = 'gs://'\n",
    "    if args.job_dir.startswith(scheme):\n",
    "        print(f\"Reading input job_dir: {args.job_dir}\")\n",
    "        job_dir = args.job_dir.split(\"/\")\n",
    "        bucket_name = job_dir[2]\n",
    "        object_prefix = \"/\".join(job_dir[3:]).rstrip(\"/\")\n",
    "        print(f\"Reading object_prefix: {object_prefix}\")\n",
    "\n",
    "        if object_prefix:\n",
    "            model_path = '{}/{}'.format(object_prefix, \"xgboost\")\n",
    "        else:\n",
    "            model_path = '{}'.format(\"xgboost\")\n",
    "            \n",
    "        print(f\"The model path is {model_path}\")\n",
    "        bucket = storage.Client().bucket(bucket_name)    \n",
    "        local_path = os.path.join(\"/tmp\", \"xgboost\")\n",
    "        files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
    "        for file in files:\n",
    "            local_file = os.path.join(local_path, file)\n",
    "            blob = bucket.blob(\"/\".join([model_path, file]))\n",
    "            blob.upload_from_filename(local_file)\n",
    "        print(local_file)\n",
    "        print(f\"gs://{bucket_name}/{model_path}\")\n",
    "        print(f\"Saved model files in gs://{bucket_name}/{model_path}\")\n",
    "    else:\n",
    "        print(f\"Saved model files at {os.path.join('/tmp', args.model_name)}\")\n",
    "        print(f\"To save model files in GCS bucket, please specify job_dir starting with gs://\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1702b-45ec-4dca-8aaf-992886d3c3ca",
   "metadata": {},
   "source": [
    "## Training Code with Dask + CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e89b392d-c322-49d3-b801-639097275a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/task.py\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import wait\n",
    "from dask import array as da\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from xgboost import dask as dxgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "import argparse\n",
    "import time\n",
    "import utils\n",
    "import gcsfs\n",
    "import dask_cudf\n",
    "import os, json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--dataset_source', \n",
    "    dest='dataset',\n",
    "    type=str,\n",
    "    help='Dataset.')\n",
    "parser.add_argument(\n",
    "    '--job-dir',\n",
    "    dest='job_dir',\n",
    "    type=str,\n",
    "    default=os.getenv('AIP_MODEL_DIR'),\n",
    "    help='GCS location to export models')\n",
    "parser.add_argument(\n",
    "    '--model-name',\n",
    "    dest='model_name',\n",
    "    default=\"custom-train\",\n",
    "    help='The name of your saved model')\n",
    "parser.add_argument(\n",
    "    '--num-gpu-per-worker', type=str, help='num of workers',\n",
    "    default=2)\n",
    "parser.add_argument(\n",
    "    '--threads-per-worker', type=str, help='num of threads per worker',\n",
    "    default=4)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def using_quantile_device_dmatrix(client: Client, dataset_source, job_dir, model_name):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"[INFO] ------ Importing dataset {dataset_source}\")\n",
    "    df = dask_cudf.read_csv(dataset_source)\n",
    "\n",
    "    print(\"Cleaning and standarizing dataset\")\n",
    "    df = df.dropna() \n",
    "\n",
    "    print(f\"[INFO] ------ Splitting dataset\")\n",
    "    df_train, df_eval = df.random_split([0.8, 0.2], random_state=123)\n",
    "    df_train_features= df_train.drop('Cover_Type', axis=1)\n",
    "    df_eval_features= df_eval.drop('Cover_Type', axis=1)\n",
    "    df_train_labels = df_train.pop('Cover_Type')\n",
    "    df_eval_labels = df_eval.pop('Cover_Type')\n",
    "\n",
    "    print(xgb.__version__)\n",
    "\n",
    "    print(\"[INFO] ------ Dataset for dask\")\n",
    "    dtrain = dxgb.DaskDeviceQuantileDMatrix(client, df_train_features, df_train_labels)\n",
    "    \n",
    "    print(\"[INFO] ------ Dataset for dask\")\n",
    "    dvalid = dxgb.DaskDeviceQuantileDMatrix(client, df_eval_features, df_eval_labels)\n",
    "    print(\"[INFO]: ------ QuantileDMatrix is formed in {} seconds ---\".format((time.time() - start_time)))\n",
    "\n",
    "    del df_train_features\n",
    "    del df_train_labels\n",
    "    del df_eval_features\n",
    "    del df_eval_labels\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"Training\")\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\n",
    "            \"verbosity\": 2, \n",
    "            \"tree_method\": \"gpu_hist\", \n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": [\"mlogloss\"],\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"gamma\": 0.9,\n",
    "            \"subsample\": 0.5,\n",
    "            \"max_depth\": 9,\n",
    "            \"num_class\": 8\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=10,\n",
    "        evals=[(dvalid, \"valid1\")],\n",
    "        early_stopping_rounds=5\n",
    "    ) \n",
    "    print(\"[INFO]: ------ Training is completed in {} seconds ---\".format((time.time() - start_time)))\n",
    "\n",
    "    # Saving models and exporting performance metrics\n",
    "    \n",
    "    df_eval_metrics = pd.DataFrame(output[\"history\"][\"valid1\"])\n",
    "    model = output[\"booster\"]\n",
    "    best_model = model[: model.best_iteration]\n",
    "    print(f\"[INFO] ------ Best model: {best_model}\")\n",
    "    temp_dir = \"/tmp/xgboost\"\n",
    "    os.mkdir(temp_dir)\n",
    "    print(job_dir)\n",
    "    best_model.save_model(\"{}/{}\".format(temp_dir, model_name))\n",
    "    df_eval_metrics.to_json(\"{}/all_results.json\".format(temp_dir))\n",
    "\n",
    "    utils.save_model(args)\n",
    "\n",
    "def get_scheduler_info():\n",
    "    scheduler_ip =  subprocess.check_output(['hostname','--all-ip-addresses'])\n",
    "    scheduler_ip = scheduler_ip.decode('UTF-8').split()[0]\n",
    "    scheduler_port = '8786'\n",
    "    scheduler_uri = '{}:{}'.format(scheduler_ip, scheduler_port)\n",
    "    return scheduler_ip, scheduler_uri\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"[INFO] ------ Creating dask cluster\")\n",
    "    \n",
    "    sched_ip, sched_uri = get_scheduler_info()\n",
    "    \n",
    "    print(f\"[INFO] ------ Sched_ip and Sched_uri, {sched_ip}, {sched_uri}\")\n",
    "\n",
    "    print(\"[INFO]: ------ LocalCUDACluster is being formed \")\n",
    "    \n",
    "    with LocalCUDACluster(\n",
    "        ip=sched_ip,\n",
    "        n_workers=int(args.num_gpu_per_worker), \n",
    "        threads_per_worker=int(args.threads_per_worker) \n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            print('[INFO]: ------ Calling main function ')\n",
    "            using_quantile_device_dmatrix(client, dataset_source=args.dataset, job_dir=args.job_dir, model_name=args.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defad8e8-c925-4e38-8545-20bff43bb3bd",
   "metadata": {},
   "source": [
    "## Wrapping Code (Custom Container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3403c9-df5c-44dd-b3ad-af71a7ff01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/Dockerfile\n",
    "\n",
    "FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
    "\n",
    "RUN pip install google.cloud[storage] \\\n",
    "  && pip install gcsfs \\\n",
    "  && pip install pandas\n",
    "\n",
    "COPY trainer trainer/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76450e-37a6-4f19-983b-8b49e0394fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create: Repositories and Push Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d3daff53-b1b1-4912-a584-f1a15fe6207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $TRAINING_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8d878b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  10.75kB\n",
      "\u001b[91m[WARNING]: Empty continuation line found in:\n",
      "    RUN pip install google-cloud-storage   && pip install gcsfs COPY trainer trainer/\n",
      "[WARNING]: Empty continuation lines will become errors in a future release.\n",
      "\u001b[0mStep 1/3 : FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
      " ---> 6f5057ed56a0\n",
      "Step 2/3 : RUN pip install google-cloud-storage   && pip install gcsfs COPY trainer trainer/\n",
      " ---> Running in c07d92678834\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 2.9 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 KB 12.7 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-cloud-storage) (2.27.1)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 KB 12.9 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.10.0-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.2/167.2 KB 11.8 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 KB 14.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.19.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.0.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (36 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.12)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 12.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, googleapis-common-protos, google-crc32c, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed google-api-core-2.8.2 google-auth-2.10.0 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.9\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mERROR: Invalid requirement: 'trainer/'\n",
      "Hint: It looks like a path. File 'trainer/' does not exist.\n",
      "\u001b[0mThe command '/bin/bash -c pip install google-cloud-storage   && pip install gcsfs COPY trainer trainer/' returned a non-zero code: 1\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_URI training/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c880026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_xgb_gpu]\n",
      "\n",
      "\u001b[1B91d07aa7: Preparing \n",
      "\u001b[1B02f738af: Preparing \n",
      "\u001b[1Bd20a3982: Preparing \n",
      "\u001b[1B177f7dba: Preparing \n",
      "\u001b[1Be2396578: Preparing \n",
      "\u001b[1B749f56f5: Preparing \n",
      "\u001b[1Ba57855d6: Preparing \n",
      "\u001b[1Beca41527: Preparing \n",
      "\u001b[1B32e4a10b: Preparing \n",
      "\u001b[1Bc89280f3: Preparing \n",
      "\u001b[1Bd1e3350d: Preparing \n",
      "\u001b[1B8e8f7e67: Preparing \n",
      "\u001b[1B0767a47c: Layer already exists \u001b[6A\u001b[2K\u001b[3A\u001b[2Klatest: digest: sha256:8fd54135b058de05c60ff36b497c6d19ed6d61427be2e4db45cea8b776931f0b size: 3063\n"
     ]
    }
   ],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128fb06-e3d4-4775-89ee-1a09d5c33dd3",
   "metadata": {},
   "source": [
    "## Create: Vertex Pipe Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0c1afce7-600e-43f6-90e4-ddd1be60d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Output, Model, Metrics, Model)\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    ")\n",
    "def get_train_job_details(\n",
    "    model_dir: str,\n",
    "    model_display_name: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    eval_metric_key: str\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    metrics_uri = \"{}/model/xgboost/all_results.json\".format(model_dir)\n",
    "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
    "    for k,v in metrics_df.items():\n",
    "        logging.info(f\"    {k} -> {v}\")\n",
    "        metrics.log_metric(k, min(v.values()))\n",
    "        \n",
    "    eval_metric = (min(metrics_df[eval_metric_key].values()) if eval_metric_key in metrics_df.keys() else None)\n",
    "    model.metadata[eval_metric_key] = eval_metric\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\"])\n",
    "    \n",
    "    return outputs(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adaf9b-4066-44a0-b696-2e9f7439f1bb",
   "metadata": {},
   "source": [
    "## Create: Prediction Custom Container Vertex|Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5a3087f0-3e36-4e56-b073-60f930cf9161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/app.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from flask import Flask, request, Response, jsonify\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "\n",
    "# Model Download from gcs\n",
    "\n",
    "fname = \"model.json\"\n",
    "\n",
    "with open(fname, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{fname}\", model\n",
    "    )\n",
    "\n",
    "# Loading model\n",
    "print(\"[INFO] ------ Loading model from: {}\".format(fname))\n",
    "model = xgb.Booster(model_file=fname)\n",
    "\n",
    "# Creation of the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Flask route for Liveness checks\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'])\n",
    "def isalive():\n",
    "    status_code = Response(status=200)\n",
    "    return status_code\n",
    "\n",
    "# Flask route for predictions\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'],methods=['GET','POST'])\n",
    "def prediction():\n",
    "    _features = ['Id','Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "                          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', \n",
    "                          'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9',\n",
    "                          'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19', \n",
    "                          'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29',\n",
    "                          'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "    data = request.get_json(silent=True, force=True)\n",
    "    dmf = xgb.DMatrix(pd.DataFrame(data[\"instances\"], columns=_features))\n",
    "    response = pd.DataFrame(model.predict(dmf))\n",
    "    logging.info(f\"Response: {response}\")\n",
    "    return jsonify({\"Cover Type\": str(response.idxmax(axis=1)[0])})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa60ef-8088-447b-914f-812294877734",
   "metadata": {},
   "source": [
    "### Preparing Docker Container Declarative Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "086026cc-fff9-454a-8608-cc413d222701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/requirements.txt\n",
    "\n",
    "google-cloud-storage\n",
    "numpy\n",
    "pandas\n",
    "flask\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "05bfdf9f-f5f8-4e6e-bb1d-ae4dc335a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/Dockerfile\n",
    "\n",
    "FROM python:3.7-buster\n",
    "\n",
    "RUN mkdir my-model\n",
    "\n",
    "COPY app.py ./app.py\n",
    "COPY requirements.txt ./requirements.txt\n",
    "RUN pip install -r requirements.txt \n",
    "\n",
    "# Flask Env Variable\n",
    "ENV FLASK_APP=app\n",
    "\n",
    "# Expose port 8080\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD flask run --host=0.0.0.0 --port=8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725956dc-2ee0-43cd-96bb-1f64796ddc44",
   "metadata": {},
   "source": [
    "### Push Container Image to Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ae37a7f8-8eb6-4066-867f-d2c868cdd94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $PREDICTION_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "34444603-e3ae-4d4a-85ca-ab8e7017fb15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 2.5 KiB before compression.\n",
      "Uploading tarball of [prediction/.] to [gs://jchavezar-demo_cloudbuild/source/1659824719.677456-63972e551a1f492c91461731f87a4a24.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/0edd44f7-1237-4962-a716-b814034efd25].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/0edd44f7-1237-4962-a716-b814034efd25?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0edd44f7-1237-4962-a716-b814034efd25\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1659824719.677456-63972e551a1f492c91461731f87a4a24.tgz#1659824720077592\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1659824719.677456-63972e551a1f492c91461731f87a4a24.tgz#1659824720077592...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/8 : FROM python:3.7-buster\n",
      "3.7-buster: Pulling from library/python\n",
      "7e6a53d1988f: Pulling fs layer\n",
      "4fe4e1c58b4a: Pulling fs layer\n",
      "cc915d298757: Pulling fs layer\n",
      "f08b88f29371: Pulling fs layer\n",
      "28bcb226aca8: Pulling fs layer\n",
      "40e111dacc82: Pulling fs layer\n",
      "b25055bda02d: Pulling fs layer\n",
      "1650e1d4c2e0: Pulling fs layer\n",
      "9a88d4d5625b: Pulling fs layer\n",
      "f08b88f29371: Waiting\n",
      "28bcb226aca8: Waiting\n",
      "40e111dacc82: Waiting\n",
      "b25055bda02d: Waiting\n",
      "1650e1d4c2e0: Waiting\n",
      "9a88d4d5625b: Waiting\n",
      "4fe4e1c58b4a: Verifying Checksum\n",
      "4fe4e1c58b4a: Download complete\n",
      "cc915d298757: Verifying Checksum\n",
      "cc915d298757: Download complete\n",
      "7e6a53d1988f: Verifying Checksum\n",
      "7e6a53d1988f: Download complete\n",
      "40e111dacc82: Verifying Checksum\n",
      "40e111dacc82: Download complete\n",
      "f08b88f29371: Verifying Checksum\n",
      "f08b88f29371: Download complete\n",
      "1650e1d4c2e0: Verifying Checksum\n",
      "1650e1d4c2e0: Download complete\n",
      "9a88d4d5625b: Download complete\n",
      "b25055bda02d: Verifying Checksum\n",
      "b25055bda02d: Download complete\n",
      "28bcb226aca8: Verifying Checksum\n",
      "28bcb226aca8: Download complete\n",
      "7e6a53d1988f: Pull complete\n",
      "4fe4e1c58b4a: Pull complete\n",
      "cc915d298757: Pull complete\n",
      "f08b88f29371: Pull complete\n",
      "28bcb226aca8: Pull complete\n",
      "40e111dacc82: Pull complete\n",
      "b25055bda02d: Pull complete\n",
      "1650e1d4c2e0: Pull complete\n",
      "9a88d4d5625b: Pull complete\n",
      "Digest: sha256:c0683a2e5ae43163f0441f02d78789c892c118e0c023890f1b8edf5e3b7b1963\n",
      "Status: Downloaded newer image for python:3.7-buster\n",
      " ---> 1902e2432d77\n",
      "Step 2/8 : RUN mkdir my-model\n",
      " ---> Running in 756c4ef66e51\n",
      "Removing intermediate container 756c4ef66e51\n",
      " ---> 053bd9aaf9ee\n",
      "Step 3/8 : COPY app.py ./app.py\n",
      " ---> a6dfb5abba82\n",
      "Step 4/8 : COPY requirements.txt ./requirements.txt\n",
      " ---> 81c92a0b7bb9\n",
      "Step 5/8 : RUN pip install -r requirements.txt\n",
      " ---> Running in b6efadc45890\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 11.1 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 35.4 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 40.7 MB/s eta 0:00:00\n",
      "Collecting flask\n",
      "  Downloading Flask-2.2.1-py3-none-any.whl (101 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 KB 17.6 MB/s eta 0:00:00\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.9/192.9 MB 5.8 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 KB 16.9 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 KB 13.1 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.10.0-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.2/167.2 KB 25.2 MB/s eta 0:00:00\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 10.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 503.5/503.5 KB 44.0 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 30.3 MB/s eta 0:00:00\n",
      "Collecting Werkzeug>=2.2.0\n",
      "  Downloading Werkzeug-2.2.1-py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.4/232.4 KB 34.7 MB/s eta 0:00:00\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 17.3 MB/s eta 0:00:00\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 21.8 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting importlib-metadata>=3.6.0\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 24.2 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 KB 30.8 MB/s eta 0:00:00\n",
      "Collecting protobuf<5.0.0dev,>=3.15.0\n",
      "  Downloading protobuf-4.21.4-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.4/408.4 KB 41.6 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 20.6 MB/s eta 0:00:00\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 KB 11.0 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.9/139.9 KB 21.5 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 KB 26.2 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 13.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, pyasn1, zipp, urllib3, typing-extensions, six, rsa, pyasn1-modules, protobuf, numpy, MarkupSafe, itsdangerous, idna, google-crc32c, charset-normalizer, certifi, cachetools, Werkzeug, scipy, requests, python-dateutil, Jinja2, importlib-metadata, googleapis-common-protos, google-resumable-media, google-auth, xgboost, pandas, google-api-core, click, google-cloud-core, flask, google-cloud-storage\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.1 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.0 click-8.1.3 flask-2.2.1 google-api-core-2.8.2 google-auth-2.10.0 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 idna-3.3 importlib-metadata-4.12.0 itsdangerous-2.1.2 numpy-1.21.6 pandas-1.3.5 protobuf-4.21.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.2 pytz-2022.1 requests-2.28.1 rsa-4.9 scipy-1.7.3 six-1.16.0 typing-extensions-4.3.0 urllib3-1.26.11 xgboost-1.6.1 zipp-3.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container b6efadc45890\n",
      " ---> 630fc73e7358\n",
      "Step 6/8 : ENV FLASK_APP=app\n",
      " ---> Running in 58be2909fccb\n",
      "Removing intermediate container 58be2909fccb\n",
      " ---> e4f2d5c607e7\n",
      "Step 7/8 : EXPOSE 8080\n",
      " ---> Running in a4ab9a99b8fa\n",
      "Removing intermediate container a4ab9a99b8fa\n",
      " ---> 00076096343a\n",
      "Step 8/8 : CMD flask run --host=0.0.0.0 --port=8080\n",
      " ---> Running in 9beeb1a16847\n",
      "Removing intermediate container 9beeb1a16847\n",
      " ---> c0405b8d2b4b\n",
      "Successfully built c0405b8d2b4b\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu]\n",
      "d6fb48cc99db: Preparing\n",
      "e0fc834dac1e: Preparing\n",
      "37e37f538376: Preparing\n",
      "b2c9351e65f8: Preparing\n",
      "1db1a8577818: Preparing\n",
      "aebaae58a42a: Preparing\n",
      "a31ded3cf4db: Preparing\n",
      "6e8d86d6d73d: Preparing\n",
      "1e90ee259bc5: Preparing\n",
      "91a7fe198f83: Preparing\n",
      "a657c4b93150: Preparing\n",
      "eea33d4fb826: Preparing\n",
      "37a609cfa10d: Preparing\n",
      "aebaae58a42a: Waiting\n",
      "1e90ee259bc5: Waiting\n",
      "91a7fe198f83: Waiting\n",
      "a657c4b93150: Waiting\n",
      "eea33d4fb826: Waiting\n",
      "37a609cfa10d: Waiting\n",
      "a31ded3cf4db: Waiting\n",
      "6e8d86d6d73d: Waiting\n",
      "1db1a8577818: Layer already exists\n",
      "aebaae58a42a: Layer already exists\n",
      "a31ded3cf4db: Layer already exists\n",
      "6e8d86d6d73d: Layer already exists\n",
      "e0fc834dac1e: Pushed\n",
      "b2c9351e65f8: Pushed\n",
      "1e90ee259bc5: Layer already exists\n",
      "37e37f538376: Pushed\n",
      "eea33d4fb826: Layer already exists\n",
      "91a7fe198f83: Layer already exists\n",
      "a657c4b93150: Layer already exists\n",
      "37a609cfa10d: Layer already exists\n",
      "d6fb48cc99db: Pushed\n",
      "latest: digest: sha256:56ddd537c514f8242f462551a2fece47c3112633d4bedee352e9ae2a65298ced size: 3053\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                        STATUS\n",
      "0edd44f7-1237-4962-a716-b814034efd25  2022-08-06T22:25:20+00:00  1M54S     gs://jchavezar-demo_cloudbuild/source/1659824719.677456-63972e551a1f492c91461731f87a4a24.tgz  us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $PREDICTION_IMAGE_URI prediction/. --timeout 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f5e54-1647-42e2-b15c-235ae18b163e",
   "metadata": {},
   "source": [
    "## Create: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "afac2a5d-347d-4d73-80a5-12dc9d08d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline, Condition\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp as custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "\n",
    "@pipeline(name='dask-gpu-1')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    custom_train_name: str,\n",
    "    region: str,\n",
    "    eval_acc_threshold: float,\n",
    "    eval_metric_key: str,\n",
    "    model_file_bucket: str,\n",
    "):\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-32\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 4\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"env\": [{\"name\": \"AIP_TRAINING_DATA_URI\", \"value\":'test'}],\n",
    "            \"command\": [\n",
    "                \"python\",\n",
    "                \"trainer/task.py\"\n",
    "            ],\n",
    "            \"args\": [\n",
    "                \"--dataset_source\", \"gs://vtx-datasets-public/cover_type_4Mrows.csv\",\n",
    "                \"--model-name\", \"model.json\",\n",
    "                \"--num-gpu-per-worker\", \"4\",\n",
    "                \"--threads-per-worker\", \"4\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    ]\n",
    "    train_with_cpu_task = custom_job(\n",
    "        project=project_id,\n",
    "        display_name=custom_train_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=model_file_bucket\n",
    "    )\n",
    "    get_train_details_task = get_train_job_details(\n",
    "        model_dir=MODEL_FILE_BUCKET,\n",
    "        model_display_name=\"xgboost-dask\",\n",
    "        eval_metric_key=eval_metric_key, # mlogloss\n",
    "    ).after(train_with_cpu_task)\n",
    "    \n",
    "    with Condition(\n",
    "        get_train_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=\"gs://vtx-models/model/xgboost\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PREDICTION_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        custom_model_upload_job = gcc.ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=\"xgb-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_op)\n",
    "        \n",
    "        endpoint_create_job = gcc.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "        \n",
    "        custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "            model=custom_model_upload_job.outputs[\"model\"],\n",
    "            endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=\"xgboost_model_end\",\n",
    "            traffic_split={\"0\":\"100\"},\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8c3de-97f0-410e-8293-cd3d77bf70f1",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "2fa4335c-36f7-438e-9d46-c22ab5b1e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='dask_cpu.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572bb69-8054-46bd-939b-252ee0b3b7ec",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "6c998a74-c573-4732-bc80-6ab24877e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220806203310\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220806203310')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dask-gpu-1-20220806203310?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"dask_cpu\",\n",
    "    template_path=\"dask_cpu.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'custom_train_name': CUSTOM_TRAIN_NAME,\n",
    "        'region': REGION,\n",
    "        'eval_acc_threshold': 0.5,\n",
    "        'eval_metric_key': 'mlogloss', # mlogloss\n",
    "        'model_file_bucket': MODEL_FILE_BUCKET,\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit(service_account='vtx-pipe@jchavezar-demo.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6daa2a",
   "metadata": {},
   "source": [
    "![](images/vertex-pipe-gpu.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe07e0b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
