{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a337da5c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This is just a bunch of snippets ordered to create a quick pipeline which demonstrates Vertex capabilities, the pipeline contains different technologies, frameworks, etc, like dask, rapids, docker for the training leveraging GPU and using Flask as webserver for online predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a784d-6402-442d-9a66-a47fa6a8f4ad",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0366ebbc-760e-4510-b176-c57a805ab6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "REGION = 'us-central1'\n",
    "CUSTOM_TRAIN_NAME = 'gpu_custom_job'\n",
    "PIPELINE_ROOT_PATH = 'gs://vtx-root-path'\n",
    "MODEL_FILE_BUCKET = 'gs://vtx-pipe-models'\n",
    "TRAINING_REPOSITORY = 'trainings'\n",
    "IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{TRAINING_REPOSITORY}/train_xgb_gpu:latest'\n",
    "PREDICTION_REPOSITORY = 'predictions'\n",
    "PREDICTION_IMAGE_URI = f'us-central1-docker.pkg.dev/jchavezar-demo/{PREDICTION_REPOSITORY}/pred_xgb_cpu:latest'\n",
    "ART_REG = IMAGE_URI.split('/')[0]\n",
    "DATASET_DISPLAY_NAME = 'covertype-4Mr'\n",
    "DATASET_SOURCE = 'gs://vtx-datasets-public/cover_type_4Mrows.csv'\n",
    "NUM_GPU_TRAIN=\"4\"\n",
    "NUM_THREAD_PER_GPU=\"4\"\n",
    "BQ_SOURCE_DATASET='demos'\n",
    "BQ_SOURCE_TABLE='cover_type_4Mrows'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04690df7-d0e8-4a84-a118-b1c11c5c522e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d42af723-702e-41a3-8b86-a8bc8deb6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr training\n",
    "!rm -fr prediction\n",
    "!mkdir training\n",
    "!mkdir prediction\n",
    "!mkdir training/trainer\n",
    "!touch training/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2197d9e-6788-40d7-9ade-5a3495ba1004",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils for Storing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec3e6d22-d977-40da-8134-8947a37c5dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/utils.py\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def save_model(args):\n",
    "    \"\"\"Saves the model to Google Cloud Storage or local file system\n",
    "    Args:\n",
    "      args: contains name for saved model.\n",
    "    \"\"\"\n",
    "    scheme = 'gs://'\n",
    "    if args.job_dir.startswith(scheme):\n",
    "        print(f\"Reading input job_dir: {args.job_dir}\")\n",
    "        job_dir = args.job_dir.split(\"/\")\n",
    "        bucket_name = job_dir[2]\n",
    "        object_prefix = \"/\".join(job_dir[3:]).rstrip(\"/\")\n",
    "        print(f\"Reading object_prefix: {object_prefix}\")\n",
    "\n",
    "        if object_prefix:\n",
    "            model_path = '{}/{}'.format(object_prefix, \"xgboost\")\n",
    "        else:\n",
    "            model_path = '{}'.format(\"xgboost\")\n",
    "            \n",
    "        print(f\"The model path is {model_path}\")\n",
    "        bucket = storage.Client().bucket(bucket_name)    \n",
    "        local_path = os.path.join(\"/tmp\", \"xgboost\")\n",
    "        files = [f for f in os.listdir(local_path) if os.path.isfile(os.path.join(local_path, f))]\n",
    "        for file in files:\n",
    "            local_file = os.path.join(local_path, file)\n",
    "            blob = bucket.blob(\"/\".join([model_path, file]))\n",
    "            blob.upload_from_filename(local_file)\n",
    "        print(local_file)\n",
    "        print(f\"gs://{bucket_name}/{model_path}\")\n",
    "        print(f\"Saved model files in gs://{bucket_name}/{model_path}\")\n",
    "    else:\n",
    "        print(f\"Saved model files at {os.path.join('/tmp', args.model_name)}\")\n",
    "        print(f\"To save model files in GCS bucket, please specify job_dir starting with gs://\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1702b-45ec-4dca-8aaf-992886d3c3ca",
   "metadata": {},
   "source": [
    "## Training Code with Dask + CUDA (GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e89b392d-c322-49d3-b801-639097275a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/trainer/task.py\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import wait\n",
    "from dask import array as da\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from xgboost import dask as dxgb\n",
    "from xgboost.dask import DaskDMatrix\n",
    "import argparse\n",
    "import time\n",
    "import utils\n",
    "import gcsfs\n",
    "import dask_cudf\n",
    "import os, json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from dask.utils import parse_bytes\n",
    "import dask_bigquery\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--project_id', \n",
    "    dest='project_id',\n",
    "    type=str,\n",
    "    help='The Project ID')\n",
    "parser.add_argument(\n",
    "    '--bq_dataset', \n",
    "    dest='bq_dataset',\n",
    "    type=str,\n",
    "    help='BigQuery Dataset')\n",
    "parser.add_argument(\n",
    "    '--bq_table', \n",
    "    dest='bq_table',\n",
    "    type=str,\n",
    "    help='BigQuery Table')\n",
    "parser.add_argument(\n",
    "    '--job-dir',\n",
    "    dest='job_dir',\n",
    "    type=str,\n",
    "    default=os.getenv('AIP_MODEL_DIR'),\n",
    "    help='GCS location to export models')\n",
    "parser.add_argument(\n",
    "    '--model-name',\n",
    "    dest='model_name',\n",
    "    default=\"custom-train\",\n",
    "    help='The name of your saved model')\n",
    "parser.add_argument(\n",
    "    '--num-gpu-per-worker', type=str, help='num of workers',\n",
    "    default=2)\n",
    "parser.add_argument(\n",
    "    '--threads-per-worker', type=str, help='num of threads per worker',\n",
    "    default=4)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "def using_quantile_device_dmatrix(\n",
    "    client: Client,\n",
    "    project_id,\n",
    "    dataset_source,\n",
    "    table, \n",
    "    job_dir, \n",
    "    model_name):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"[INFO] ------ Importing '{dataset_source}/{table}' dataset from BigQuery\")\n",
    "    df = dask_bigquery.read_gbq(\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_source,\n",
    "        table_id=table)\n",
    "    df = dask_cudf.from_dask_dataframe(df)\n",
    "    print(f\"[INFO] ------ Import Done\")\n",
    "\n",
    "    print(\"Cleaning and standarizing dataset\")\n",
    "    df = df.dropna() \n",
    "\n",
    "    print(f\"[INFO] ------ Splitting dataset\")\n",
    "    df_train, df_eval = df.random_split([0.8, 0.2], random_state=123)\n",
    "    df_train_features= df_train.drop('Cover_Type', axis=1)\n",
    "    df_eval_features= df_eval.drop('Cover_Type', axis=1)\n",
    "    df_train_labels = df_train.pop('Cover_Type')\n",
    "    df_eval_labels = df_eval.pop('Cover_Type')\n",
    "\n",
    "    print(xgb.__version__)\n",
    "\n",
    "    print(\"[INFO] ------ Dataset for dask\")\n",
    "    dtrain = dxgb.DaskDeviceQuantileDMatrix(client, df_train_features, df_train_labels)\n",
    "    \n",
    "    print(\"[INFO] ------ Dataset for dask\")\n",
    "    dvalid = dxgb.DaskDeviceQuantileDMatrix(client, df_eval_features, df_eval_labels)\n",
    "    print(\"[INFO]: ------ QuantileDMatrix is formed in {} seconds ---\".format((time.time() - start_time)))\n",
    "\n",
    "    del df_train_features\n",
    "    del df_train_labels\n",
    "    del df_eval_features\n",
    "    del df_eval_labels\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"Training\")\n",
    "    output = xgb.dask.train(\n",
    "        client,\n",
    "        {\n",
    "            \"verbosity\": 2, \n",
    "            \"tree_method\": \"gpu_hist\", \n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"eval_metric\": [\"mlogloss\"],\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"gamma\": 0.9,\n",
    "            \"subsample\": 0.5,\n",
    "            \"max_depth\": 9,\n",
    "            \"num_class\": 8\n",
    "        },\n",
    "        dtrain,\n",
    "        num_boost_round=10,\n",
    "        evals=[(dvalid, \"valid1\")],\n",
    "        early_stopping_rounds=5\n",
    "    ) \n",
    "    print(\"[INFO]: ------ Training is completed in {} seconds ---\".format((time.time() - start_time)))\n",
    "\n",
    "    # Saving models and exporting performance metrics\n",
    "    \n",
    "    df_eval_metrics = pd.DataFrame(output[\"history\"][\"valid1\"])\n",
    "    model = output[\"booster\"]\n",
    "    best_model = model[: model.best_iteration]\n",
    "    print(f\"[INFO] ------ Best model: {best_model}\")\n",
    "    temp_dir = \"/tmp/xgboost\"\n",
    "    os.mkdir(temp_dir)\n",
    "    print(job_dir)\n",
    "    best_model.save_model(\"{}/{}\".format(temp_dir, model_name))\n",
    "    df_eval_metrics.to_json(\"{}/all_results.json\".format(temp_dir))\n",
    "\n",
    "    utils.save_model(args)\n",
    "\n",
    "def get_scheduler_info():\n",
    "    scheduler_ip =  subprocess.check_output(['hostname','--all-ip-addresses'])\n",
    "    scheduler_ip = scheduler_ip.decode('UTF-8').split()[0]\n",
    "    scheduler_port = '8786'\n",
    "    scheduler_uri = '{}:{}'.format(scheduler_ip, scheduler_port)\n",
    "    return scheduler_ip, scheduler_uri\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"[INFO] ------ Creating dask cluster\")\n",
    "    \n",
    "    sched_ip, sched_uri = get_scheduler_info()\n",
    "    \n",
    "    print(f\"[INFO] ------ Sched_ip and Sched_uri, {sched_ip}, {sched_uri}\")\n",
    "\n",
    "    print(\"[INFO]: ------ LocalCUDACluster is being formed \")\n",
    "    \n",
    "    with LocalCUDACluster(\n",
    "        ip=sched_ip,\n",
    "        n_workers=int(args.num_gpu_per_worker), \n",
    "        threads_per_worker=int(args.threads_per_worker) \n",
    "    ) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            print('[INFO]: ------ Calling main function ')\n",
    "            using_quantile_device_dmatrix(\n",
    "                client, \n",
    "                project_id=args.project_id,\n",
    "                dataset_source=args.bq_dataset,\n",
    "                table=args.bq_table, \n",
    "                job_dir=args.job_dir, \n",
    "                model_name=args.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defad8e8-c925-4e38-8545-20bff43bb3bd",
   "metadata": {},
   "source": [
    "## Wrapping Code (Custom Container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ee3403c9-df5c-44dd-b3ad-af71a7ff01de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training/Dockerfile\n",
    "\n",
    "FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
    "\n",
    "RUN pip install google.cloud[storage] \\\n",
    "  && pip install gcsfs \\\n",
    "  && pip install pandas \\\n",
    "  && pip install dask-bigquery\n",
    "\n",
    "COPY trainer trainer/\n",
    "\n",
    "ENTRYPOINT [\"python\", \"trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76450e-37a6-4f19-983b-8b49e0394fdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create: Repositories and Push Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3daff53-b1b1-4912-a584-f1a15fe6207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $TRAINING_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d878b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/4 : FROM rapidsai/rapidsai-nightly:22.04-cuda11.2-base-ubuntu20.04-py3.9\n",
      " ---> 6f5057ed56a0\n",
      "Step 2/4 : RUN pip install google.cloud[storage]   && pip install gcsfs   && pip install pandas   && pip install dask-bigquery\n",
      " ---> Using cache\n",
      " ---> 7d59643cd238\n",
      "Step 3/4 : COPY trainer trainer/\n",
      " ---> Using cache\n",
      " ---> bc35036cc99b\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"trainer/task.py\"]\n",
      " ---> Using cache\n",
      " ---> ca223fdbfa25\n",
      "Successfully built ca223fdbfa25\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_xgb_gpu:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $IMAGE_URI training/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c880026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/trainings/train_xgb_gpu]\n",
      "\n",
      "\u001b[1Bb1281643: Preparing \n",
      "\u001b[1Bdc3974b1: Preparing \n",
      "\u001b[1Bd20a3982: Preparing \n",
      "\u001b[1B177f7dba: Preparing \n",
      "\u001b[1Be2396578: Preparing \n",
      "\u001b[1B749f56f5: Preparing \n",
      "\u001b[1Ba57855d6: Preparing \n",
      "\u001b[1Beca41527: Preparing \n",
      "\u001b[1B32e4a10b: Preparing \n",
      "\u001b[1Bc89280f3: Preparing \n",
      "\u001b[1Bd1e3350d: Preparing \n",
      "\u001b[1B8e8f7e67: Preparing \n",
      "\u001b[2B8e8f7e67: Layer already exists \u001b[8A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:ce56f29d66c4aa991df5e20e66bff0c4080f14765cc6d4f9bac2235588b8b49e size: 3064\n"
     ]
    }
   ],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128fb06-e3d4-4775-89ee-1a09d5c33dd3",
   "metadata": {},
   "source": [
    "## Create: Vertex Pipe Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c1afce7-600e-43f6-90e4-ddd1be60d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Output, Model, Metrics, Model)\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "    ],\n",
    ")\n",
    "def get_train_job_details(\n",
    "    model_dir: str,\n",
    "    model_display_name: str,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    eval_metric_key: str\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    metrics_uri = \"{}/model/xgboost/all_results.json\".format(model_dir)\n",
    "    metrics_df = pd.read_json(metrics_uri, typ=\"series\")\n",
    "    for k,v in metrics_df.items():\n",
    "        logging.info(f\"    {k} -> {v}\")\n",
    "        metrics.log_metric(k, min(v.values()))\n",
    "        \n",
    "    eval_metric = (min(metrics_df[eval_metric_key].values()) if eval_metric_key in metrics_df.keys() else None)\n",
    "    model.metadata[eval_metric_key] = eval_metric\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\"])\n",
    "    \n",
    "    return outputs(eval_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adaf9b-4066-44a0-b696-2e9f7439f1bb",
   "metadata": {},
   "source": [
    "## Create: Prediction Custom Container Vertex|Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5a3087f0-3e36-4e56-b073-60f930cf9161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/app.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from flask import Flask, request, Response, jsonify\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "\n",
    "# Model Download from gcs\n",
    "\n",
    "fname = \"model.json\"\n",
    "\n",
    "with open(fname, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{fname}\", model\n",
    "    )\n",
    "\n",
    "# Loading model\n",
    "print(\"[INFO] ------ Loading model from: {}\".format(fname))\n",
    "model = xgb.Booster(model_file=fname)\n",
    "\n",
    "# Creation of the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Flask route for Liveness checks\n",
    "@app.route(os.environ['AIP_HEALTH_ROUTE'])\n",
    "def isalive():\n",
    "    status_code = Response(status=200)\n",
    "    return status_code\n",
    "\n",
    "# Flask route for predictions\n",
    "@app.route(os.environ['AIP_PREDICT_ROUTE'],methods=['GET','POST'])\n",
    "def prediction():\n",
    "    _features = ['Id','Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "                          'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', \n",
    "                          'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9',\n",
    "                          'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19', \n",
    "                          'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29',\n",
    "                          'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
    "    data = request.get_json(silent=True, force=True)\n",
    "    dmf = xgb.DMatrix(pd.DataFrame(data[\"instances\"], columns=_features))\n",
    "    response = pd.DataFrame(model.predict(dmf))\n",
    "    logging.info(f\"Response: {response}\")\n",
    "    return jsonify({\"Cover Type\": str(response.idxmax(axis=1)[0])})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa60ef-8088-447b-914f-812294877734",
   "metadata": {},
   "source": [
    "### Preparing Docker Container Declarative Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "086026cc-fff9-454a-8608-cc413d222701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/requirements.txt\n",
    "\n",
    "google-cloud-storage\n",
    "numpy\n",
    "pandas\n",
    "flask\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "05bfdf9f-f5f8-4e6e-bb1d-ae4dc335a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prediction/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile prediction/Dockerfile\n",
    "\n",
    "FROM python:3.7-buster\n",
    "\n",
    "RUN mkdir my-model\n",
    "\n",
    "COPY app.py ./app.py\n",
    "COPY requirements.txt ./requirements.txt\n",
    "RUN pip install -r requirements.txt \n",
    "\n",
    "# Flask Env Variable\n",
    "ENV FLASK_APP=app\n",
    "\n",
    "# Expose port 8080\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD flask run --host=0.0.0.0 --port=8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725956dc-2ee0-43cd-96bb-1f64796ddc44",
   "metadata": {},
   "source": [
    "### Push Container Image to Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae37a7f8-8eb6-4066-867f-d2c868cdd94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $PREDICTION_REPOSITORY --location $REGION --repository-format docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "34444603-e3ae-4d4a-85ca-ab8e7017fb15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 2.5 KiB before compression.\n",
      "Uploading tarball of [prediction/.] to [gs://jchavezar-demo_cloudbuild/source/1660308785.521859-49460976c99e4cd1aa9bd6dc9d31aa5e.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/c8b89412-343e-41ca-b749-72234d628fc0].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/c8b89412-343e-41ca-b749-72234d628fc0?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c8b89412-343e-41ca-b749-72234d628fc0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1660308785.521859-49460976c99e4cd1aa9bd6dc9d31aa5e.tgz#1660308786181606\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1660308785.521859-49460976c99e4cd1aa9bd6dc9d31aa5e.tgz#1660308786181606...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/8 : FROM python:3.7-buster\n",
      "3.7-buster: Pulling from library/python\n",
      "7e6a53d1988f: Already exists\n",
      "4fe4e1c58b4a: Pulling fs layer\n",
      "cc915d298757: Pulling fs layer\n",
      "f08b88f29371: Pulling fs layer\n",
      "28bcb226aca8: Pulling fs layer\n",
      "40e111dacc82: Pulling fs layer\n",
      "b25055bda02d: Pulling fs layer\n",
      "1650e1d4c2e0: Pulling fs layer\n",
      "14e3fa8854fc: Pulling fs layer\n",
      "28bcb226aca8: Waiting\n",
      "40e111dacc82: Waiting\n",
      "b25055bda02d: Waiting\n",
      "1650e1d4c2e0: Waiting\n",
      "14e3fa8854fc: Waiting\n",
      "4fe4e1c58b4a: Verifying Checksum\n",
      "4fe4e1c58b4a: Download complete\n",
      "cc915d298757: Verifying Checksum\n",
      "cc915d298757: Download complete\n",
      "40e111dacc82: Verifying Checksum\n",
      "40e111dacc82: Download complete\n",
      "f08b88f29371: Verifying Checksum\n",
      "f08b88f29371: Download complete\n",
      "b25055bda02d: Verifying Checksum\n",
      "b25055bda02d: Download complete\n",
      "1650e1d4c2e0: Verifying Checksum\n",
      "1650e1d4c2e0: Download complete\n",
      "14e3fa8854fc: Verifying Checksum\n",
      "14e3fa8854fc: Download complete\n",
      "4fe4e1c58b4a: Pull complete\n",
      "cc915d298757: Pull complete\n",
      "28bcb226aca8: Verifying Checksum\n",
      "28bcb226aca8: Download complete\n",
      "f08b88f29371: Pull complete\n",
      "28bcb226aca8: Pull complete\n",
      "40e111dacc82: Pull complete\n",
      "b25055bda02d: Pull complete\n",
      "1650e1d4c2e0: Pull complete\n",
      "14e3fa8854fc: Pull complete\n",
      "Digest: sha256:e09d86f517517a90d0d2e3263e02ecd6a493844ac6263748f02bf57d07ef7181\n",
      "Status: Downloaded newer image for python:3.7-buster\n",
      " ---> 4275b277d707\n",
      "Step 2/8 : RUN mkdir my-model\n",
      " ---> Running in 0a7b565b53b8\n",
      "Removing intermediate container 0a7b565b53b8\n",
      " ---> ca0e0aa23edd\n",
      "Step 3/8 : COPY app.py ./app.py\n",
      " ---> 2e907ab1fda8\n",
      "Step 4/8 : COPY requirements.txt ./requirements.txt\n",
      " ---> 6e0f66d2d81c\n",
      "Step 5/8 : RUN pip install -r requirements.txt\n",
      " ---> Running in c32dd7011d18\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 KB 12.1 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 32.5 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 51.4 MB/s eta 0:00:00\n",
      "Collecting flask\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.5/101.5 KB 17.4 MB/s eta 0:00:00\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.9/192.9 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 KB 10.6 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.10.0-py2.py3-none-any.whl (167 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.2/167.2 KB 25.7 MB/s eta 0:00:00\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 KB 17.0 MB/s eta 0:00:00\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.3.3-py2.py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.9/76.9 KB 13.2 MB/s eta 0:00:00\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.2-py2.py3-none-any.whl (504 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 504.7/504.7 KB 45.2 MB/s eta 0:00:00\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 KB 31.5 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 KB 20.8 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata>=3.6.0\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 KB 30.4 MB/s eta 0:00:00\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 16.9 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 22.7 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.7/211.7 KB 29.1 MB/s eta 0:00:00\n",
      "Collecting protobuf<5.0.0dev,>=3.15.0\n",
      "  Downloading protobuf-4.21.5-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.4/408.4 KB 42.6 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting six>=1.9.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 24.1 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 KB 11.2 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.2/160.2 KB 24.8 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.9/139.9 KB 22.1 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 14.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, pyasn1, zipp, urllib3, typing-extensions, six, rsa, pyasn1-modules, protobuf, numpy, MarkupSafe, itsdangerous, idna, google-crc32c, charset-normalizer, certifi, cachetools, Werkzeug, scipy, requests, python-dateutil, Jinja2, importlib-metadata, googleapis-common-protos, google-resumable-media, google-auth, xgboost, pandas, google-api-core, click, google-cloud-core, flask, google-cloud-storage\n",
      "Successfully installed Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.0 click-8.1.3 flask-2.2.2 google-api-core-2.8.2 google-auth-2.10.0 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.3.0 google-resumable-media-2.3.3 googleapis-common-protos-1.56.4 idna-3.3 importlib-metadata-4.12.0 itsdangerous-2.1.2 numpy-1.21.6 pandas-1.3.5 protobuf-4.21.5 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.2 pytz-2022.2 requests-2.28.1 rsa-4.9 scipy-1.7.3 six-1.16.0 typing-extensions-4.3.0 urllib3-1.26.11 xgboost-1.6.1 zipp-3.8.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container c32dd7011d18\n",
      " ---> f88accba93fa\n",
      "Step 6/8 : ENV FLASK_APP=app\n",
      " ---> Running in 30bdca929f6f\n",
      "Removing intermediate container 30bdca929f6f\n",
      " ---> 29ecbdb1aeeb\n",
      "Step 7/8 : EXPOSE 8080\n",
      " ---> Running in 5a278c8e243d\n",
      "Removing intermediate container 5a278c8e243d\n",
      " ---> 2d6d0f94fcbf\n",
      "Step 8/8 : CMD flask run --host=0.0.0.0 --port=8080\n",
      " ---> Running in d2fe4b9266de\n",
      "Removing intermediate container d2fe4b9266de\n",
      " ---> b3ccf86ab07e\n",
      "Successfully built b3ccf86ab07e\n",
      "Successfully tagged us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu]\n",
      "55f93b4e88fc: Preparing\n",
      "0161b39f3406: Preparing\n",
      "24c61f976ebd: Preparing\n",
      "e83ac63ea411: Preparing\n",
      "f495f8c37c83: Preparing\n",
      "aebaae58a42a: Preparing\n",
      "a31ded3cf4db: Preparing\n",
      "6e8d86d6d73d: Preparing\n",
      "1e90ee259bc5: Preparing\n",
      "91a7fe198f83: Preparing\n",
      "a657c4b93150: Preparing\n",
      "eea33d4fb826: Preparing\n",
      "37a609cfa10d: Preparing\n",
      "aebaae58a42a: Waiting\n",
      "a31ded3cf4db: Waiting\n",
      "6e8d86d6d73d: Waiting\n",
      "1e90ee259bc5: Waiting\n",
      "91a7fe198f83: Waiting\n",
      "a657c4b93150: Waiting\n",
      "eea33d4fb826: Waiting\n",
      "37a609cfa10d: Waiting\n",
      "0161b39f3406: Pushed\n",
      "e83ac63ea411: Pushed\n",
      "24c61f976ebd: Pushed\n",
      "aebaae58a42a: Layer already exists\n",
      "6e8d86d6d73d: Layer already exists\n",
      "a31ded3cf4db: Layer already exists\n",
      "1e90ee259bc5: Layer already exists\n",
      "91a7fe198f83: Layer already exists\n",
      "a657c4b93150: Layer already exists\n",
      "eea33d4fb826: Layer already exists\n",
      "37a609cfa10d: Layer already exists\n",
      "f495f8c37c83: Pushed\n",
      "55f93b4e88fc: Pushed\n",
      "latest: digest: sha256:4044c99ab02e40ca52a3cdb4708a9536abe96a471b45f5ed0aa2e3ebe3790181 size: 3053\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                                        STATUS\n",
      "c8b89412-343e-41ca-b749-72234d628fc0  2022-08-12T12:53:06+00:00  1M49S     gs://jchavezar-demo_cloudbuild/source/1660308785.521859-49460976c99e4cd1aa9bd6dc9d31aa5e.tgz  us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_xgb_cpu (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $PREDICTION_IMAGE_URI prediction/. --timeout 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f5e54-1647-42e2-b15c-235ae18b163e",
   "metadata": {},
   "source": [
    "## Create: Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "afac2a5d-347d-4d73-80a5-12dc9d08d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline, Condition\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp as custom_job\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "\n",
    "@pipeline(name='dask-gpu-1')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    custom_train_name: str,\n",
    "    region: str,\n",
    "    eval_acc_threshold: float,\n",
    "    eval_metric_key: str,\n",
    "    model_file_bucket: str,\n",
    "):\n",
    "    worker_pool_specs = [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-32\",\n",
    "            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "            \"accelerator_count\": 4\n",
    "        },\n",
    "        \"replica_count\": \"1\",\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"env\": [{\"name\": \"AIP_TRAINING_DATA_URI\", \"value\":'test'}],\n",
    "            \"command\": [\n",
    "                \"python\",\n",
    "                \"trainer/task.py\"\n",
    "            ],\n",
    "            \"args\": [\n",
    "                \"--project_id\", PROJECT_ID,\n",
    "                \"--bq_dataset\", BQ_SOURCE_DATASET,\n",
    "                \"--bq_table\", BQ_SOURCE_TABLE,\n",
    "                \"--model-name\", \"model.json\",\n",
    "                \"--num-gpu-per-worker\", NUM_GPU_TRAIN,\n",
    "                \"--threads-per-worker\", NUM_THREAD_PER_GPU\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    ]\n",
    "    train_with_cpu_task = custom_job(\n",
    "        project=project_id,\n",
    "        display_name=custom_train_name,\n",
    "        worker_pool_specs=worker_pool_specs,\n",
    "        base_output_directory=model_file_bucket\n",
    "    )\n",
    "    get_train_details_task = get_train_job_details(\n",
    "        model_dir=MODEL_FILE_BUCKET,\n",
    "        model_display_name=\"xgboost-dask\",\n",
    "        eval_metric_key=eval_metric_key, # mlogloss\n",
    "    ).after(train_with_cpu_task)\n",
    "    \n",
    "    with Condition(\n",
    "        get_train_details_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=\"gs://vtx-models/model/xgboost\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PREDICTION_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        custom_model_upload_job = gcc.ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=\"xgb-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_op)\n",
    "        \n",
    "        endpoint_create_job = gcc.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "        \n",
    "        custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "            model=custom_model_upload_job.outputs[\"model\"],\n",
    "            endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=\"xgboost_model_end\",\n",
    "            traffic_split={\"0\":\"100\"},\n",
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8c3de-97f0-410e-8293-cd3d77bf70f1",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2fa4335c-36f7-438e-9d46-c22ab5b1e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='dask_cpu.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572bb69-8054-46bd-939b-252ee0b3b7ec",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c998a74-c573-4732-bc80-6ab24877e38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220812085500\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/dask-gpu-1-20220812085500')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dask-gpu-1-20220812085500?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"dask_cpu\",\n",
    "    template_path=\"dask_cpu.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'custom_train_name': CUSTOM_TRAIN_NAME,\n",
    "        'region': REGION,\n",
    "        'eval_acc_threshold': 0.5,\n",
    "        'eval_metric_key': 'mlogloss', # mlogloss\n",
    "        'model_file_bucket': MODEL_FILE_BUCKET,\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit(service_account='vtx-pipe@jchavezar-demo.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6daa2a",
   "metadata": {},
   "source": [
    "![](images/vertex-pipe-gpu.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe07e0b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
