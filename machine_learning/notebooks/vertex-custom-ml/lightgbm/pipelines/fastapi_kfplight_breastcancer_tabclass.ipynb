{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Demostrate AI capabilities using lightGBM framework, Vertex Custom Training Containers, Fast API as webserver (Prediction Custom Containers) in a Vertex Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URI = 'gs://vtx-datasets-public/breast_cancer_data.csv'\n",
    "PIPELINE_ROOT_PATH = 'gs://vtx-root-path'\n",
    "PROJECT_ID= 'jchavezar-demo'\n",
    "MODELS_URI = 'gs://vtx-models/lightgbm'\n",
    "PRED_IMAGE_URI = 'us-central1-docker.pkg.dev/jchavezar-demo/predictions/pred_lightgbm_cpu:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataset component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Output, Artifact)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"sklearn\"\n",
    "        ]\n",
    ")\n",
    "def get_data(\n",
    "    datasource: str,\n",
    "    dataset_xtrain: Output[Artifact],\n",
    "    dataset_ytrain: Output[Artifact],\n",
    "    dataset_xtest: Output[Artifact],\n",
    "    dataset_ytest: Output[Artifact]\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(datasource)\n",
    "    X = df[['mean_radius','mean_texture','mean_perimeter','mean_area','mean_smoothness']]\n",
    "    y = df['diagnosis']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    X_train.to_csv(dataset_xtrain.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    X_test.to_csv(dataset_xtest.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    y_train.to_csv(dataset_ytrain.path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "    y_test.to_csv(dataset_ytest.path + \".csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (Input)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"lightgbm\",\n",
    "        \"google-cloud-storage\"]\n",
    ")\n",
    "def train(\n",
    "    project_id: str,\n",
    "    dataset_xtrain: Input[Artifact],\n",
    "    dataset_ytrain: Input[Artifact],\n",
    "    model_uri: str\n",
    "    ):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import lightgbm as lgb\n",
    "    from google.cloud import storage\n",
    "\n",
    "    X_train = pd.read_csv(dataset_xtrain.path+\".csv\")\n",
    "    y_train = pd.read_csv(dataset_ytrain.path+\".csv\").diagnosis\n",
    "    clf = lgb.LGBMClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    file_name = \"/tmp/model.pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(clf, file)\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    _bucket = model_uri.split('/')[2]\n",
    "    _suffix = \"/\".join(model_uri.split('/')[3:]).rstrip(\"/\")\n",
    "    bucket = storage_client.get_bucket(_bucket)\n",
    "    print(bucket)\n",
    "    print(_suffix)\n",
    "    blob = bucket.blob(f'{_suffix}/model.pkl')\n",
    "    blob.upload_from_filename('/tmp/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import ClassificationMetrics, Metrics\n",
    "from typing import NamedTuple\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas\",\n",
    "        \"gcsfs\",\n",
    "        \"lightgbm\",\n",
    "        \"sklearn\",\n",
    "        \"google-cloud-storage\"\n",
    "        ]\n",
    ")\n",
    "def evaluate_model(\n",
    "    project_id: str,\n",
    "    dataset_xtest: Input[Artifact],\n",
    "    dataset_ytest: Input[Artifact],\n",
    "    model_uri: str,\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    kpi: Output[Metrics]\n",
    "    ) -> NamedTuple(\n",
    "    \"Outputs\", [(\"eval_metric\", float)]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from google.cloud import storage\n",
    "    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "    from collections import namedtuple\n",
    "\n",
    "    X_test = pd.read_csv(dataset_xtest.path+\".csv\")\n",
    "    y_test = pd.read_csv(dataset_ytest.path+\".csv\")\n",
    "\n",
    "    # Load Model File\n",
    "\n",
    "    file_name = '/tmp/model.pkl'\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    _bucket = model_uri.split('/')[2]\n",
    "    _suffix = \"/\".join(model_uri.split('/')[3:]).rstrip(\"/\")\n",
    "    bucket = storage_client.get_bucket(_bucket)\n",
    "    blob = bucket.blob(f'{_suffix}/model.pkl')\n",
    "    blob.download_to_filename(file_name)\n",
    "\n",
    "    with open(file_name, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy))\n",
    "\n",
    "    y_scores = model.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "        y_true=y_test.to_numpy(), \n",
    "        y_score=y_scores, \n",
    "        pos_label=True)\n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"False\", \"True\"],\n",
    "        confusion_matrix(\n",
    "            y_test, y_pred\n",
    "        ).tolist(),\n",
    "    )\n",
    "\n",
    "    kpi.log_metric(\"accuracy\", float(accuracy))\n",
    "    outputs = namedtuple(\"Outputs\", [\"eval_metric\"])\n",
    "\n",
    "    return outputs(float(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Prediction Server (FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -fr custom_6\n",
    "!mkdir custom_6\n",
    "!mkdir custom_6/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_6/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_6/app/main.py\n",
    "\n",
    "from google.cloud import storage\n",
    "from fastapi import Request, FastAPI\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "x=os.environ['AIP_STORAGE_URI']\n",
    "print(f'[INFO] ------ {x}', file=sys.stderr)\n",
    "\n",
    "# Loading Model File\n",
    "\n",
    "file_name = 'model.pkl'\n",
    "client = storage.Client(project=os.environ['PROJECT_ID'])\n",
    "with open(file_name, \"wb\") as model:\n",
    "    client.download_blob_to_file(\n",
    "        f\"{os.environ['AIP_STORAGE_URI']}/{file_name}\", model\n",
    "    )\n",
    "with open(file_name, 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Webserver methods\n",
    "\n",
    "@app.get('/')\n",
    "def get_root():\n",
    "    return {'message': 'Welcome to Breast Cancer Prediction'}\n",
    "@app.get('/health_check')\n",
    "def health():\n",
    "    return 200\n",
    "if os.environ.get('AIP_PREDICT_ROUTE') is not None:\n",
    "    method = os.environ['AIP_PREDICT_ROUTE']\n",
    "else:\n",
    "    method = '/predict'\n",
    "print(method)\n",
    "@app.post(method)\n",
    "async def predict(request: Request):\n",
    "    print(\"----------------- PREDICTING -----------------\")\n",
    "    body = await request.json()\n",
    "    instances = body[\"instances\"]\n",
    "    outputs = model.predict(instances)\n",
    "    print(f'[INFO] ------ {outputs}, {type(outputs)}', file=sys.stderr)\n",
    "    response = outputs.tolist()\n",
    "    print(\"----------------- OUTPUTS -----------------\")\n",
    "    return {\"predictions\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_6/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_6/Dockerfile\n",
    "\n",
    "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
    "\n",
    "COPY app /app\n",
    "WORKDIR /app\n",
    "RUN pip install joblib google-cloud-storage lightgbm\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "\n",
    "EXPOSE 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit -t $PRED_IMAGE_URI custom_6/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline, Condition\n",
    "from kfp.v2.components import importer_node\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "@pipeline(name='lightgbm-light')\n",
    "def pipeline(\n",
    "    datasource: str,\n",
    "    project_id: str,\n",
    "    model_uri: str,\n",
    "    eval_acc_threshold: float,\n",
    "    ):\n",
    "    get_data_task = get_data(datasource)\n",
    "    train_task = train(\n",
    "        project_id,\n",
    "        get_data_task.outputs[\"dataset_xtrain\"], \n",
    "        get_data_task.outputs[\"dataset_ytrain\"],\n",
    "        model_uri,\n",
    "        )\n",
    "    eval_task = evaluate_model(\n",
    "        project_id,\n",
    "        get_data_task.outputs[\"dataset_xtest\"], \n",
    "        get_data_task.outputs[\"dataset_ytest\"],\n",
    "        model_uri).after(train_task)\n",
    "\n",
    "\n",
    "    with Condition(\n",
    "        eval_task.outputs[\"eval_metric\"] > eval_acc_threshold,\n",
    "        name=\"model-deploy-decision\",\n",
    "    ):\n",
    "        import_unmanaged_model_op = importer_node.importer(\n",
    "            artifact_uri=\"gs://vtx-models/lightgbm\",\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": PRED_IMAGE_URI,\n",
    "                    \"env\": [\n",
    "                        {\n",
    "                            \"name\": \"PROJECT_ID\",\n",
    "                            \"value\": PROJECT_ID},\n",
    "                    ],\n",
    "                    \"predictRoute\": \"/predict\",\n",
    "                    \"healthRoute\": \"/health_check\",\n",
    "                    \"ports\": [\n",
    "                        {\n",
    "                            \"containerPort\": 8080\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        custom_model_upload_job = gcc.ModelUploadOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"lightgbm-model\",\n",
    "            unmanaged_container_model=import_unmanaged_model_op.outputs[\"artifact\"],\n",
    "            ).after(import_unmanaged_model_op)\n",
    "\n",
    "        endpoint_create_job = gcc.EndpointCreateOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"pipelines-created-endpoint\",\n",
    "        )\n",
    "            \n",
    "        custom_model_deploy_job = (gcc.ModelDeployOp(\n",
    "            model=custom_model_upload_job.outputs[\"model\"],\n",
    "            endpoint=endpoint_create_job.outputs[\"endpoint\"],\n",
    "            deployed_model_display_name=\"lightgbm_model_end\",\n",
    "            traffic_split={\"0\":\"100\"},\n",
    "            dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1\n",
    "        )).set_caching_options(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='lightgbm-light.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/lightgbm-light-20220815085002\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/lightgbm-light-20220815085002')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/lightgbm-light-20220815085002?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"lightgbm-light\",\n",
    "    template_path=\"lightgbm-light.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        \"datasource\": DATASET_URI,\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"model_uri\": MODELS_URI,\n",
    "        \"eval_acc_threshold\": 0.5,\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit(service_account='vtx-pipe@jchavezar-demo.iam.gserviceaccount.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/vertex-pipe-lightgbm-cpu.png)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "61f821d259e852bb8dda541b337ba40be66c16e8431d3e97d4d2c7f8d54d4461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
