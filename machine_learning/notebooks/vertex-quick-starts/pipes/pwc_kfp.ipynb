{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c79ee092-48f0-4634-a073-28150e2a26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, Output, Artifact, Input\n",
    "\n",
    "@component(packages_to_install=[\"pandas\", \"gcsfs\"])\n",
    "def prepare_data(dataset_train: str, dataset_test: str, train_uri: Output[Artifact], test_uri: Output[Artifact]):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    train_dataset = pd.read_csv(dataset_train)\n",
    "    test_dataset = pd.read_csv(dataset_test)\n",
    "    \n",
    "    train_stats = train_dataset.describe()\n",
    "    train_stats.pop(\"MPG\")\n",
    "    train_stats = train_stats.transpose()\n",
    "    \n",
    "    train_labels = train_dataset.pop('MPG')\n",
    "    test_labels = test_dataset.pop('MPG')\n",
    "    \n",
    "    def norm(x):\n",
    "        return (x - train_stats['mean']) / train_stats['std']\n",
    "    normed_train_data = pd.concat([norm(train_dataset), train_labels], axis=1)\n",
    "    normed_test_data = pd.concat([norm(test_dataset), test_labels], axis=1)\n",
    "    \n",
    "    normed_train_data.to_csv(train_uri.path, index=False)\n",
    "    normed_test_data.to_csv(test_uri.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d1cb64e-e5f6-4dfe-962d-cb50a060a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"tensorflow\", \"pandas\"])\n",
    "def train_component(train_uri: Input[Artifact], test_uri: Input[Artifact], model_uri: str):\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    ## Load data\n",
    "    \n",
    "    normed_train_data = pd.read_csv(train_uri.path)\n",
    "    normed_test_data = pd.read_csv(test_uri.path)\n",
    "    \n",
    "    train_labels = normed_train_data.pop('MPG')\n",
    "    test_labels = normed_test_data.pop('MPG')\n",
    "    \n",
    "    print(normed_train_data)\n",
    "    \n",
    "    ## Building Model Function (neural network with 2 hidden layers, 64 neurons each, relu as activation layer.\n",
    "    \n",
    "    def build_model():\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(normed_train_data.keys())]),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['mae', 'mse'])\n",
    "        return model  \n",
    "    \n",
    "    model = build_model()\n",
    "    EPOCHS = 1000\n",
    "\n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    early_history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, callbacks=[early_stop])\n",
    "    \n",
    "    # Export model and save to GCS\n",
    "    model.save(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1472ee10-ae00-4423-8cce-d44d578a2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline\n",
    "from kfp.v2.dsl import importer\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "@pipeline(name=\"kfp-tf-mpg\")\n",
    "def pipeline(\n",
    "    model_uri: str,\n",
    "    project: str\n",
    "):\n",
    "    prepare_data_job = prepare_data(\n",
    "        dataset_train=\"gs://vtx-datasets/public/mpg/train.csv\",\n",
    "        dataset_test=\"gs://vtx-datasets/public/mpg/test.csv\"\n",
    "    )\n",
    "    train_component_job = train_component(\n",
    "        train_uri = prepare_data_job.outputs[\"train_uri\"], \n",
    "        test_uri = prepare_data_job.outputs[\"test_uri\"],\n",
    "        model_uri = model_uri\n",
    "    )\n",
    "    import_unmanaged_model_task = importer(\n",
    "        artifact_uri = model_uri,\n",
    "        artifact_class = artifact_types.UnmanagedContainerModel,\n",
    "        metadata = {\n",
    "            \"containerSpec\": {\n",
    "                \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-10:latest\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    model_upload_job = gcc.ModelUploadOp(\n",
    "        project = project,\n",
    "        display_name = \"kfp-mpg-model\",\n",
    "        unmanaged_container_model = import_unmanaged_model_task.outputs[\"artifact\"]\n",
    "    )\n",
    "    model_upload_job.after(train_component_job)\n",
    "    create_endpoint_op = gcc.EndpointCreateOp(\n",
    "        project = project,\n",
    "        display_name = \"kfp-mpg-endpoint\",\n",
    "    )\n",
    "    model_deploy_op = gcc.ModelDeployOp(\n",
    "        model = model_upload_job.outputs[\"model\"],\n",
    "        endpoint = create_endpoint_op.outputs['endpoint'],\n",
    "        dedicated_resources_machine_type = 'n1-standard-4',\n",
    "        dedicated_resources_min_replica_count = 1,\n",
    "        dedicated_resources_max_replica_count = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "46930eb0-6cac-478f-b2f3-9a5a114e86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='kfp-tf-mpg.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4411a46-f0e7-4d72-9a9f-9f31519a01ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/kfp-tf-mpg-20221122131032\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/kfp-tf-mpg-20221122131032')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kfp-tf-mpg-20221122131032?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"kfp-v2\",\n",
    "    template_path=\"kfp-tf-mpg.json\",\n",
    "    pipeline_root=\"gs://vtx-pipe-root/\",\n",
    "    parameter_values={\n",
    "        \"model_uri\": \"gs://vtx-models/mpg-kfp/\",\n",
    "        \"project\": \"jchavezar-demo\"\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e54b-84e4-4250-8a26-657b8cf10249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
