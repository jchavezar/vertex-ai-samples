{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93418941-0ac0-4953-a0d7-d1bb4b72434c",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f947c01b-dbef-4a99-845b-44fb896f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "TRAINING_IMAGE_URI = f'gcr.io/{PROJECT_ID}/demos-train-aws:latest'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'\n",
    "DATASET_DIR = 's3://gml-datasets/fraud_detection.csv'\n",
    "MODEL_DIR = 'gs://vtx-models/aws'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc827c5-cafa-403f-a769-db6c3423d3ec",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539aaeec-025a-413b-85e2-a6c84b2aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"1_custom_train_job\"):\n",
    "    os.makedirs(\"1_custom_train_job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a2f7d-e852-428c-9488-146ac72fa66a",
   "metadata": {},
   "source": [
    "## Create Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbcd2d4-2e66-4979-82ba-1f19cffb190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/main.py\n",
    "\n",
    "# Extracting information from AWS S3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.environ[\"FILE_URI\"],\n",
    "    storage_options={\n",
    "        \"key\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        \"secret\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=1)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "X_train = train_df.iloc[:,:-1]\n",
    "X_test = test_df.iloc[:,:-1]\n",
    "y_train = train_df['Class'].astype(np.float32)\n",
    "y_test = test_df['Class'].astype(np.float32)\n",
    "\n",
    "# Standarization\n",
    "\n",
    "X_train_norm = (X_train-X_train.mean())/X_train.std()\n",
    "X_test_norm = (X_test-X_test.mean())/X_test.std()\n",
    "        \n",
    "# Model\n",
    "\n",
    "def create_model(my_learning_rate, ds_length):\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Dense(16, activation='relu', input_shape=[ds_length]),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "        \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(x, y, model, epochs,\n",
    "                batch_size=None, shuffle=True):\n",
    "    history = model.fit(x=x, y=y, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "ds_length = len(X_train_norm.keys())\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, ds_length)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(X_train_norm, y_train, my_model, epochs, \n",
    "                           batch_size)\n",
    "\n",
    "my_model.save(os.environ['AIP_MODEL_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e667f-004d-4bb3-9eec-18a0bd642d56",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5839f34a-7d36-470a-ba84-5f1b41dfac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/Dockerfile\n",
    "\n",
    "FROM tensorflow/tensorflow\n",
    "\n",
    "RUN pip install boto3 pandas s3fs\n",
    "RUN pip install --upgrade protobuf==3.20.0\n",
    "\n",
    "COPY main.py /main.py\n",
    "\n",
    "CMD [\"python\", \"/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b86159-88db-4a31-a11d-a09b97311114",
   "metadata": {},
   "source": [
    "## Create Docker Image with CloudBuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f82034e-165c-4d37-939a-bd6c76c5a109",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.5 KiB before compression.\n",
      "Uploading tarball of [1_custom_train_job/.] to [gs://jchavezar-demo_cloudbuild/source/1665073488.663336-e35748380d50498aac9941f25ed10284.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/7ec67e17-1856-4e08-8045-5643d5b13ec4].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/7ec67e17-1856-4e08-8045-5643d5b13ec4?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"7ec67e17-1856-4e08-8045-5643d5b13ec4\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1665073488.663336-e35748380d50498aac9941f25ed10284.tgz#1665073488885022\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1665073488.663336-e35748380d50498aac9941f25ed10284.tgz#1665073488885022...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM tensorflow/tensorflow\n",
      "latest: Pulling from tensorflow/tensorflow\n",
      "675920708c8b: Pulling fs layer\n",
      "645d09d18273: Pulling fs layer\n",
      "c0a0749639e0: Pulling fs layer\n",
      "a719c076b335: Pulling fs layer\n",
      "9fc4f7385e6e: Pulling fs layer\n",
      "71d88d514827: Pulling fs layer\n",
      "fbda67fc861f: Pulling fs layer\n",
      "b14946c4f6e6: Pulling fs layer\n",
      "9fc4f7385e6e: Waiting\n",
      "71d88d514827: Waiting\n",
      "fbda67fc861f: Waiting\n",
      "b14946c4f6e6: Waiting\n",
      "675920708c8b: Verifying Checksum\n",
      "675920708c8b: Download complete\n",
      "a719c076b335: Verifying Checksum\n",
      "a719c076b335: Download complete\n",
      "9fc4f7385e6e: Verifying Checksum\n",
      "9fc4f7385e6e: Download complete\n",
      "645d09d18273: Verifying Checksum\n",
      "645d09d18273: Download complete\n",
      "fbda67fc861f: Verifying Checksum\n",
      "fbda67fc861f: Download complete\n",
      "b14946c4f6e6: Verifying Checksum\n",
      "b14946c4f6e6: Download complete\n",
      "c0a0749639e0: Verifying Checksum\n",
      "c0a0749639e0: Download complete\n",
      "675920708c8b: Pull complete\n",
      "71d88d514827: Verifying Checksum\n",
      "71d88d514827: Download complete\n",
      "645d09d18273: Pull complete\n",
      "c0a0749639e0: Pull complete\n",
      "a719c076b335: Pull complete\n",
      "9fc4f7385e6e: Pull complete\n",
      "71d88d514827: Pull complete\n",
      "fbda67fc861f: Pull complete\n",
      "b14946c4f6e6: Pull complete\n",
      "Digest: sha256:7f9f23ce2473eb52d17fe1b465c79c3a3604047343e23acc036296f512071bc9\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:latest\n",
      " ---> 976c17ec6daa\n",
      "Step 2/5 : RUN pip install boto3 pandas s3fs\n",
      " ---> Running in 7d32ade8aede\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.87-py3-none-any.whl (132 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Collecting botocore<1.28.0,>=1.27.87\n",
      "  Downloading botocore-1.27.87-py3-none-any.whl (9.2 MB)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.4-py2.py3-none-any.whl (500 kB)\n",
      "Collecting aiobotocore~=2.4.0\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore<1.28.0,>=1.27.87->boto3) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.8/dist-packages (from aiobotocore~=2.4.0->s3fs) (1.14.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.1.1)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from aioitertools>=0.5.1->aiobotocore~=2.4.0->s3fs) (4.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.3)\n",
      "Installing collected packages: jmespath, python-dateutil, botocore, s3transfer, boto3, pytz, pandas, aioitertools, multidict, yarl, frozenlist, attrs, aiosignal, async-timeout, aiohttp, aiobotocore, fsspec, s3fs\n",
      "Successfully installed aiobotocore-2.4.0 aiohttp-3.8.3 aioitertools-0.11.0 aiosignal-1.2.0 async-timeout-4.0.2 attrs-22.1.0 boto3-1.24.87 botocore-1.27.87 frozenlist-1.3.1 fsspec-2022.8.2 jmespath-1.0.1 multidict-6.0.2 pandas-1.5.0 python-dateutil-2.8.2 pytz-2022.4 s3fs-2022.8.2 s3transfer-0.6.0 yarl-1.8.1\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you'll have botocore 1.27.87 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 7d32ade8aede\n",
      " ---> 1c321781c3a8\n",
      "Step 3/5 : RUN pip install --upgrade protobuf==3.20.0\n",
      " ---> Running in 48deac69c167\n",
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "Successfully installed protobuf-3.20.0\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow-cpu 2.10.0 requires protobuf<3.20,>=3.9.2, but you'll have protobuf 3.20.0 which is incompatible.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you'll have protobuf 3.20.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 48deac69c167\n",
      " ---> 40d6764cdf51\n",
      "Step 4/5 : COPY main.py /main.py\n",
      " ---> 3b37d1955d8c\n",
      "Step 5/5 : CMD [\"python\", \"/main.py\"]\n",
      " ---> Running in c196cd99c772\n",
      "Removing intermediate container c196cd99c772\n",
      " ---> 81bfb81d165d\n",
      "Successfully built 81bfb81d165d\n",
      "Successfully tagged gcr.io/jchavezar-demo/demos-train-aws:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jchavezar-demo/demos-train-aws:latest\n",
      "The push refers to repository [gcr.io/jchavezar-demo/demos-train-aws]\n",
      "c450331effb8: Preparing\n",
      "19a72ac2f105: Preparing\n",
      "55dc3cd6a564: Preparing\n",
      "dd0a82e2d36d: Preparing\n",
      "259c731efc4e: Preparing\n",
      "d573ca940b7f: Preparing\n",
      "df1276fb1511: Preparing\n",
      "d87a57459221: Preparing\n",
      "bc2da9ec8d67: Preparing\n",
      "e6bb44b23860: Preparing\n",
      "b40ed86654e5: Preparing\n",
      "d573ca940b7f: Waiting\n",
      "df1276fb1511: Waiting\n",
      "d87a57459221: Waiting\n",
      "bc2da9ec8d67: Waiting\n",
      "e6bb44b23860: Waiting\n",
      "b40ed86654e5: Waiting\n",
      "259c731efc4e: Layer already exists\n",
      "dd0a82e2d36d: Layer already exists\n",
      "d573ca940b7f: Layer already exists\n",
      "df1276fb1511: Layer already exists\n",
      "d87a57459221: Layer already exists\n",
      "bc2da9ec8d67: Layer already exists\n",
      "b40ed86654e5: Layer already exists\n",
      "e6bb44b23860: Layer already exists\n",
      "c450331effb8: Pushed\n",
      "19a72ac2f105: Pushed\n",
      "55dc3cd6a564: Pushed\n",
      "latest: digest: sha256:957c40d2223955d22faf11e5e40bed465d0b677e4b5111df5d2e93a30f6c811a size: 2631\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                           STATUS\n",
      "7ec67e17-1856-4e08-8045-5643d5b13ec4  2022-10-06T16:24:49+00:00  1M4S      gs://jchavezar-demo_cloudbuild/source/1665073488.663336-e35748380d50498aac9941f25ed10284.tgz  gcr.io/jchavezar-demo/demos-train-aws (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAINING_IMAGE_URI 1_custom_train_job/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2de95d-2a56-4320-9f88-472a99770954",
   "metadata": {},
   "source": [
    "## Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0045c865-ef6e-42d4-80a9-d1dfa327f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import env\n",
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machineSpec': {\n",
    "            'machineType': 'n1-standard-4',\n",
    "    },\n",
    "        'replicaCount': 1,\n",
    "        'containerSpec': {\n",
    "            'image_uri': TRAINING_IMAGE_URI,\n",
    "            'env': [\n",
    "                {\n",
    "                    'name': 'FILE_URI',\n",
    "                    'value': DATASET_DIR\n",
    "                },\n",
    "                {\n",
    "                    'name': 'AWS_ACCESS_KEY_ID',\n",
    "                    'value': env.AWS_ACCESS_KEY_ID\n",
    "                },\n",
    "                {\n",
    "                    'name': 'AWS_SECRET_ACCESS_KEY',\n",
    "                    'value': env.AWS_SECRET_ACCESS_KEY\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@pipeline(name='aws-gcp-test')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    model_dir: str,\n",
    "    serving_image_uri: str\n",
    "):\n",
    "    custom_train_task = CustomTrainingJobOp(\n",
    "        project=project_id,\n",
    "        display_name='custom_train_task',\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "        base_output_directory=model_dir\n",
    "    )\n",
    "    \n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=f'tf-kfp-prebuilt-model-upload-job',\n",
    "        artifact_uri=f'{model_dir}/model',\n",
    "        serving_container_image_uri=serving_image_uri,\n",
    "    ).after(custom_train_task)\n",
    "    \n",
    "    create_endpoint_task = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name='fraud_det'\n",
    "    ).after(model_upload_task)\n",
    "    \n",
    "    model_deploy_task = ModelDeployOp(\n",
    "        endpoint=create_endpoint_task.outputs[\"endpoint\"],\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_machine_type='n1-standard-4'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a94eb7-2b34-4ad1-88e7-f8ffd290e032",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236b2dff-6a3e-4d7a-9c27-6e714819cf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='aws_gcp_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea69d87-82fb-4a15-b856-29655e155874",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1d7eb3-d109-4867-89f7-3c806297211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/aws-gcp-test-20221006162737\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/aws-gcp-test-20221006162737')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/aws-gcp-test-20221006162737?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name='customjob-aws-gcp',\n",
    "    template_path='aws_gcp_test.json',\n",
    "    pipeline_root='gs://vtx-path-root',\n",
    "    parameter_values={\n",
    "        'project_id': 'jchavezar-demo',\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'serving_image_uri': SERVING_CONTAINER_IMAGE_URI\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc8d8bd-58f7-476c-86e4-0cf9be61a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vtx-models/aws/model/\n",
      "gs://vtx-models/aws/model/keras_metadata.pb\n",
      "gs://vtx-models/aws/model/saved_model.pb\n",
      "gs://vtx-models/aws/model/assets/\n",
      "gs://vtx-models/aws/model/variables/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://vtx-models/aws/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcb27b-589f-4535-aec2-89d19c44620b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "tf",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
