{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93418941-0ac0-4953-a0d7-d1bb4b72434c",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f947c01b-dbef-4a99-845b-44fb896f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "TRAINING_IMAGE_URI = f'gcr.io/{PROJECT_ID}/demos-train-aws:latest'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'\n",
    "DATASET_DIR = 's3://gml-datasets/fraud_detection.csv'\n",
    "MODEL_DIR = 'gs://vtx-models/aws/hpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc827c5-cafa-403f-a769-db6c3423d3ec",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "539aaeec-025a-413b-85e2-a6c84b2aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"1_custom_train_job\"):\n",
    "    os.makedirs(\"1_custom_train_job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a2f7d-e852-428c-9488-146ac72fa66a",
   "metadata": {},
   "source": [
    "## Create Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6dbcd2d4-2e66-4979-82ba-1f19cffb190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/main.py\n",
    "\n",
    "# Extracting information from AWS S3\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "def get_args():\n",
    "    '''Parses args. Must include all hyperparameters you want to tune.'''\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        required=True,\n",
    "        type=float,\n",
    "        help='learning rate')\n",
    "    parser.add_argument(\n",
    "        '--num_neurons_1',\n",
    "        required=True,\n",
    "        type=int,\n",
    "        help='number of units in first hidden layer')\n",
    "    parser.add_argument(\n",
    "        '--num_neurons_2',\n",
    "        required=True,\n",
    "        type=int,\n",
    "        help='number of units in second hidden layer')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def preprocess_data(dataset: str):\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        dataset,\n",
    "        storage_options={\n",
    "            \"key\": os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            \"secret\": os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    train_df = df.sample(frac=0.8, random_state=1)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.iloc[:,:-1]\n",
    "    X_test = test_df.iloc[:,:-1]\n",
    "    y_train = train_df['Class'].astype(np.float32)\n",
    "    y_test = test_df['Class'].astype(np.float32)\n",
    "    \n",
    "    # Standarization\n",
    "    \n",
    "    X_train_norm = (X_train-X_train.mean())/X_train.std()\n",
    "    X_test_norm = (X_test-X_test.mean())/X_test.std()\n",
    "    \n",
    "    return X_train_norm, y_train, X_test_norm, y_test \n",
    "    \n",
    "    # Model\n",
    "\n",
    "def create_model(\n",
    "    ds_length: int,\n",
    "    my_learning_rate: float, \n",
    "    nn_1: int, \n",
    "    nn_2: int):\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Dense(nn_1, activation='relu', input_shape=[ds_length]),\n",
    "        layers.Dense(nn_2, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "        \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "    x, \n",
    "    y, \n",
    "    model, \n",
    "    epochs,\n",
    "    batch_size=None, \n",
    "    shuffle=True\n",
    "):\n",
    "    history = model.fit(\n",
    "        x=x, \n",
    "        y=y, \n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, shuffle=shuffle)\n",
    "    \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # The following variables are the hyperparameters.\n",
    "    args = get_args()\n",
    "    print(args.learning_rate)\n",
    "    \n",
    "    X_train_norm, y_train, X_test_norm, y_test = preprocess_data(os.environ['FILE_URI'])\n",
    "    ds_length = len(X_train_norm.keys())\n",
    "    \n",
    "    # Establish the model's topography.\n",
    "    my_model = create_model(ds_length, args.learning_rate, args.num_neurons_1, args.num_neurons_2)\n",
    "    \n",
    "    # Train the model on the training set.\n",
    "    hist = train_model(X_train_norm, y_train, my_model, epochs, \n",
    "                           batch_size)\n",
    "    \n",
    "    # DEFINE METRIC\n",
    "    hp_metric = hist['accuracy'][0]\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=hp_metric,\n",
    "        global_step=epochs)\n",
    "    \n",
    "    import sys\n",
    "    \n",
    "    print(os.environ['AIP_MODEL_DIR'])\n",
    "    print(os.environ['AIP_MODEL_DIR'], file=sys.stderr)\n",
    "\n",
    "    my_model.save(os.environ['AIP_MODEL_DIR'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e667f-004d-4bb3-9eec-18a0bd642d56",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5839f34a-7d36-470a-ba84-5f1b41dfac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/Dockerfile\n",
    "\n",
    "FROM tensorflow/tensorflow\n",
    "\n",
    "RUN pip install boto3 pandas s3fs\n",
    "RUN pip install --upgrade protobuf==3.20.0\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "COPY main.py /main.py\n",
    "\n",
    "CMD [\"python\", \"/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b86159-88db-4a31-a11d-a09b97311114",
   "metadata": {},
   "source": [
    "## Create Docker Image with CloudBuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f82034e-165c-4d37-939a-bd6c76c5a109",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 4.0 KiB before compression.\n",
      "Uploading tarball of [1_custom_train_job/.] to [gs://jchavezar-demo_cloudbuild/source/1665083813.342888-8c64544d4ce64462b8f82fe08ded2489.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/06916014-e298-4213-be9e-012dde90c1b4].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/06916014-e298-4213-be9e-012dde90c1b4?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"06916014-e298-4213-be9e-012dde90c1b4\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1665083813.342888-8c64544d4ce64462b8f82fe08ded2489.tgz#1665083813565708\n",
      "Copying gs://jchavezar-demo_cloudbuild/source/1665083813.342888-8c64544d4ce64462b8f82fe08ded2489.tgz#1665083813565708...\n",
      "/ [1 files][  1.7 KiB/  1.7 KiB]                                                \n",
      "Operation completed over 1 objects/1.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/6 : FROM tensorflow/tensorflow\n",
      "latest: Pulling from tensorflow/tensorflow\n",
      "675920708c8b: Pulling fs layer\n",
      "645d09d18273: Pulling fs layer\n",
      "c0a0749639e0: Pulling fs layer\n",
      "a719c076b335: Pulling fs layer\n",
      "9fc4f7385e6e: Pulling fs layer\n",
      "71d88d514827: Pulling fs layer\n",
      "fbda67fc861f: Pulling fs layer\n",
      "b14946c4f6e6: Pulling fs layer\n",
      "a719c076b335: Waiting\n",
      "9fc4f7385e6e: Waiting\n",
      "71d88d514827: Waiting\n",
      "fbda67fc861f: Waiting\n",
      "b14946c4f6e6: Waiting\n",
      "675920708c8b: Verifying Checksum\n",
      "675920708c8b: Download complete\n",
      "645d09d18273: Verifying Checksum\n",
      "645d09d18273: Download complete\n",
      "9fc4f7385e6e: Verifying Checksum\n",
      "9fc4f7385e6e: Download complete\n",
      "a719c076b335: Verifying Checksum\n",
      "a719c076b335: Download complete\n",
      "fbda67fc861f: Verifying Checksum\n",
      "fbda67fc861f: Download complete\n",
      "b14946c4f6e6: Download complete\n",
      "c0a0749639e0: Verifying Checksum\n",
      "c0a0749639e0: Download complete\n",
      "675920708c8b: Pull complete\n",
      "71d88d514827: Verifying Checksum\n",
      "71d88d514827: Download complete\n",
      "645d09d18273: Pull complete\n",
      "c0a0749639e0: Pull complete\n",
      "a719c076b335: Pull complete\n",
      "9fc4f7385e6e: Pull complete\n",
      "71d88d514827: Pull complete\n",
      "fbda67fc861f: Pull complete\n",
      "b14946c4f6e6: Pull complete\n",
      "Digest: sha256:7f9f23ce2473eb52d17fe1b465c79c3a3604047343e23acc036296f512071bc9\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:latest\n",
      " ---> 976c17ec6daa\n",
      "Step 2/6 : RUN pip install boto3 pandas s3fs\n",
      " ---> Running in f0f675467530\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.87-py3-none-any.whl (132 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.28.0,>=1.27.87\n",
      "  Downloading botocore-1.27.87-py3-none-any.whl (9.2 MB)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.4-py2.py3-none-any.whl (500 kB)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting fsspec==2022.8.2\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting aiobotocore~=2.4.0\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore<1.28.0,>=1.27.87->boto3) (1.26.12)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.1.1)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.8/dist-packages (from aiobotocore~=2.4.0->s3fs) (1.14.1)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.3)\n",
      "Requirement already satisfied: typing_extensions>=4.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from aioitertools>=0.5.1->aiobotocore~=2.4.0->s3fs) (4.3.0)\n",
      "Installing collected packages: jmespath, python-dateutil, botocore, s3transfer, boto3, pytz, pandas, fsspec, frozenlist, aiosignal, multidict, async-timeout, attrs, yarl, aiohttp, aioitertools, aiobotocore, s3fs\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "aiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you'll have botocore 1.27.87 which is incompatible.\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.4.0 aiohttp-3.8.3 aioitertools-0.11.0 aiosignal-1.2.0 async-timeout-4.0.2 attrs-22.1.0 boto3-1.24.87 botocore-1.27.87 frozenlist-1.3.1 fsspec-2022.8.2 jmespath-1.0.1 multidict-6.0.2 pandas-1.5.0 python-dateutil-2.8.2 pytz-2022.4 s3fs-2022.8.2 s3transfer-0.6.0 yarl-1.8.1\n",
      "\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container f0f675467530\n",
      " ---> 1c362d703bd9\n",
      "Step 3/6 : RUN pip install --upgrade protobuf==3.20.0\n",
      " ---> Running in 5903d3b9b3c4\n",
      "Collecting protobuf==3.20.0\n",
      "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow-cpu 2.10.0 requires protobuf<3.20,>=3.9.2, but you'll have protobuf 3.20.0 which is incompatible.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you'll have protobuf 3.20.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.0\n",
      "\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 5903d3b9b3c4\n",
      " ---> 5337e46044f2\n",
      "Step 4/6 : RUN pip install cloudml-hypertune\n",
      " ---> Running in c24f622f4b04\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3972 sha256=f98aae36d43030eb52cf9af140d4bd519a749de3b11a9dadc4534ecb43eb4329\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/fb/3b/365271726c73d8bc0b5bf39ef0f5db5a9c75b2babe4fd67794\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container c24f622f4b04\n",
      " ---> d2f75187fea8\n",
      "Step 5/6 : COPY main.py /main.py\n",
      " ---> 4f3dc7d22d6a\n",
      "Step 6/6 : CMD [\"python\", \"/main.py\"]\n",
      " ---> Running in 5c046248dce3\n",
      "Removing intermediate container 5c046248dce3\n",
      " ---> 20fdbb7a0f4e\n",
      "Successfully built 20fdbb7a0f4e\n",
      "Successfully tagged gcr.io/jchavezar-demo/demos-train-aws:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jchavezar-demo/demos-train-aws:latest\n",
      "The push refers to repository [gcr.io/jchavezar-demo/demos-train-aws]\n",
      "cfd78c74f564: Preparing\n",
      "7e5ddf84103e: Preparing\n",
      "6ce2081dc396: Preparing\n",
      "22240422cf4a: Preparing\n",
      "dd0a82e2d36d: Preparing\n",
      "259c731efc4e: Preparing\n",
      "d573ca940b7f: Preparing\n",
      "df1276fb1511: Preparing\n",
      "d87a57459221: Preparing\n",
      "bc2da9ec8d67: Preparing\n",
      "e6bb44b23860: Preparing\n",
      "b40ed86654e5: Preparing\n",
      "d573ca940b7f: Waiting\n",
      "df1276fb1511: Waiting\n",
      "d87a57459221: Waiting\n",
      "bc2da9ec8d67: Waiting\n",
      "e6bb44b23860: Waiting\n",
      "b40ed86654e5: Waiting\n",
      "259c731efc4e: Waiting\n",
      "dd0a82e2d36d: Layer already exists\n",
      "259c731efc4e: Layer already exists\n",
      "d573ca940b7f: Layer already exists\n",
      "df1276fb1511: Layer already exists\n",
      "d87a57459221: Layer already exists\n",
      "bc2da9ec8d67: Layer already exists\n",
      "e6bb44b23860: Layer already exists\n",
      "b40ed86654e5: Layer already exists\n",
      "7e5ddf84103e: Pushed\n",
      "cfd78c74f564: Pushed\n",
      "6ce2081dc396: Pushed\n",
      "22240422cf4a: Pushed\n",
      "latest: digest: sha256:5c9e8b6c3b3c636acc376abe7f31608f69d0e1a78fe55dff7cb6a24c71ea6f7c size: 2840\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                           STATUS\n",
      "06916014-e298-4213-be9e-012dde90c1b4  2022-10-06T19:16:53+00:00  1M21S     gs://jchavezar-demo_cloudbuild/source/1665083813.342888-8c64544d4ce64462b8f82fe08ded2489.tgz  gcr.io/jchavezar-demo/demos-train-aws (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAINING_IMAGE_URI 1_custom_train_job/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2de95d-2a56-4320-9f88-472a99770954",
   "metadata": {},
   "source": [
    "## Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e0c5f68c-269d-44f4-a699-e17c513d5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Job Specs\n",
    "import env\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-4',\n",
    "    },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': TRAINING_IMAGE_URI,\n",
    "            'env': [\n",
    "                {\n",
    "                    'name': 'FILE_URI',\n",
    "                    'value': DATASET_DIR\n",
    "                },\n",
    "                {\n",
    "                    'name': 'AWS_ACCESS_KEY_ID',\n",
    "                    'value': env.AWS_ACCESS_KEY_ID\n",
    "                },\n",
    "                {\n",
    "                    'name': 'AWS_SECRET_ACCESS_KEY',\n",
    "                    'value': env.AWS_SECRET_ACCESS_KEY\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "metric_spec=hyperparameter_tuning_job.serialize_metrics({'accuracy': 'maximize'})\n",
    "parameter_spec = hyperparameter_tuning_job.serialize_parameters({\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "    \"num_neurons_1\": hpt.DiscreteParameterSpec(values=[16, 32, 64, 128, 256], scale=None),\n",
    "    \"num_neurons_2\": hpt.DiscreteParameterSpec(values=[16, 32, 64, 128, 256], scale=None)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0045c865-ef6e-42d4-80a9-d1dfa327f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "\n",
    "@pipeline(name='aws-gcp-test')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    model_dir: str,\n",
    "    serving_image_uri: str\n",
    "):\n",
    "    hp_tuning_task = hyperparameter_tuning_job.HyperparameterTuningJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name='hpt_custom_train_task',\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "        study_spec_metrics=metric_spec,\n",
    "        study_spec_parameters=parameter_spec,\n",
    "        max_trial_count=15,\n",
    "        parallel_trial_count=3,\n",
    "        base_output_directory=model_dir\n",
    "    )\n",
    "    \n",
    "    trials_task = hyperparameter_tuning_job.GetTrialsOp(\n",
    "      gcp_resources=hp_tuning_task.outputs['gcp_resources'])\n",
    "\n",
    "    best_trial_task = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "      trials=trials_task.output, study_spec_metrics=metric_spec)\n",
    "\n",
    "    is_accuracy_beyond_threshold_task = hyperparameter_tuning_job.IsMetricBeyondThresholdOp(\n",
    "      trial=best_trial_task.output, study_spec_metrics=metric_spec, threshold=0.7)\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        is_accuracy_beyond_threshold_task.output == \"true\",\n",
    "        name=\"deploy_decision\",        \n",
    "    ):\n",
    "        \n",
    "        best_hyperparameters_task = hyperparameter_tuning_job.GetHyperparametersOp(\n",
    "            trial=best_trial_task.output)\n",
    "        \n",
    "        # Construct new worker_pool_specs based on best hyperparameters\n",
    "        worker_pool_specs_task = hyperparameter_tuning_job.GetWorkerPoolSpecsOp(\n",
    "          best_hyperparameters=best_hyperparameters_task.output,\n",
    "          worker_pool_specs=worker_pool_specs\n",
    "        )\n",
    "        \n",
    "        # Train new model based on new worker_pool_specs\n",
    "        training_task = CustomTrainingJobOp(\n",
    "          project=project_id,\n",
    "          display_name='training-job',\n",
    "          worker_pool_specs=worker_pool_specs_task.output\n",
    "        )\n",
    "        \n",
    "        model_upload_task = ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=f'tf-kfp-prebuilt-model-upload-job',\n",
    "            artifact_uri=f'{model_dir}/model',\n",
    "            serving_container_image_uri=serving_image_uri,\n",
    "        ).after(training_task)\n",
    "        \n",
    "        create_endpoint_task = EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name='fraud_det'\n",
    "        ).after(model_upload_task)\n",
    "        \n",
    "        model_deploy_task = ModelDeployOp(\n",
    "            endpoint=create_endpoint_task.outputs[\"endpoint\"],\n",
    "            model=model_upload_task.outputs[\"model\"],\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_machine_type='n1-standard-4'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac07a5de-4b86-40e2-b60a-6bccc6a2b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "\n",
    "@pipeline(name='aws-gcp-test')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    model_dir: str,\n",
    "    serving_image_uri: str\n",
    "):\n",
    "    hp_tuning_task = hyperparameter_tuning_job.HyperparameterTuningJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name='hpt_custom_train_task',\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "        study_spec_metrics=metric_spec,\n",
    "        study_spec_parameters=parameter_spec,\n",
    "        max_trial_count=15,\n",
    "        parallel_trial_count=3,\n",
    "        base_output_directory=model_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a94eb7-2b34-4ad1-88e7-f8ffd290e032",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "236b2dff-6a3e-4d7a-9c27-6e714819cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='aws_gcp_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea69d87-82fb-4a15-b856-29655e155874",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f1d7eb3-d109-4867-89f7-3c806297211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/aws-gcp-test-20221006192405\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/aws-gcp-test-20221006192405')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/aws-gcp-test-20221006192405?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name='customjob-aws-gcp',\n",
    "    template_path='aws_gcp_test.json',\n",
    "    pipeline_root='gs://vtx-path-root',\n",
    "    parameter_values={\n",
    "        'project_id': 'jchavezar-demo',\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'serving_image_uri': SERVING_CONTAINER_IMAGE_URI\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04958afc-770c-42da-bf2d-533d356eea47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pipelines",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "pipelines",
   "language": "python",
   "name": "pipelines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
