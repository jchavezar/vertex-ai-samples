{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93418941-0ac0-4953-a0d7-d1bb4b72434c",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f947c01b-dbef-4a99-845b-44fb896f74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jchavezar-demo'\n",
    "TRAINING_IMAGE_URI = f'gcr.io/{PROJECT_ID}/demos-hpt-train-azure:latest'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-9:latest'\n",
    "DATASET_NAME = 'fraud_detection.csv'\n",
    "MODEL_DIR = 'gs://vtx-models/azure/hpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc827c5-cafa-403f-a769-db6c3423d3ec",
   "metadata": {},
   "source": [
    "## Create Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539aaeec-025a-413b-85e2-a6c84b2aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"1_custom_train_job\"):\n",
    "    os.makedirs(\"1_custom_train_job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a2f7d-e852-428c-9488-146ac72fa66a",
   "metadata": {},
   "source": [
    "## Create Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbcd2d4-2e66-4979-82ba-1f19cffb190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/main.py\n",
    "\n",
    "# Extracting information from Azure Blob Storage\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "def get_args():\n",
    "    '''Parses args. Must include all hyperparameters you want to tune.'''\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        required=True,\n",
    "        type=float,\n",
    "        help='learning rate')\n",
    "    parser.add_argument(\n",
    "        '--num_neurons_1',\n",
    "        required=True,\n",
    "        type=int,\n",
    "        help='number of units in first hidden layer')\n",
    "    parser.add_argument(\n",
    "        '--num_neurons_2',\n",
    "        required=True,\n",
    "        type=int,\n",
    "        help='number of units in second hidden layer')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def preprocess_data(dataset: str):\n",
    "    \n",
    "    connect_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\n",
    "    \n",
    "    # Create the BlobServiceClient object\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "    blob_client_instance = blob_service_client.get_blob_client('vertex', dataset, snapshot=None)\n",
    "    \n",
    "    with open(dataset, 'wb') as my_blob:\n",
    "        blob_data = blob_client_instance.download_blob()\n",
    "        blob_data.readinto(my_blob)\n",
    "    \n",
    "    df = pd.read_csv(dataset)\n",
    "    \n",
    "    print(df.head(10))\n",
    "    \n",
    "    train_df = df.sample(frac=0.8, random_state=1)\n",
    "    test_df = df.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.iloc[:,:-1]\n",
    "    X_test = test_df.iloc[:,:-1]\n",
    "    y_train = train_df['Class'].astype(np.float32)\n",
    "    y_test = test_df['Class'].astype(np.float32)\n",
    "    \n",
    "    # Standarization\n",
    "    \n",
    "    X_train_norm = (X_train-X_train.mean())/X_train.std()\n",
    "    X_test_norm = (X_test-X_test.mean())/X_test.std()\n",
    "    \n",
    "    return X_train_norm, y_train, X_test_norm, y_test \n",
    "    \n",
    "    # Model\n",
    "\n",
    "def create_model(\n",
    "    ds_length: int,\n",
    "    my_learning_rate: float, \n",
    "    nn_1: int, \n",
    "    nn_2: int):\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    METRICS = [\n",
    "      tf.keras.metrics.TruePositives(name='tp'),\n",
    "      tf.keras.metrics.FalsePositives(name='fp'),\n",
    "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall'),\n",
    "      tf.keras.metrics.AUC(name='auc'),\n",
    "      tf.keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        layers.Dense(nn_1, activation='relu', input_shape=[ds_length]),\n",
    "        layers.Dense(nn_2, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "        \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "    x, \n",
    "    y, \n",
    "    model, \n",
    "    epochs,\n",
    "    batch_size=None, \n",
    "    shuffle=True\n",
    "):\n",
    "    history = model.fit(\n",
    "        x=x, \n",
    "        y=y, \n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs, shuffle=shuffle)\n",
    "    \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # The following variables are the hyperparameters.\n",
    "    args = get_args()\n",
    "    print(args.learning_rate, file=sys.stderr)\n",
    "    \n",
    "    X_train_norm, y_train, X_test_norm, y_test = preprocess_data(os.environ['FILE_NAME'])\n",
    "    ds_length = len(X_train_norm.keys())\n",
    "    \n",
    "    # Establish the model's topography.\n",
    "    my_model = create_model(ds_length, args.learning_rate, args.num_neurons_1, args.num_neurons_2)\n",
    "    \n",
    "    # Train the model on the training set.\n",
    "    hist = train_model(X_train_norm, y_train, my_model, epochs, \n",
    "                           batch_size)\n",
    "    \n",
    "    # DEFINE METRIC\n",
    "    hp_metric = hist['accuracy'][0]\n",
    "    \n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=hp_metric,\n",
    "        global_step=epochs)\n",
    "    \n",
    "    import sys\n",
    "    \n",
    "    print(os.environ['AIP_MODEL_DIR'])\n",
    "    print(os.environ['AIP_MODEL_DIR'], file=sys.stderr)\n",
    "\n",
    "    my_model.save(os.environ['AIP_MODEL_DIR'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e667f-004d-4bb3-9eec-18a0bd642d56",
   "metadata": {},
   "source": [
    "## Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5839f34a-7d36-470a-ba84-5f1b41dfac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 1_custom_train_job/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_custom_train_job/Dockerfile\n",
    "\n",
    "FROM tensorflow/tensorflow\n",
    "\n",
    "RUN pip install azure-storage-blob azure-identity pandas\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "COPY main.py /main.py\n",
    "\n",
    "CMD [\"python\", \"/main.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b86159-88db-4a31-a11d-a09b97311114",
   "metadata": {},
   "source": [
    "## Create Docker Image with CloudBuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82034e-165c-4d37-939a-bd6c76c5a109",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 4.4 KiB before compression.\n",
      "Uploading tarball of [1_custom_train_job/.] to [gs://jchavezar-demo_cloudbuild/source/1672861136.620697-449cf746e41b42658c863bb6f2e776e1.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jchavezar-demo/locations/global/builds/b51d71c2-b075-4d5a-b9e0-f9e6b5ee3cce].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/b51d71c2-b075-4d5a-b9e0-f9e6b5ee3cce?project=569083142710 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"b51d71c2-b075-4d5a-b9e0-f9e6b5ee3cce\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jchavezar-demo_cloudbuild/source/1672861136.620697-449cf746e41b42658c863bb6f2e776e1.tgz#1672861136936357\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit -t $TRAINING_IMAGE_URI 1_custom_train_job/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2de95d-2a56-4320-9f88-472a99770954",
   "metadata": {},
   "source": [
    "## Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c5f68c-269d-44f4-a699-e17c513d5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Job Specs\n",
    "import env\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "\n",
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-4',\n",
    "    },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': TRAINING_IMAGE_URI,\n",
    "            'env': [\n",
    "                {\n",
    "                    'name': 'FILE_NAME',\n",
    "                    'value': DATASET_NAME\n",
    "                },\n",
    "                {\n",
    "                    'name': 'AZURE_STORAGE_CONNECTION_STRING',\n",
    "                    'value': env.AZURE_STORAGE_CONNECTION_STRING\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "metric_spec=hyperparameter_tuning_job.serialize_metrics({'accuracy': 'maximize'})\n",
    "parameter_spec = hyperparameter_tuning_job.serialize_parameters({\n",
    "    \"learning_rate\": hpt.DoubleParameterSpec(min=0.001, max=1, scale=\"log\"),\n",
    "    \"num_neurons_1\": hpt.DiscreteParameterSpec(values=[16, 32, 64, 128, 256], scale=None),\n",
    "    \"num_neurons_2\": hpt.DiscreteParameterSpec(values=[16, 32, 64, 128, 256], scale=None)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0045c865-ef6e-42d4-80a9-d1dfa327f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "\n",
    "@pipeline(name='azure-gcp-test')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    model_dir: str,\n",
    "    serving_image_uri: str\n",
    "):\n",
    "    hp_tuning_task = hyperparameter_tuning_job.HyperparameterTuningJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name='hpt_custom_train_task',\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "        study_spec_metrics=metric_spec,\n",
    "        study_spec_parameters=parameter_spec,\n",
    "        max_trial_count=15,\n",
    "        parallel_trial_count=3,\n",
    "        base_output_directory=model_dir\n",
    "    )\n",
    "    \n",
    "    trials_task = hyperparameter_tuning_job.GetTrialsOp(\n",
    "      gcp_resources=hp_tuning_task.outputs['gcp_resources'])\n",
    "\n",
    "    best_trial_task = hyperparameter_tuning_job.GetBestTrialOp(\n",
    "      trials=trials_task.output, study_spec_metrics=metric_spec)\n",
    "\n",
    "    is_accuracy_beyond_threshold_task = hyperparameter_tuning_job.IsMetricBeyondThresholdOp(\n",
    "      trial=best_trial_task.output, study_spec_metrics=metric_spec, threshold=0.7)\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        is_accuracy_beyond_threshold_task.output == \"true\",\n",
    "        name=\"deploy_decision\",        \n",
    "    ):\n",
    "        \n",
    "        best_hyperparameters_task = hyperparameter_tuning_job.GetHyperparametersOp(\n",
    "            trial=best_trial_task.output)\n",
    "        \n",
    "        # Construct new worker_pool_specs based on best hyperparameters\n",
    "        worker_pool_specs_task = hyperparameter_tuning_job.GetWorkerPoolSpecsOp(\n",
    "          best_hyperparameters=best_hyperparameters_task.output,\n",
    "          worker_pool_specs=worker_pool_specs\n",
    "        )\n",
    "        \n",
    "        # Train new model based on new worker_pool_specs\n",
    "        training_task = CustomTrainingJobOp(\n",
    "          project=project_id,\n",
    "          display_name='training-job',\n",
    "          worker_pool_specs=worker_pool_specs_task.output\n",
    "        )\n",
    "        \n",
    "        model_upload_task = ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=f'tf-kfp-prebuilt-model-upload-job',\n",
    "            artifact_uri=f'{model_dir}/model',\n",
    "            serving_container_image_uri=serving_image_uri,\n",
    "        ).after(training_task)\n",
    "        \n",
    "        create_endpoint_task = EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name='fraud_det'\n",
    "        ).after(model_upload_task)\n",
    "        \n",
    "        model_deploy_task = ModelDeployOp(\n",
    "            endpoint=create_endpoint_task.outputs[\"endpoint\"],\n",
    "            model=model_upload_task.outputs[\"model\"],\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_machine_type='n1-standard-4'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac07a5de-4b86-40e2-b60a-6bccc6a2b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "from google_cloud_pipeline_components.experimental import hyperparameter_tuning_job\n",
    "from google_cloud_pipeline_components.aiplatform import (\n",
    "    EndpointCreateOp,\n",
    "    ModelDeployOp,\n",
    "    ModelUploadOp,\n",
    ")\n",
    "\n",
    "@pipeline(name='azure-gcp-test')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    model_dir: str,\n",
    "    serving_image_uri: str\n",
    "):\n",
    "    hp_tuning_task = hyperparameter_tuning_job.HyperparameterTuningJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name='hpt_custom_train_task',\n",
    "        worker_pool_specs = worker_pool_specs,\n",
    "        study_spec_metrics=metric_spec,\n",
    "        study_spec_parameters=parameter_spec,\n",
    "        max_trial_count=15,\n",
    "        parallel_trial_count=3,\n",
    "        base_output_directory=model_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a94eb7-2b34-4ad1-88e7-f8ffd290e032",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236b2dff-6a3e-4d7a-9c27-6e714819cf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='azure_gcp_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea69d87-82fb-4a15-b856-29655e155874",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1d7eb3-d109-4867-89f7-3c806297211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/azure-gcp-test-20221228153659\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/azure-gcp-test-20221228153659')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/azure-gcp-test-20221228153659?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name='customjob-azure-gcp',\n",
    "    template_path='azure_gcp_test.json',\n",
    "    pipeline_root='gs://vtx-path-root',\n",
    "    parameter_values={\n",
    "        'project_id': 'jchavezar-demo',\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'serving_image_uri': SERVING_CONTAINER_IMAGE_URI\n",
    "    },\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cpr",
   "name": "tf2-gpu.2-10.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m98"
  },
  "kernelspec": {
   "display_name": "cpr",
   "language": "python",
   "name": "cpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
