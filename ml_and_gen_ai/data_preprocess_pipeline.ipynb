{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp.dsl import component, Output, Input, Dataset, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "project_id = \"jesusarguelles-sandbox\"\n",
    "pipeline_root_path = \"gs://jesusarguelles-staging/\"\n",
    "bucket_id = \"jesusarguelles-datasets-public\"\n",
    "bucket_folder_name = \"money_laundering_detection\"\n",
    "raw_file_name = \"paysim_dataset.csv\"\n",
    "raw_data_full_path = f\"gs://{bucket_id}/{bucket_folder_name}/{raw_file_name}\"\n",
    "filtered_data_1 = \"filtered_data_1.csv\"\n",
    "filtered_data_2 = \"filtered_data_2.csv\"\n",
    "filtered_data_3 = \"filtered_data_3.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"gcsfs\", \"pandas\"]\n",
    ")\n",
    "def data_preprocess_stage_1(\n",
    "        raw_dataset: str,\n",
    "        output_dataset_one : Output[Dataset],\n",
    "        output_dataset_two : Output[Dataset]\n",
    "):\n",
    "    import os\n",
    "    import csv\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from random import randint\n",
    "\n",
    "\n",
    "    logging.warning(\"DATA PREPROCESSING 1 STAGE\")\n",
    "    logging.warning(\"Reading Dataset...\")\n",
    "\n",
    "    X = pd.read_csv(raw_dataset)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Dataset\")\n",
    "\n",
    "    nameOrigCol = 3\n",
    "    nameDestCol = 6\n",
    "    nameOrig = []\n",
    "    nameDest = []\n",
    "    nameCount = {}\n",
    "    namesWithMoreThanOneOccurrence = []\n",
    "\n",
    "    logging.warning(\"Checking Each Person's Transactions Count...\")\n",
    "\n",
    "    for name in X[:, nameOrigCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameOrig.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    for name in X[:, nameDestCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameDest.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    logging.warning(\"Count Identification Done\")\n",
    "\n",
    "    logging.warning(\"Calculating Median ...\")\n",
    "\n",
    "    countArr = []\n",
    "    count = 0\n",
    "    for attr, value in nameCount.items() :\n",
    "        if value > 40 :\n",
    "            countArr.append(value)\n",
    "            count += 1\n",
    "    median = np.median(countArr)\n",
    "\n",
    "    logging.warning(f\"Median : {median}\")\n",
    "\n",
    "    logging.warning(\"Filtering Data Based on Transactions Count...\")\n",
    "    csv_golden_data = []\n",
    "\n",
    "    for i in range(X.shape[0]) :\n",
    "        if nameCount.get(X[i, 3], -1) > 40 or nameCount.get(X[i, 6], -1) > 40 :\n",
    "            csv_golden_data.append(X[i, :])\n",
    "\n",
    "    logging.warning(\"Filtering Done\")\n",
    "\n",
    "    logging.warning(\"Storing Filtered Data in data_processed folder...\")\n",
    "\n",
    "    new_file_name = \"filtered_data.csv\"\n",
    "\n",
    "    with open(output_dataset_one.path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_golden_data)\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 2 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 1 dataset...\")\n",
    "\n",
    "    X = pd.DataFrame(csv_golden_data)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 1 dataset...\")\n",
    "\n",
    "    csv_dataset_primary = []\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 6\n",
    "    oldbalanceDest = 7\n",
    "    accountType = 8\n",
    "    isFraud = 9\n",
    "    isFlaggedFraud = 10\n",
    "\n",
    "    logging.warning(\"Changing Labels of Type Column ...\")\n",
    "\n",
    "    transfer = [\"WIRE_IN\", \"WIRE_OUT\"]\n",
    "    for i in range(X.shape[0]):\n",
    "        arr = []\n",
    "        arr.append(X[i,step])\n",
    "        if X[i,trans_type] ==\"PAYMENT\":\n",
    "            arr.append(\"CREDIT\")\n",
    "        elif X[i,trans_type] ==\"TRANSFER\":\n",
    "            arr.append(transfer[randint(0,1)])\n",
    "        else:\n",
    "            arr.append(X[i,trans_type])\n",
    "        arr.append(X[i,amount])\n",
    "        arr.append(X[i,nameOrig])\n",
    "        arr.append(X[i,oldbalanceOrg])\n",
    "        arr.append(X[i,nameDest])\n",
    "        arr.append(X[i,oldbalanceDest])\n",
    "        if X[i,trans_type] == \"TRANSFER\":\n",
    "            arr.append(\"FOREIGN\")\n",
    "        else:\n",
    "            arr.append(\"DOMESTIC\")\n",
    "\n",
    "        arr.append(X[i,isFraud])\n",
    "        arr.append(X[i,isFlaggedFraud])\n",
    "\n",
    "        csv_dataset_primary.append(arr)\n",
    "\n",
    "    logging.warning(\"Changing Labels Done\")\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "\n",
    "    columns=['step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "             'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_primary = pd.DataFrame(csv_dataset_primary, columns=columns)\n",
    "\n",
    "    data_primary.to_csv(output_dataset_two.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\"]\n",
    ")\n",
    "def data_preprocess_stage_2(\n",
    "        input_dataset: Input[Dataset],\n",
    "        output_dataset_three: Output[Dataset]\n",
    "):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    # data_path = f'gs://{bucket_id}/{folder_id}/filtered_data_2.csv'\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 3 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 2 dataset...\")\n",
    "\n",
    "    X = pd.read_csv(input_dataset.path)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 2 dataset\")\n",
    "\n",
    "    #col\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 5\n",
    "    oldbalanceDest = 6\n",
    "    accountType = 7\n",
    "    isFraud = 8\n",
    "\n",
    "    #col\n",
    "    entity = 0\n",
    "    incommingDomestic30 = 1\n",
    "    incommingDomestic60 = 2\n",
    "    incommingDomestic90 = 3\n",
    "    outgoingDomestic30 = 4\n",
    "    outgoingDomestic60 = 5\n",
    "    outgoingDomestic90 = 6\n",
    "    incommingForeign30 = 7\n",
    "    incommingForeign60 = 8\n",
    "    incommingForeign90 = 9\n",
    "    outgoingForeign30 = 10\n",
    "    outgoingForeign60 = 11\n",
    "    outgoingForeign90 = 12\n",
    "    incoming_domestic_count_30 = 13\n",
    "    incoming_domestic_count_60 = 14\n",
    "    incoming_domestic_count_90 = 15\n",
    "    outgoing_domestic_count_30 = 16\n",
    "    outgoing_domestic_count_60 = 17\n",
    "    outgoing_domestic_count_90 = 18\n",
    "    incoming_foreign_count_30 = 19\n",
    "    incoming_foreign_count_60 = 20\n",
    "    incoming_foreign_count_90 = 21\n",
    "    outgoing_foreign_count_30 = 22\n",
    "    outgoing_foreign_count_60 = 23\n",
    "    outgoing_foreign_count_90 = 24\n",
    "    balance_difference_30 = 25\n",
    "    balance_difference_60 = 26\n",
    "    balance_difference_90 = 27\n",
    "    isFraudSec = 28\n",
    "\n",
    "    csv_dataset_secondary = []\n",
    "    entities_pos = {}\n",
    "    enititesDict = {}\n",
    "\n",
    "    logging.warning(\"Creating New Features Using Transaction History...\")\n",
    "\n",
    "    def getSecRow(entity):\n",
    "        return [entity,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        source_entity = X[i,nameOrig]\n",
    "        dest_entity = X[i,nameDest]\n",
    "\n",
    "        source_pos = entities_pos.get(source_entity,-1)\n",
    "        if source_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[source_entity] = pos\n",
    "            source_pos = pos\n",
    "\n",
    "            row = getSecRow(source_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        dest_pos = entities_pos.get(dest_entity,-1)\n",
    "        if dest_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[dest_entity] = pos\n",
    "            dest_pos = pos\n",
    "\n",
    "            row = getSecRow(dest_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        transferAmountSource = 0\n",
    "        transferAmountDest = 0\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_IN\" or X[i,trans_type] == \"CREDIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_OUT\" or X[i,trans_type] == \"DEBIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_IN\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign90] += X[i,amount]\n",
    "                # print(dest_pos,outgoingForeign90,i,amount)\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_OUT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if enititesDict.get(source_entity,-1) == -1:\n",
    "            enititesDict[source_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceOrg],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        if enititesDict.get(dest_entity,-1) == -1:\n",
    "            enititesDict[dest_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceDest],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        incomingForSource = [\"CASH_IN\",\"CREDIT\",\"WIRE_IN\"]\n",
    "        incomingForDest = [\"CASH_OUT\",\"DEBIT\",\"WIRE_OUT\"]\n",
    "        outgoingForDest = incomingForSource\n",
    "        outgoingForSource = incomingForDest\n",
    "\n",
    "        if X[i,step]<=30:\n",
    "            enititesDict[source_entity]['day30Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day30Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "        if X[i,step]<=60:\n",
    "            enititesDict[source_entity]['day60Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day60Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "        if X[i,step]<=90:\n",
    "            enititesDict[source_entity]['day90Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day90Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][balance_difference_30] = enititesDict[source_entity]['day30Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_60] = enititesDict[source_entity]['day60Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_90] = enititesDict[source_entity]['day90Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_30] = enititesDict[source_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_30] = enititesDict[source_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_60] = enititesDict[source_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_60] = enititesDict[source_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_90] = enititesDict[source_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_90] = enititesDict[source_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_30] = enititesDict[source_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_30] = enititesDict[source_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_60] = enititesDict[source_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_60] = enititesDict[source_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_90] = enititesDict[source_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_90] = enititesDict[source_entity]['countOutgoingForeign90']\n",
    "\n",
    "        csv_dataset_secondary[source_pos][isFraudSec] = csv_dataset_secondary[source_pos][isFraudSec] or X[i,isFraud]\n",
    "\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_30] = enititesDict[dest_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_30] = enititesDict[dest_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_60] = enititesDict[dest_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_60] = enititesDict[dest_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_90] = enititesDict[dest_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_90] = enititesDict[dest_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_30] = enititesDict[dest_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_30] = enititesDict[dest_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_60] = enititesDict[dest_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_60] = enititesDict[dest_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_90] = enititesDict[dest_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_90] = enititesDict[dest_entity]['countOutgoingForeign90']\n",
    "\n",
    "\n",
    "    columns = ['entity','incoming_domestic_amount_30','incoming_domestic_amount_60','incoming_domestic_amount_90',\n",
    "               'outgoing_domestic_amount_30','outgoing_domestic_amount_60','outgoing_domestic_amount_90',\n",
    "               'incoming_foreign_amount_30','incoming_foreign_amount_60','incoming_foreign_amount_90',\n",
    "               'outgoing_foreign_amount_30','outgoing_foreign_amount_60','outgoing_foreign_amount_90',\n",
    "               'incoming_domestic_count_30','incoming_domestic_count_60','incoming_domestic_count_90',\n",
    "               'outgoing_domestic_count_30','outgoing_domestic_count_60','outgoing_domestic_count_90',\n",
    "               'incoming_foreign_count_30','incoming_foreign_count_60','incoming_foreign_count_90',\n",
    "               'outgoing_foreign_count_30','outgoing_foreign_count_60','outgoing_foreign_count_90',\n",
    "               'balance_difference_30','balance_difference_60','balance_difference_90','isFraud']\n",
    "\n",
    "    logging.warning(\"Creating New Features Done\")\n",
    "\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "    # filtered_data_3.csv\n",
    "    data_secondary = pd.DataFrame(csv_dataset_secondary, columns=columns)\n",
    "    data_secondary.to_csv(output_dataset_three.path,index=False)\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def feature_selection(\n",
    "        input_dataset : Input[Dataset],\n",
    "        output_dataset : Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1: filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param output_dataset: feature_importances.csv\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"FEATURE SELECTION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    dataframeX = pd.read_csv(input_dataset.path)\n",
    "    col_names = list(dataframeX.columns.values)\n",
    "    dataMat = dataframeX.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    logging.warning(\"Creating X and Y Variables...\")\n",
    "\n",
    "    X = dataMat[:,1:-2]\n",
    "    Y = dataMat[:,-1]\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of Y: {Y.shape}\")\n",
    "\n",
    "    logging.warning(\"Instiantiating Random Forest Model...\")\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    logging.warning(\"Fitting Data...\")\n",
    "\n",
    "    model.fit(X, Y.astype(int))\n",
    "\n",
    "    logging.warning(\"Checking Feature Importances...\")\n",
    "\n",
    "    feature_imp = model.feature_importances_\n",
    "\n",
    "    sorted_feature_vals = np.sort(feature_imp)\n",
    "    sorted_feature_indexes = np.argsort(feature_imp)\n",
    "\n",
    "    logging.warning(\"Significant Features in decreasing order of importance: \")\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances in reports...\")\n",
    "\n",
    "    fea_imp = [[col_names[i+2], feature_imp[i]] for i in reversed(sorted_feature_indexes)]\n",
    "    features = pd.DataFrame(fea_imp, columns=[\"features\", \"importance_score\"])\n",
    "    # feature_importances.csv\n",
    "    features.to_csv(output_dataset.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def segment_generation(\n",
    "        input_dataset_1 : Input[Dataset],\n",
    "        input_dataset_2 : Input[Dataset],\n",
    "        input_dataset_3 : Input[Dataset],\n",
    "        silhoutte_scores : Output[Dataset],\n",
    "        final_dataset_output: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1:  filtered_data_2.csv from data-preprocess-stage-1\n",
    "    :param input_dataset_2:  filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param input_dataset_3:  feature_importances.csv from features_election\n",
    "    :param silhoutte_scores: Metrics\n",
    "    :param final_dataset_output: final_dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Segment Generation\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"SEGMENT GENERATOR STAGE\")\n",
    "\n",
    "    def getClusterPredictions(data, true_k):\n",
    "        model = KMeans(n_clusters=true_k)\n",
    "        model.fit(data)\n",
    "        prediction = model.predict(data)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def getBestCluster(X,_min=2,_max=10):\n",
    "        selected_cluster = 0\n",
    "        previous_sil_coeff = 0.001 #some random small number not 0\n",
    "        sc_vals = []\n",
    "        for n_cluster in range(_min, _max):\n",
    "            kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "            label = kmeans.labels_\n",
    "\n",
    "            sil_coeff = silhouette_score(X, label, metric='euclidean', sample_size=1000)\n",
    "            sc_vals.append(sil_coeff)\n",
    "            # print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))\n",
    "\n",
    "            percent_change = (sil_coeff-previous_sil_coeff)*100/previous_sil_coeff\n",
    "\n",
    "            # return when below a threshold of 1%\n",
    "            if percent_change<1:\n",
    "                selected_cluster = n_cluster-1\n",
    "\n",
    "            previous_sil_coeff = sil_coeff\n",
    "\n",
    "        return selected_cluster or _max, sc_vals\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    X_dataframe = pd.read_csv(input_dataset_2.path)\n",
    "    X = X_dataframe.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    col_names = list(X_dataframe.columns.values)\n",
    "\n",
    "    X_trimmed_features = np.zeros((X.shape[0],1))\n",
    "\n",
    "    logging.warning(\"Importing Feature Importances...\")\n",
    "\n",
    "    #feature_path = f'gs://{bucket_id}/{folder_id}/feature_importances.csv'\n",
    "    features = pd.read_csv(input_dataset_3.path)\n",
    "\n",
    "    logging.warning(\"Selecting Top 13 Features for CLustering...\")\n",
    "\n",
    "    top_13 = features.iloc[:13, 0].tolist()\n",
    "\n",
    "    logging.warning(\"Top 13 Features stored in List\")\n",
    "\n",
    "    for feature in top_13:\n",
    "        X_trimmed_features = np.concatenate((X_trimmed_features,np.expand_dims(X_dataframe[feature],axis=1)),axis=1)\n",
    "    X_trimmed_features = X_trimmed_features[:,1:]\n",
    "\n",
    "    logging.warning(\"Choosing Best Number Of Clusters...\")\n",
    "\n",
    "    min_value = 2\n",
    "    max_value = 10\n",
    "    true_k, sc_vals = getBestCluster(X_trimmed_features,_min=min_value,_max=max_value)\n",
    "    true_k = 5\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores...\")\n",
    "\n",
    "\n",
    "    sil_score = [[i, sc_vals[i-min_value]] for i in range(min_value, max_value)]\n",
    "    sil = pd.DataFrame(sil_score, columns=[\"no_of_clusters\", \"silhoutte_score\"])\n",
    "    sil.to_csv(silhoutte_scores.path, index=False)\n",
    "    #sil.to_csv(f'gs://{bucket_id}/{folder_id}/silhoutte_scores.csv', index=False)\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores Done\")\n",
    "\n",
    "    logging.warning(\"Creating Clusters with Best No Of Clusters...\")\n",
    "\n",
    "    prediction = getClusterPredictions(X_trimmed_features, true_k)\n",
    "    seg_dict = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        seg_dict[X[i,0]] = prediction[i]\n",
    "\n",
    "    logging.warning(\"Inputing Filtered Data 2 Dataset...\")\n",
    "\n",
    "    X_dataframe_pri = pd.read_csv(input_dataset_1.path)\n",
    "    X_pri = X_dataframe_pri.to_numpy()\n",
    "    col_names = list(X_dataframe_pri.columns.values)\n",
    "\n",
    "    logging.warning(\"Read Filtered 2 Data\")\n",
    "\n",
    "    logging.warning(\"Creating Final Dataset with segments...\")\n",
    "\n",
    "    X_with_segments = []\n",
    "    for i in range(X_pri.shape[0]):\n",
    "        X_with_segments.append(np.concatenate(([[seg_dict[X_pri[i,3]]]],np.expand_dims(X_pri[i,:],axis=0)),axis=1)[0])\n",
    "\n",
    "    segmented_columns = ['segment','step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "                         'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_segmented = pd.DataFrame(X_with_segments, columns = segmented_columns)\n",
    "    data_segmented = data_segmented.drop('isFlaggedFraud', axis=1)\n",
    "    data_segmented.to_csv(final_dataset_output.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Final Dataset Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\", \"catboost\"]\n",
    ")\n",
    "def training(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str,\n",
    "        input_dataset : Input[Dataset],\n",
    "        model_path: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset: fina_dataset.csv from segment-generation\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pickle\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"MODEL CREATION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Final Dataset...\")\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_id)\n",
    "\n",
    "    dataMat = pd.read_csv(input_dataset.path)\n",
    "    data = dataMat.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Final Dataset\")\n",
    "\n",
    "    logging.warning(\"Checking Categorical Features...\")\n",
    "\n",
    "    cat_feat = [i for i in dataMat.columns if dataMat[i].dtypes == 'O']\n",
    "\n",
    "    logging.warning(\"Checking Missing Values...\")\n",
    "\n",
    "    a = dict(dataMat.isnull().sum())\n",
    "    b = [[i, a[i]] for i in a.keys()]\n",
    "    missing = pd.DataFrame(b, columns=['features', 'null_values_count'])\n",
    "\n",
    "    logging.warning(\"Storing Missing Values...\")\n",
    "\n",
    "    missing.to_csv(\"missing_values.csv\", index=False)\n",
    "\n",
    "    logging.warning(\"Storing Missing Values Done\")\n",
    "\n",
    "    logging.warning(\"Encoding Categorical Features...\")\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    print(\"------\")\n",
    "    print(cat_feat)\n",
    "    print(\"------\")\n",
    "\n",
    "    label_encoders = {}\n",
    "    label_mappings = {}\n",
    "\n",
    "    for i in cat_feat:\n",
    "        encoder.fit(dataMat[i])\n",
    "        dataMat[i] = encoder.transform(dataMat[i])\n",
    "\n",
    "        label_encoders[i] = encoder\n",
    "        label_mappings[i] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "    blob = bucket.blob(f\"{bucket_folder}/label_encoder.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    logging.warning(\"Features Encoding Done\")\n",
    "\n",
    "    logging.warning(\"Creating X and y variables ...\")\n",
    "\n",
    "    X = dataMat.iloc[:, :-1]\n",
    "    y = dataMat['isFraud']\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of y: {y.shape}\")\n",
    "\n",
    "    logging.warning(\"Splitting Dataset...\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    logging.warning(\"Instantiating Model...\")\n",
    "\n",
    "    model = CatBoostClassifier(random_state=42, class_weights={0:1, 1:12}, silent=True)\n",
    "\n",
    "    logging.warning(\"Fitting Model...\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_cat = model.predict(X_test)\n",
    "\n",
    "    logging.warning(\"Saving Model...\")\n",
    "\n",
    "    #model_path = \"model.pkl\"\n",
    "    blob = bucket.blob(f\"{bucket_folder}/model.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    logging.warning(\"Saving Model Metrics...\")\n",
    "\n",
    "    metric_file_path = \"performance.json\"\n",
    "    # with open(metric_file_path, \"r\") as f:\n",
    "    #     data = json.load(f)\n",
    "\n",
    "    model_metric = {\n",
    "        \"time_stamp\": datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_cat).tolist(),\n",
    "        \"precision\": precision_score(y_test, y_pred_cat),\n",
    "        \"recall\": recall_score(y_test, y_pred_cat),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_cat)\n",
    "    }\n",
    "\n",
    "    # data['model_metric'].append(model_metric)\n",
    "    # with open(metric_file_path, \"w\") as f:\n",
    "    #     json.dump(data, f, indent=4)\n",
    "\n",
    "    logging.warning(\"Model Metrics Stored\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "\n",
    "@pipeline(name=\"money_laundering_detection\")\n",
    "def pipeline(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str\n",
    "):\n",
    "    preproces_job_1 = data_preprocess_stage_1(raw_dataset=raw_data_full_path)\n",
    "    preprocess_job_2 = data_preprocess_stage_2(input_dataset=preproces_job_1.outputs[\"output_dataset_two\"])\n",
    "    feature_selection_job = feature_selection(input_dataset=preprocess_job_2.outputs[\"output_dataset_three\"])\n",
    "    segment_generation_job = segment_generation(\n",
    "        input_dataset_1=preproces_job_1.outputs[\"output_dataset_two\"],\n",
    "        input_dataset_2=preprocess_job_2.outputs[\"output_dataset_three\"],\n",
    "        input_dataset_3=feature_selection_job.outputs[\"output_dataset\"]\n",
    "    )\n",
    "    training_job = training(\n",
    "        project_id = project_id,\n",
    "        bucket_id = bucket_id,\n",
    "        bucket_folder = bucket_folder,\n",
    "        input_dataset=segment_generation_job.outputs[\"final_dataset_output\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='money_laundering_detection.yaml'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/390227712642/locations/us-central1/pipelineJobs/money-laundering-detection-20240507145523\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/390227712642/locations/us-central1/pipelineJobs/money-laundering-detection-20240507145523')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/money-laundering-detection-20240507145523?project=390227712642\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=project_id,\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"money_laundering_detection\",\n",
    "    template_path=\"money_laundering_detection.yaml\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,\n",
    "        'bucket_id': bucket_id,\n",
    "        'bucket_folder': bucket_folder_name\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = storage.Client(project=\"jesusarguelles-sandbox\").bucket(\"jesusarguelles-datasets-public\")\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/model.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/label_encoder.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    encoder = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelEncoder()",
      "text/html": "<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"â–¸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"â–¾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LabelEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [1],\n",
    "    \"trans_type\": [\"DEBIT\"],\n",
    "    \"amount\": [181.00],\n",
    "    \"nameOrig\": [\"C1900366749\"],\n",
    "    \"oldbalanceOrg\": [4465.0],\n",
    "    \"nameDest\": [\"C997608398\"],\n",
    "    \"oldbalanceDest\": [\"10845.0\"],\n",
    "    \"accountType\": [\"DOMESTIC\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "        segment  step trans_type      amount     nameOrig  oldbalanceOrg  \\\n0             0     1   CASH_OUT      181.00   C840083671         181.00   \n350           0     1   CASH_OUT   416001.33   C749981943           0.00   \n448           0     1   CASH_OUT  1277212.77   C467632528     1277212.77   \n480           0     1   CASH_OUT    35063.63  C1635772897       35063.63   \n794           0     1   CASH_OUT   132842.64    C13692003        4499.08   \n...         ...   ...        ...         ...          ...            ...   \n211794        0   551   CASH_OUT   813992.49   C990823587      813992.49   \n211914        0   567   CASH_OUT   175203.45  C1731825076      175203.45   \n212005        2   572   CASH_OUT  2000718.20  C1426906570     2000718.20   \n212451        0   600   CASH_OUT   612229.86   C199984853      612229.86   \n212492        2   614   CASH_OUT  2739248.30  C2038452856     2739248.30   \n\n           nameDest  oldbalanceDest accountType  isFraud  \n0         C38997010        21182.00    DOMESTIC        1  \n350      C667346055          102.00    DOMESTIC        1  \n448      C716083600            0.00    DOMESTIC        1  \n480     C1983025922        31140.00    DOMESTIC        1  \n794      C297927961            0.00    DOMESTIC        1  \n...             ...             ...         ...      ...  \n211794  C1506986844     24794625.20    DOMESTIC        1  \n211914  C1046160944      2735776.45    DOMESTIC        1  \n212005   C154319946     10984451.39    DOMESTIC        1  \n212451  C1428539340      2275776.03    DOMESTIC        1  \n212492  C1576408634     15505667.41    DOMESTIC        1  \n\n[122 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment</th>\n      <th>step</th>\n      <th>trans_type</th>\n      <th>amount</th>\n      <th>nameOrig</th>\n      <th>oldbalanceOrg</th>\n      <th>nameDest</th>\n      <th>oldbalanceDest</th>\n      <th>accountType</th>\n      <th>isFraud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>181.00</td>\n      <td>C840083671</td>\n      <td>181.00</td>\n      <td>C38997010</td>\n      <td>21182.00</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>350</th>\n      <td>0</td>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>416001.33</td>\n      <td>C749981943</td>\n      <td>0.00</td>\n      <td>C667346055</td>\n      <td>102.00</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>448</th>\n      <td>0</td>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>1277212.77</td>\n      <td>C467632528</td>\n      <td>1277212.77</td>\n      <td>C716083600</td>\n      <td>0.00</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>480</th>\n      <td>0</td>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>35063.63</td>\n      <td>C1635772897</td>\n      <td>35063.63</td>\n      <td>C1983025922</td>\n      <td>31140.00</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>0</td>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>132842.64</td>\n      <td>C13692003</td>\n      <td>4499.08</td>\n      <td>C297927961</td>\n      <td>0.00</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>211794</th>\n      <td>0</td>\n      <td>551</td>\n      <td>CASH_OUT</td>\n      <td>813992.49</td>\n      <td>C990823587</td>\n      <td>813992.49</td>\n      <td>C1506986844</td>\n      <td>24794625.20</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>211914</th>\n      <td>0</td>\n      <td>567</td>\n      <td>CASH_OUT</td>\n      <td>175203.45</td>\n      <td>C1731825076</td>\n      <td>175203.45</td>\n      <td>C1046160944</td>\n      <td>2735776.45</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>212005</th>\n      <td>2</td>\n      <td>572</td>\n      <td>CASH_OUT</td>\n      <td>2000718.20</td>\n      <td>C1426906570</td>\n      <td>2000718.20</td>\n      <td>C154319946</td>\n      <td>10984451.39</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>212451</th>\n      <td>0</td>\n      <td>600</td>\n      <td>CASH_OUT</td>\n      <td>612229.86</td>\n      <td>C199984853</td>\n      <td>612229.86</td>\n      <td>C1428539340</td>\n      <td>2275776.03</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>212492</th>\n      <td>2</td>\n      <td>614</td>\n      <td>CASH_OUT</td>\n      <td>2739248.30</td>\n      <td>C2038452856</td>\n      <td>2739248.30</td>\n      <td>C1576408634</td>\n      <td>15505667.41</td>\n      <td>DOMESTIC</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>122 rows Ã— 10 columns</p>\n</div>"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isFraud == 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_335641/1306939043.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
      "/tmp/ipykernel_335641/1306939043.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
      "/tmp/ipykernel_335641/1306939043.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
      "/tmp/ipykernel_335641/1306939043.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "to_predict_df = df[df.isFraud == 1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "segment                     0\nstep                        2\ntrans_type           WIRE_OUT\namount               18627.02\nnameOrig          C1375503918\noldbalanceOrg        18627.02\nnameDest           C234430897\noldbalanceDest            0.0\naccountType           FOREIGN\nisFraud                     1\nName: 1423, dtype: object"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isFraud == 1].iloc[7,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict_df = df[df.isFraud == 1].iloc[:,:-1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "to_predict_df.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "to_predict_df = df[df.isFraud == 1].iloc[:,:-1]\n",
    "to_predict_df.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   segment  step trans_type      amount     nameOrig  oldbalanceOrg  \\\n",
      "0        0     1   CASH_OUT      181.00   C840083671         181.00   \n",
      "1        0     1   CASH_OUT   416001.33   C749981943           0.00   \n",
      "2        0     1   CASH_OUT  1277212.77   C467632528     1277212.77   \n",
      "3        0     1   CASH_OUT    35063.63  C1635772897       35063.63   \n",
      "4        0     1   CASH_OUT   132842.64    C13692003        4499.08   \n",
      "5        0     2   CASH_OUT  1096187.24    C77163673     1096187.24   \n",
      "6        0     2   CASH_OUT   963532.14   C430329518      963532.14   \n",
      "7        0     2   WIRE_OUT    18627.02  C1375503918       18627.02   \n",
      "8        0     3    WIRE_IN    10539.37  C1134864869       10539.37   \n",
      "\n",
      "      nameDest  oldbalanceDest accountType  \n",
      "0    C38997010        21182.00    DOMESTIC  \n",
      "1   C667346055          102.00    DOMESTIC  \n",
      "2   C716083600            0.00    DOMESTIC  \n",
      "3  C1983025922        31140.00    DOMESTIC  \n",
      "4   C297927961            0.00    DOMESTIC  \n",
      "5   C644345897            0.00    DOMESTIC  \n",
      "6   C991505714       132382.57    DOMESTIC  \n",
      "7   C234430897            0.00     FOREIGN  \n",
      "8   C118648358            0.00     FOREIGN  \n",
      "[0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "print(to_predict_df.head(9))\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "print(model.predict(to_predict_df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "     segment  step  trans_type      amount  nameOrig  oldbalanceOrg  nameDest  \\\n0          0     1           0      181.00       109         181.00        69   \n1          0     1           0   416001.33       102           0.00        89   \n2          0     1           0  1277212.77        87     1277212.77        92   \n3          0     1           0    35063.63        44       35063.63        47   \n4          0     1           0   132842.64        19        4499.08        64   \n..       ...   ...         ...         ...       ...            ...       ...   \n117        0   551           0   813992.49       121      813992.49        19   \n118        0   567           0   175203.45        53      175203.45         1   \n119        2   572           0  2000718.20        27     2000718.20        21   \n120        0   600           0   612229.86        68      612229.86        14   \n121        2   614           0  2739248.30        71     2739248.30        25   \n\n     oldbalanceDest  accountType  \n0          21182.00            0  \n1            102.00            0  \n2              0.00            0  \n3          31140.00            0  \n4              0.00            0  \n..              ...          ...  \n117     24794625.20            0  \n118      2735776.45            0  \n119     10984451.39            0  \n120      2275776.03            0  \n121     15505667.41            0  \n\n[122 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment</th>\n      <th>step</th>\n      <th>trans_type</th>\n      <th>amount</th>\n      <th>nameOrig</th>\n      <th>oldbalanceOrg</th>\n      <th>nameDest</th>\n      <th>oldbalanceDest</th>\n      <th>accountType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>181.00</td>\n      <td>109</td>\n      <td>181.00</td>\n      <td>69</td>\n      <td>21182.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>416001.33</td>\n      <td>102</td>\n      <td>0.00</td>\n      <td>89</td>\n      <td>102.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1277212.77</td>\n      <td>87</td>\n      <td>1277212.77</td>\n      <td>92</td>\n      <td>0.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>35063.63</td>\n      <td>44</td>\n      <td>35063.63</td>\n      <td>47</td>\n      <td>31140.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>132842.64</td>\n      <td>19</td>\n      <td>4499.08</td>\n      <td>64</td>\n      <td>0.00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>0</td>\n      <td>551</td>\n      <td>0</td>\n      <td>813992.49</td>\n      <td>121</td>\n      <td>813992.49</td>\n      <td>19</td>\n      <td>24794625.20</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>0</td>\n      <td>567</td>\n      <td>0</td>\n      <td>175203.45</td>\n      <td>53</td>\n      <td>175203.45</td>\n      <td>1</td>\n      <td>2735776.45</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>2</td>\n      <td>572</td>\n      <td>0</td>\n      <td>2000718.20</td>\n      <td>27</td>\n      <td>2000718.20</td>\n      <td>21</td>\n      <td>10984451.39</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>0</td>\n      <td>600</td>\n      <td>0</td>\n      <td>612229.86</td>\n      <td>68</td>\n      <td>612229.86</td>\n      <td>14</td>\n      <td>2275776.03</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>2</td>\n      <td>614</td>\n      <td>0</td>\n      <td>2739248.30</td>\n      <td>71</td>\n      <td>2739248.30</td>\n      <td>25</td>\n      <td>15505667.41</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>122 rows Ã— 9 columns</p>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df 0     2   WIRE_OUT    18627.02  C1375503918       18627.02#%%\n",
    "from kfp import compiler\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp.dsl import component, Output, Input, Dataset, pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "project_id = \"jesusarguelles-sandbox\"\n",
    "pipeline_root_path = \"gs://jesusarguelles-staging/\"\n",
    "bucket_id = \"jesusarguelles-datasets-public\"\n",
    "bucket_folder_name = \"money_laundering_detection\"\n",
    "raw_file_name = \"paysim_dataset.csv\"\n",
    "raw_data_full_path = f\"gs://{bucket_id}/{bucket_folder_name}/{raw_file_name}\"\n",
    "filtered_data_1 = \"filtered_data_1.csv\"\n",
    "filtered_data_2 = \"filtered_data_2.csv\"\n",
    "filtered_data_3 = \"filtered_data_3.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"gcsfs\", \"pandas\"]\n",
    ")\n",
    "def data_preprocess_stage_1(\n",
    "        raw_dataset: str,\n",
    "        output_dataset_one : Output[Dataset],\n",
    "        output_dataset_two : Output[Dataset]\n",
    "):\n",
    "    import os\n",
    "    import csv\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from random import randint\n",
    "\n",
    "\n",
    "    logging.warning(\"DATA PREPROCESSING 1 STAGE\")\n",
    "    logging.warning(\"Reading Dataset...\")\n",
    "\n",
    "    X = pd.read_csv(raw_dataset)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Dataset\")\n",
    "\n",
    "    nameOrigCol = 3\n",
    "    nameDestCol = 6\n",
    "    nameOrig = []\n",
    "    nameDest = []\n",
    "    nameCount = {}\n",
    "    namesWithMoreThanOneOccurrence = []\n",
    "\n",
    "    logging.warning(\"Checking Each Person's Transactions Count...\")\n",
    "\n",
    "    for name in X[:, nameOrigCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameOrig.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    for name in X[:, nameDestCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameDest.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    logging.warning(\"Count Identification Done\")\n",
    "\n",
    "    logging.warning(\"Calculating Median ...\")\n",
    "\n",
    "    countArr = []\n",
    "    count = 0\n",
    "    for attr, value in nameCount.items() :\n",
    "        if value > 40 :\n",
    "            countArr.append(value)\n",
    "            count += 1\n",
    "    median = np.median(countArr)\n",
    "\n",
    "    logging.warning(f\"Median : {median}\")\n",
    "\n",
    "    logging.warning(\"Filtering Data Based on Transactions Count...\")\n",
    "    csv_golden_data = []\n",
    "\n",
    "    for i in range(X.shape[0]) :\n",
    "        if nameCount.get(X[i, 3], -1) > 40 or nameCount.get(X[i, 6], -1) > 40 :\n",
    "            csv_golden_data.append(X[i, :])\n",
    "\n",
    "    logging.warning(\"Filtering Done\")\n",
    "\n",
    "    logging.warning(\"Storing Filtered Data in data_processed folder...\")\n",
    "\n",
    "    new_file_name = \"filtered_data.csv\"\n",
    "\n",
    "    with open(output_dataset_one.path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_golden_data)\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 2 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 1 dataset...\")\n",
    "\n",
    "    X = pd.DataFrame(csv_golden_data)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 1 dataset...\")\n",
    "\n",
    "    csv_dataset_primary = []\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 6\n",
    "    oldbalanceDest = 7\n",
    "    accountType = 8\n",
    "    isFraud = 9\n",
    "    isFlaggedFraud = 10\n",
    "\n",
    "    logging.warning(\"Changing Labels of Type Column ...\")\n",
    "\n",
    "    transfer = [\"WIRE_IN\", \"WIRE_OUT\"]\n",
    "    for i in range(X.shape[0]):\n",
    "        arr = []\n",
    "        arr.append(X[i,step])\n",
    "        if X[i,trans_type] ==\"PAYMENT\":\n",
    "            arr.append(\"CREDIT\")\n",
    "        elif X[i,trans_type] ==\"TRANSFER\":\n",
    "            arr.append(transfer[randint(0,1)])\n",
    "        else:\n",
    "            arr.append(X[i,trans_type])\n",
    "        arr.append(X[i,amount])\n",
    "        arr.append(X[i,nameOrig])\n",
    "        arr.append(X[i,oldbalanceOrg])\n",
    "        arr.append(X[i,nameDest])\n",
    "        arr.append(X[i,oldbalanceDest])\n",
    "        if X[i,trans_type] == \"TRANSFER\":\n",
    "            arr.append(\"FOREIGN\")\n",
    "        else:\n",
    "            arr.append(\"DOMESTIC\")\n",
    "\n",
    "        arr.append(X[i,isFraud])\n",
    "        arr.append(X[i,isFlaggedFraud])\n",
    "\n",
    "        csv_dataset_primary.append(arr)\n",
    "\n",
    "    logging.warning(\"Changing Labels Done\")\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "\n",
    "    columns=['step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "             'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_primary = pd.DataFrame(csv_dataset_primary, columns=columns)\n",
    "\n",
    "    data_primary.to_csv(output_dataset_two.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\"]\n",
    ")\n",
    "def data_preprocess_stage_2(\n",
    "        input_dataset: Input[Dataset],\n",
    "        output_dataset_three: Output[Dataset]\n",
    "):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    # data_path = f'gs://{bucket_id}/{folder_id}/filtered_data_2.csv'\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 3 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 2 dataset...\")\n",
    "\n",
    "    X = pd.read_csv(input_dataset.path)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 2 dataset\")\n",
    "\n",
    "    #col\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 5\n",
    "    oldbalanceDest = 6\n",
    "    accountType = 7\n",
    "    isFraud = 8\n",
    "\n",
    "    #col\n",
    "    entity = 0\n",
    "    incommingDomestic30 = 1\n",
    "    incommingDomestic60 = 2\n",
    "    incommingDomestic90 = 3\n",
    "    outgoingDomestic30 = 4\n",
    "    outgoingDomestic60 = 5\n",
    "    outgoingDomestic90 = 6\n",
    "    incommingForeign30 = 7\n",
    "    incommingForeign60 = 8\n",
    "    incommingForeign90 = 9\n",
    "    outgoingForeign30 = 10\n",
    "    outgoingForeign60 = 11\n",
    "    outgoingForeign90 = 12\n",
    "    incoming_domestic_count_30 = 13\n",
    "    incoming_domestic_count_60 = 14\n",
    "    incoming_domestic_count_90 = 15\n",
    "    outgoing_domestic_count_30 = 16\n",
    "    outgoing_domestic_count_60 = 17\n",
    "    outgoing_domestic_count_90 = 18\n",
    "    incoming_foreign_count_30 = 19\n",
    "    incoming_foreign_count_60 = 20\n",
    "    incoming_foreign_count_90 = 21\n",
    "    outgoing_foreign_count_30 = 22\n",
    "    outgoing_foreign_count_60 = 23\n",
    "    outgoing_foreign_count_90 = 24\n",
    "    balance_difference_30 = 25\n",
    "    balance_difference_60 = 26\n",
    "    balance_difference_90 = 27\n",
    "    isFraudSec = 28\n",
    "\n",
    "    csv_dataset_secondary = []\n",
    "    entities_pos = {}\n",
    "    enititesDict = {}\n",
    "\n",
    "    logging.warning(\"Creating New Features Using Transaction History...\")\n",
    "\n",
    "    def getSecRow(entity):\n",
    "        return [entity,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        source_entity = X[i,nameOrig]\n",
    "        dest_entity = X[i,nameDest]\n",
    "\n",
    "        source_pos = entities_pos.get(source_entity,-1)\n",
    "        if source_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[source_entity] = pos\n",
    "            source_pos = pos\n",
    "\n",
    "            row = getSecRow(source_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        dest_pos = entities_pos.get(dest_entity,-1)\n",
    "        if dest_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[dest_entity] = pos\n",
    "            dest_pos = pos\n",
    "\n",
    "            row = getSecRow(dest_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        transferAmountSource = 0\n",
    "        transferAmountDest = 0\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_IN\" or X[i,trans_type] == \"CREDIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_OUT\" or X[i,trans_type] == \"DEBIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_IN\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign90] += X[i,amount]\n",
    "                # print(dest_pos,outgoingForeign90,i,amount)\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_OUT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if enititesDict.get(source_entity,-1) == -1:\n",
    "            enititesDict[source_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceOrg],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        if enititesDict.get(dest_entity,-1) == -1:\n",
    "            enititesDict[dest_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceDest],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        incomingForSource = [\"CASH_IN\",\"CREDIT\",\"WIRE_IN\"]\n",
    "        incomingForDest = [\"CASH_OUT\",\"DEBIT\",\"WIRE_OUT\"]\n",
    "        outgoingForDest = incomingForSource\n",
    "        outgoingForSource = incomingForDest\n",
    "\n",
    "        if X[i,step]<=30:\n",
    "            enititesDict[source_entity]['day30Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day30Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "        if X[i,step]<=60:\n",
    "            enititesDict[source_entity]['day60Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day60Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "        if X[i,step]<=90:\n",
    "            enititesDict[source_entity]['day90Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day90Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][balance_difference_30] = enititesDict[source_entity]['day30Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_60] = enititesDict[source_entity]['day60Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_90] = enititesDict[source_entity]['day90Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_30] = enititesDict[source_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_30] = enititesDict[source_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_60] = enititesDict[source_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_60] = enititesDict[source_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_90] = enititesDict[source_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_90] = enititesDict[source_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_30] = enititesDict[source_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_30] = enititesDict[source_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_60] = enititesDict[source_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_60] = enititesDict[source_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_90] = enititesDict[source_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_90] = enititesDict[source_entity]['countOutgoingForeign90']\n",
    "\n",
    "        csv_dataset_secondary[source_pos][isFraudSec] = csv_dataset_secondary[source_pos][isFraudSec] or X[i,isFraud]\n",
    "\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_30] = enititesDict[dest_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_30] = enititesDict[dest_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_60] = enititesDict[dest_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_60] = enititesDict[dest_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_90] = enititesDict[dest_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_90] = enititesDict[dest_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_30] = enititesDict[dest_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_30] = enititesDict[dest_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_60] = enititesDict[dest_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_60] = enititesDict[dest_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_90] = enititesDict[dest_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_90] = enititesDict[dest_entity]['countOutgoingForeign90']\n",
    "\n",
    "\n",
    "    columns = ['entity','incoming_domestic_amount_30','incoming_domestic_amount_60','incoming_domestic_amount_90',\n",
    "               'outgoing_domestic_amount_30','outgoing_domestic_amount_60','outgoing_domestic_amount_90',\n",
    "               'incoming_foreign_amount_30','incoming_foreign_amount_60','incoming_foreign_amount_90',\n",
    "               'outgoing_foreign_amount_30','outgoing_foreign_amount_60','outgoing_foreign_amount_90',\n",
    "               'incoming_domestic_count_30','incoming_domestic_count_60','incoming_domestic_count_90',\n",
    "               'outgoing_domestic_count_30','outgoing_domestic_count_60','outgoing_domestic_count_90',\n",
    "               'incoming_foreign_count_30','incoming_foreign_count_60','incoming_foreign_count_90',\n",
    "               'outgoing_foreign_count_30','outgoing_foreign_count_60','outgoing_foreign_count_90',\n",
    "               'balance_difference_30','balance_difference_60','balance_difference_90','isFraud']\n",
    "\n",
    "    logging.warning(\"Creating New Features Done\")\n",
    "\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "    # filtered_data_3.csv\n",
    "    data_secondary = pd.DataFrame(csv_dataset_secondary, columns=columns)\n",
    "    data_secondary.to_csv(output_dataset_three.path,index=False)\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def feature_selection(\n",
    "        input_dataset : Input[Dataset],\n",
    "        output_dataset : Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1: filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param output_dataset: feature_importances.csv\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"FEATURE SELECTION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    dataframeX = pd.read_csv(input_dataset.path)\n",
    "    col_names = list(dataframeX.columns.values)\n",
    "    dataMat = dataframeX.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    logging.warning(\"Creating X and Y Variables...\")\n",
    "\n",
    "    X = dataMat[:,1:-2]\n",
    "    Y = dataMat[:,-1]\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of Y: {Y.shape}\")\n",
    "\n",
    "    logging.warning(\"Instiantiating Random Forest Model...\")\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    logging.warning(\"Fitting Data...\")\n",
    "\n",
    "    model.fit(X, Y.astype(int))\n",
    "\n",
    "    logging.warning(\"Checking Feature Importances...\")\n",
    "\n",
    "    feature_imp = model.feature_importances_\n",
    "\n",
    "    sorted_feature_vals = np.sort(feature_imp)\n",
    "    sorted_feature_indexes = np.argsort(feature_imp)\n",
    "\n",
    "    logging.warning(\"Significant Features in decreasing order of importance: \")\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances in reports...\")\n",
    "\n",
    "    fea_imp = [[col_names[i+2], feature_imp[i]] for i in reversed(sorted_feature_indexes)]\n",
    "    features = pd.DataFrame(fea_imp, columns=[\"features\", \"importance_score\"])\n",
    "    # feature_importances.csv\n",
    "    features.to_csv(output_dataset.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def segment_generation(\n",
    "        input_dataset_1 : Input[Dataset],\n",
    "        input_dataset_2 : Input[Dataset],\n",
    "        input_dataset_3 : Input[Dataset],\n",
    "        silhoutte_scores : Output[Dataset],\n",
    "        final_dataset_output: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1:  filtered_data_2.csv from data-preprocess-stage-1\n",
    "    :param input_dataset_2:  filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param input_dataset_3:  feature_importances.csv from features_election\n",
    "    :param silhoutte_scores: Metrics\n",
    "    :param final_dataset_output: final_dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Segment Generation\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"SEGMENT GENERATOR STAGE\")\n",
    "\n",
    "    def getClusterPredictions(data, true_k):\n",
    "        model = KMeans(n_clusters=true_k)\n",
    "        model.fit(data)\n",
    "        prediction = model.predict(data)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def getBestCluster(X,_min=2,_max=10):\n",
    "        selected_cluster = 0\n",
    "        previous_sil_coeff = 0.001 #some random small number not 0\n",
    "        sc_vals = []\n",
    "        for n_cluster in range(_min, _max):\n",
    "            kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "            label = kmeans.labels_\n",
    "\n",
    "            sil_coeff = silhouette_score(X, label, metric='euclidean', sample_size=1000)\n",
    "            sc_vals.append(sil_coeff)\n",
    "            # print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))\n",
    "\n",
    "            percent_change = (sil_coeff-previous_sil_coeff)*100/previous_sil_coeff\n",
    "\n",
    "            # return when below a threshold of 1%\n",
    "            if percent_change<1:\n",
    "                selected_cluster = n_cluster-1\n",
    "\n",
    "            previous_sil_coeff = sil_coeff\n",
    "\n",
    "        return selected_cluster or _max, sc_vals\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    X_dataframe = pd.read_csv(input_dataset_2.path)\n",
    "    X = X_dataframe.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    col_names = list(X_dataframe.columns.values)\n",
    "\n",
    "    X_trimmed_features = np.zeros((X.shape[0],1))\n",
    "\n",
    "    logging.warning(\"Importing Feature Importances...\")\n",
    "\n",
    "    #feature_path = f'gs://{bucket_id}/{folder_id}/feature_importances.csv'\n",
    "    features = pd.read_csv(input_dataset_3.path)\n",
    "\n",
    "    logging.warning(\"Selecting Top 13 Features for CLustering...\")\n",
    "\n",
    "    top_13 = features.iloc[:13, 0].tolist()\n",
    "\n",
    "    logging.warning(\"Top 13 Features stored in List\")\n",
    "\n",
    "    for feature in top_13:\n",
    "        X_trimmed_features = np.concatenate((X_trimmed_features,np.expand_dims(X_dataframe[feature],axis=1)),axis=1)\n",
    "    X_trimmed_features = X_trimmed_features[:,1:]\n",
    "\n",
    "    logging.warning(\"Choosing Best Number Of Clusters...\")\n",
    "\n",
    "    min_value = 2\n",
    "    max_value = 10\n",
    "    true_k, sc_vals = getBestCluster(X_trimmed_features,_min=min_value,_max=max_value)\n",
    "    true_k = 5\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores...\")\n",
    "\n",
    "\n",
    "    sil_score = [[i, sc_vals[i-min_value]] for i in range(min_value, max_value)]\n",
    "    sil = pd.DataFrame(sil_score, columns=[\"no_of_clusters\", \"silhoutte_score\"])\n",
    "    sil.to_csv(silhoutte_scores.path, index=False)\n",
    "    #sil.to_csv(f'gs://{bucket_id}/{folder_id}/silhoutte_scores.csv', index=False)\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores Done\")\n",
    "\n",
    "    logging.warning(\"Creating Clusters with Best No Of Clusters...\")\n",
    "\n",
    "    prediction = getClusterPredictions(X_trimmed_features, true_k)\n",
    "    seg_dict = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        seg_dict[X[i,0]] = prediction[i]\n",
    "\n",
    "    logging.warning(\"Inputing Filtered Data 2 Dataset...\")\n",
    "\n",
    "    X_dataframe_pri = pd.read_csv(input_dataset_1.path)\n",
    "    X_pri = X_dataframe_pri.to_numpy()\n",
    "    col_names = list(X_dataframe_pri.columns.values)\n",
    "\n",
    "    logging.warning(\"Read Filtered 2 Data\")\n",
    "\n",
    "    logging.warning(\"Creating Final Dataset with segments...\")\n",
    "\n",
    "    X_with_segments = []\n",
    "    for i in range(X_pri.shape[0]):\n",
    "        X_with_segments.append(np.concatenate(([[seg_dict[X_pri[i,3]]]],np.expand_dims(X_pri[i,:],axis=0)),axis=1)[0])\n",
    "\n",
    "    segmented_columns = ['segment','step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "                         'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_segmented = pd.DataFrame(X_with_segments, columns = segmented_columns)\n",
    "    data_segmented = data_segmented.drop('isFlaggedFraud', axis=1)\n",
    "    data_segmented.to_csv(final_dataset_output.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Final Dataset Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\", \"catboost\"]\n",
    ")\n",
    "def training(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str,\n",
    "        input_dataset : Input[Dataset],\n",
    "        model_path: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset: fina_dataset.csv from segment-generation\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pickle\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"MODEL CREATION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Final Dataset...\")\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_id)\n",
    "\n",
    "    dataMat = pd.read_csv(input_dataset.path)\n",
    "    data = dataMat.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Final Dataset\")\n",
    "\n",
    "    logging.warning(\"Checking Categorical Features...\")\n",
    "\n",
    "    cat_feat = [i for i in dataMat.columns if dataMat[i].dtypes == 'O']\n",
    "\n",
    "    logging.warning(\"Checking Missing Values...\")\n",
    "\n",
    "    a = dict(dataMat.isnull().sum())\n",
    "    b = [[i, a[i]] for i in a.keys()]\n",
    "    missing = pd.DataFrame(b, columns=['features', 'null_values_count'])\n",
    "\n",
    "    logging.warning(\"Storing Missing Values...\")\n",
    "\n",
    "    missing.to_csv(\"missing_values.csv\", index=False)\n",
    "\n",
    "    logging.warning(\"Storing Missing Values Done\")\n",
    "\n",
    "    logging.warning(\"Encoding Categorical Features...\")\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    for i in cat_feat:\n",
    "        dataMat[i] = encoder.fit_transform(dataMat[i])\n",
    "\n",
    "    blob = bucket.blob(f\"{bucket_folder}/label_encoder.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    logging.warning(\"Features Encoding Done\")\n",
    "\n",
    "    logging.warning(\"Creating X and y variables ...\")\n",
    "\n",
    "    X = dataMat.iloc[:, :-1]\n",
    "    y = dataMat['isFraud']\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of y: {y.shape}\")\n",
    "\n",
    "    logging.warning(\"Splitting Dataset...\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    logging.warning(\"Instantiating Model...\")\n",
    "\n",
    "    model = CatBoostClassifier(random_state=42, class_weights={0:1, 1:12}, silent=True)\n",
    "\n",
    "    logging.warning(\"Fitting Model...\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_cat = model.predict(X_test)\n",
    "\n",
    "    logging.warning(\"Saving Model...\")\n",
    "\n",
    "    #model_path = \"model.pkl\"\n",
    "    blob = bucket.blob(f\"{bucket_folder}/model.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    logging.warning(\"Saving Model Metrics...\")\n",
    "\n",
    "    metric_file_path = \"performance.json\"\n",
    "    # with open(metric_file_path, \"r\") as f:\n",
    "    #     data = json.load(f)\n",
    "\n",
    "    model_metric = {\n",
    "        \"time_stamp\": datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_cat).tolist(),\n",
    "        \"precision\": precision_score(y_test, y_pred_cat),\n",
    "        \"recall\": recall_score(y_test, y_pred_cat),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_cat)\n",
    "    }\n",
    "\n",
    "    # data['model_metric'].append(model_metric)\n",
    "    # with open(metric_file_path, \"w\") as f:\n",
    "    #     json.dump(data, f, indent=4)\n",
    "\n",
    "    logging.warning(\"Model Metrics Stored\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "\n",
    "@pipeline(name=\"money_laundering_detection\")\n",
    "def pipeline(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str\n",
    "):\n",
    "    preproces_job_1 = data_preprocess_stage_1(raw_dataset=raw_data_full_path)\n",
    "    preprocess_job_2 = data_preprocess_stage_2(input_dataset=preproces_job_1.outputs[\"output_dataset_two\"])\n",
    "    feature_selection_job = feature_selection(input_dataset=preprocess_job_2.outputs[\"output_dataset_three\"])\n",
    "    segment_generation_job = segment_generation(\n",
    "        input_dataset_1=preproces_job_1.outputs[\"output_dataset_two\"],\n",
    "        input_dataset_2=preprocess_job_2.outputs[\"output_dataset_three\"],\n",
    "        input_dataset_3=feature_selection_job.outputs[\"output_dataset\"]\n",
    "    )\n",
    "    training_job = training(\n",
    "        project_id = project_id,\n",
    "        bucket_id = bucket_id,\n",
    "        bucket_folder = bucket_folder,\n",
    "        input_dataset=segment_generation_job.outputs[\"final_dataset_output\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='money_laundering_detection.yaml'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=project_id,\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"money_laundering_detection\",\n",
    "    template_path=\"money_laundering_detection.yaml\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,\n",
    "        'bucket_id': bucket_id,\n",
    "        'bucket_folder': bucket_folder_name\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = storage.Client(project=\"jesusarguelles-sandbox\").bucket(\"jesusarguelles-datasets-public\")\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/model.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/label_encoder.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    encoder = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [1],\n",
    "    \"trans_type\": [\"DEBIT\"],\n",
    "    \"amount\": [181.00],\n",
    "    \"nameOrig\": [\"C1900366749\"],\n",
    "    \"oldbalanceOrg\": [4465.0],\n",
    "    \"nameDest\": [\"C997608398\"],\n",
    "    \"oldbalanceDest\": [\"10845.0\"],\n",
    "    \"accountType\": [\"DOMESTIC\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.isFraud == 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "to_predict_df = df[df.isFraud == 1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.isFraud == 1].iloc[7,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df = df[df.isFraud == 1].iloc[:,:-1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df = df[df.isFraud == 1].iloc[:,:-1]\n",
    "to_predict_df.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "print(to_predict_df.head(9))\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "print(model.predict(to_predict_df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " 0     2   WIRE_OUT    18627.02  C1375503918       18627.02#%%\n",
    "from kfp import compiler\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp.dsl import component, Output, Input, Dataset, pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "project_id = \"jesusarguelles-sandbox\"\n",
    "pipeline_root_path = \"gs://jesusarguelles-staging/\"\n",
    "bucket_id = \"jesusarguelles-datasets-public\"\n",
    "bucket_folder_name = \"money_laundering_detection\"\n",
    "raw_file_name = \"paysim_dataset.csv\"\n",
    "raw_data_full_path = f\"gs://{bucket_id}/{bucket_folder_name}/{raw_file_name}\"\n",
    "filtered_data_1 = \"filtered_data_1.csv\"\n",
    "filtered_data_2 = \"filtered_data_2.csv\"\n",
    "filtered_data_3 = \"filtered_data_3.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"gcsfs\", \"pandas\"]\n",
    ")\n",
    "def data_preprocess_stage_1(\n",
    "        raw_dataset: str,\n",
    "        output_dataset_one : Output[Dataset],\n",
    "        output_dataset_two : Output[Dataset]\n",
    "):\n",
    "    import os\n",
    "    import csv\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from random import randint\n",
    "\n",
    "\n",
    "    logging.warning(\"DATA PREPROCESSING 1 STAGE\")\n",
    "    logging.warning(\"Reading Dataset...\")\n",
    "\n",
    "    X = pd.read_csv(raw_dataset)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Dataset\")\n",
    "\n",
    "    nameOrigCol = 3\n",
    "    nameDestCol = 6\n",
    "    nameOrig = []\n",
    "    nameDest = []\n",
    "    nameCount = {}\n",
    "    namesWithMoreThanOneOccurrence = []\n",
    "\n",
    "    logging.warning(\"Checking Each Person's Transactions Count...\")\n",
    "\n",
    "    for name in X[:, nameOrigCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameOrig.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    for name in X[:, nameDestCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameDest.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    logging.warning(\"Count Identification Done\")\n",
    "\n",
    "    logging.warning(\"Calculating Median ...\")\n",
    "\n",
    "    countArr = []\n",
    "    count = 0\n",
    "    for attr, value in nameCount.items() :\n",
    "        if value > 40 :\n",
    "            countArr.append(value)\n",
    "            count += 1\n",
    "    median = np.median(countArr)\n",
    "\n",
    "    logging.warning(f\"Median : {median}\")\n",
    "\n",
    "    logging.warning(\"Filtering Data Based on Transactions Count...\")\n",
    "    csv_golden_data = []\n",
    "\n",
    "    for i in range(X.shape[0]) :\n",
    "        if nameCount.get(X[i, 3], -1) > 40 or nameCount.get(X[i, 6], -1) > 40 :\n",
    "            csv_golden_data.append(X[i, :])\n",
    "\n",
    "    logging.warning(\"Filtering Done\")\n",
    "\n",
    "    logging.warning(\"Storing Filtered Data in data_processed folder...\")\n",
    "\n",
    "    new_file_name = \"filtered_data.csv\"\n",
    "\n",
    "    with open(output_dataset_one.path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_golden_data)\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 2 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 1 dataset...\")\n",
    "\n",
    "    X = pd.DataFrame(csv_golden_data)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 1 dataset...\")\n",
    "\n",
    "    csv_dataset_primary = []\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 6\n",
    "    oldbalanceDest = 7\n",
    "    accountType = 8\n",
    "    isFraud = 9\n",
    "    isFlaggedFraud = 10\n",
    "\n",
    "    logging.warning(\"Changing Labels of Type Column ...\")\n",
    "\n",
    "    transfer = [\"WIRE_IN\", \"WIRE_OUT\"]\n",
    "    for i in range(X.shape[0]):\n",
    "        arr = []\n",
    "        arr.append(X[i,step])\n",
    "        if X[i,trans_type] ==\"PAYMENT\":\n",
    "            arr.append(\"CREDIT\")\n",
    "        elif X[i,trans_type] ==\"TRANSFER\":\n",
    "            arr.append(transfer[randint(0,1)])\n",
    "        else:\n",
    "            arr.append(X[i,trans_type])\n",
    "        arr.append(X[i,amount])\n",
    "        arr.append(X[i,nameOrig])\n",
    "        arr.append(X[i,oldbalanceOrg])\n",
    "        arr.append(X[i,nameDest])\n",
    "        arr.append(X[i,oldbalanceDest])\n",
    "        if X[i,trans_type] == \"TRANSFER\":\n",
    "            arr.append(\"FOREIGN\")\n",
    "        else:\n",
    "            arr.append(\"DOMESTIC\")\n",
    "\n",
    "        arr.append(X[i,isFraud])\n",
    "        arr.append(X[i,isFlaggedFraud])\n",
    "\n",
    "        csv_dataset_primary.append(arr)\n",
    "\n",
    "    logging.warning(\"Changing Labels Done\")\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "\n",
    "    columns=['step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "             'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_primary = pd.DataFrame(csv_dataset_primary, columns=columns)\n",
    "\n",
    "    data_primary.to_csv(output_dataset_two.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\"]\n",
    ")\n",
    "def data_preprocess_stage_2(\n",
    "        input_dataset: Input[Dataset],\n",
    "        output_dataset_three: Output[Dataset]\n",
    "):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    # data_path = f'gs://{bucket_id}/{folder_id}/filtered_data_2.csv'\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 3 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 2 dataset...\")\n",
    "\n",
    "    X = pd.read_csv(input_dataset.path)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 2 dataset\")\n",
    "\n",
    "    #col\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 5\n",
    "    oldbalanceDest = 6\n",
    "    accountType = 7\n",
    "    isFraud = 8\n",
    "\n",
    "    #col\n",
    "    entity = 0\n",
    "    incommingDomestic30 = 1\n",
    "    incommingDomestic60 = 2\n",
    "    incommingDomestic90 = 3\n",
    "    outgoingDomestic30 = 4\n",
    "    outgoingDomestic60 = 5\n",
    "    outgoingDomestic90 = 6\n",
    "    incommingForeign30 = 7\n",
    "    incommingForeign60 = 8\n",
    "    incommingForeign90 = 9\n",
    "    outgoingForeign30 = 10\n",
    "    outgoingForeign60 = 11\n",
    "    outgoingForeign90 = 12\n",
    "    incoming_domestic_count_30 = 13\n",
    "    incoming_domestic_count_60 = 14\n",
    "    incoming_domestic_count_90 = 15\n",
    "    outgoing_domestic_count_30 = 16\n",
    "    outgoing_domestic_count_60 = 17\n",
    "    outgoing_domestic_count_90 = 18\n",
    "    incoming_foreign_count_30 = 19\n",
    "    incoming_foreign_count_60 = 20\n",
    "    incoming_foreign_count_90 = 21\n",
    "    outgoing_foreign_count_30 = 22\n",
    "    outgoing_foreign_count_60 = 23\n",
    "    outgoing_foreign_count_90 = 24\n",
    "    balance_difference_30 = 25\n",
    "    balance_difference_60 = 26\n",
    "    balance_difference_90 = 27\n",
    "    isFraudSec = 28\n",
    "\n",
    "    csv_dataset_secondary = []\n",
    "    entities_pos = {}\n",
    "    enititesDict = {}\n",
    "\n",
    "    logging.warning(\"Creating New Features Using Transaction History...\")\n",
    "\n",
    "    def getSecRow(entity):\n",
    "        return [entity,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        source_entity = X[i,nameOrig]\n",
    "        dest_entity = X[i,nameDest]\n",
    "\n",
    "        source_pos = entities_pos.get(source_entity,-1)\n",
    "        if source_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[source_entity] = pos\n",
    "            source_pos = pos\n",
    "\n",
    "            row = getSecRow(source_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        dest_pos = entities_pos.get(dest_entity,-1)\n",
    "        if dest_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[dest_entity] = pos\n",
    "            dest_pos = pos\n",
    "\n",
    "            row = getSecRow(dest_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        transferAmountSource = 0\n",
    "        transferAmountDest = 0\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_IN\" or X[i,trans_type] == \"CREDIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_OUT\" or X[i,trans_type] == \"DEBIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_IN\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign90] += X[i,amount]\n",
    "                # print(dest_pos,outgoingForeign90,i,amount)\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_OUT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if enititesDict.get(source_entity,-1) == -1:\n",
    "            enititesDict[source_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceOrg],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        if enititesDict.get(dest_entity,-1) == -1:\n",
    "            enititesDict[dest_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceDest],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        incomingForSource = [\"CASH_IN\",\"CREDIT\",\"WIRE_IN\"]\n",
    "        incomingForDest = [\"CASH_OUT\",\"DEBIT\",\"WIRE_OUT\"]\n",
    "        outgoingForDest = incomingForSource\n",
    "        outgoingForSource = incomingForDest\n",
    "\n",
    "        if X[i,step]<=30:\n",
    "            enititesDict[source_entity]['day30Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day30Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "        if X[i,step]<=60:\n",
    "            enititesDict[source_entity]['day60Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day60Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "        if X[i,step]<=90:\n",
    "            enititesDict[source_entity]['day90Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day90Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][balance_difference_30] = enititesDict[source_entity]['day30Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_60] = enititesDict[source_entity]['day60Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_90] = enititesDict[source_entity]['day90Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_30] = enititesDict[source_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_30] = enititesDict[source_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_60] = enititesDict[source_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_60] = enititesDict[source_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_90] = enititesDict[source_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_90] = enititesDict[source_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_30] = enititesDict[source_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_30] = enititesDict[source_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_60] = enititesDict[source_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_60] = enititesDict[source_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_90] = enititesDict[source_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_90] = enititesDict[source_entity]['countOutgoingForeign90']\n",
    "\n",
    "        csv_dataset_secondary[source_pos][isFraudSec] = csv_dataset_secondary[source_pos][isFraudSec] or X[i,isFraud]\n",
    "\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_30] = enititesDict[dest_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_30] = enititesDict[dest_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_60] = enititesDict[dest_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_60] = enititesDict[dest_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_90] = enititesDict[dest_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_90] = enititesDict[dest_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_30] = enititesDict[dest_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_30] = enititesDict[dest_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_60] = enititesDict[dest_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_60] = enititesDict[dest_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_90] = enititesDict[dest_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_90] = enititesDict[dest_entity]['countOutgoingForeign90']\n",
    "\n",
    "\n",
    "    columns = ['entity','incoming_domestic_amount_30','incoming_domestic_amount_60','incoming_domestic_amount_90',\n",
    "               'outgoing_domestic_amount_30','outgoing_domestic_amount_60','outgoing_domestic_amount_90',\n",
    "               'incoming_foreign_amount_30','incoming_foreign_amount_60','incoming_foreign_amount_90',\n",
    "               'outgoing_foreign_amount_30','outgoing_foreign_amount_60','outgoing_foreign_amount_90',\n",
    "               'incoming_domestic_count_30','incoming_domestic_count_60','incoming_domestic_count_90',\n",
    "               'outgoing_domestic_count_30','outgoing_domestic_count_60','outgoing_domestic_count_90',\n",
    "               'incoming_foreign_count_30','incoming_foreign_count_60','incoming_foreign_count_90',\n",
    "               'outgoing_foreign_count_30','outgoing_foreign_count_60','outgoing_foreign_count_90',\n",
    "               'balance_difference_30','balance_difference_60','balance_difference_90','isFraud']\n",
    "\n",
    "    logging.warning(\"Creating New Features Done\")\n",
    "\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "    # filtered_data_3.csv\n",
    "    data_secondary = pd.DataFrame(csv_dataset_secondary, columns=columns)\n",
    "    data_secondary.to_csv(output_dataset_three.path,index=False)\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def feature_selection(\n",
    "        input_dataset : Input[Dataset],\n",
    "        output_dataset : Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1: filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param output_dataset: feature_importances.csv\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"FEATURE SELECTION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    dataframeX = pd.read_csv(input_dataset.path)\n",
    "    col_names = list(dataframeX.columns.values)\n",
    "    dataMat = dataframeX.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    logging.warning(\"Creating X and Y Variables...\")\n",
    "\n",
    "    X = dataMat[:,1:-2]\n",
    "    Y = dataMat[:,-1]\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of Y: {Y.shape}\")\n",
    "\n",
    "    logging.warning(\"Instiantiating Random Forest Model...\")\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    logging.warning(\"Fitting Data...\")\n",
    "\n",
    "    model.fit(X, Y.astype(int))\n",
    "\n",
    "    logging.warning(\"Checking Feature Importances...\")\n",
    "\n",
    "    feature_imp = model.feature_importances_\n",
    "\n",
    "    sorted_feature_vals = np.sort(feature_imp)\n",
    "    sorted_feature_indexes = np.argsort(feature_imp)\n",
    "\n",
    "    logging.warning(\"Significant Features in decreasing order of importance: \")\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances in reports...\")\n",
    "\n",
    "    fea_imp = [[col_names[i+2], feature_imp[i]] for i in reversed(sorted_feature_indexes)]\n",
    "    features = pd.DataFrame(fea_imp, columns=[\"features\", \"importance_score\"])\n",
    "    # feature_importances.csv\n",
    "    features.to_csv(output_dataset.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def segment_generation(\n",
    "        input_dataset_1 : Input[Dataset],\n",
    "        input_dataset_2 : Input[Dataset],\n",
    "        input_dataset_3 : Input[Dataset],\n",
    "        silhoutte_scores : Output[Dataset],\n",
    "        final_dataset_output: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1:  filtered_data_2.csv from data-preprocess-stage-1\n",
    "    :param input_dataset_2:  filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param input_dataset_3:  feature_importances.csv from features_election\n",
    "    :param silhoutte_scores: Metrics\n",
    "    :param final_dataset_output: final_dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Segment Generation\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"SEGMENT GENERATOR STAGE\")\n",
    "\n",
    "    def getClusterPredictions(data, true_k):\n",
    "        model = KMeans(n_clusters=true_k)\n",
    "        model.fit(data)\n",
    "        prediction = model.predict(data)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def getBestCluster(X,_min=2,_max=10):\n",
    "        selected_cluster = 0\n",
    "        previous_sil_coeff = 0.001 #some random small number not 0\n",
    "        sc_vals = []\n",
    "        for n_cluster in range(_min, _max):\n",
    "            kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "            label = kmeans.labels_\n",
    "\n",
    "            sil_coeff = silhouette_score(X, label, metric='euclidean', sample_size=1000)\n",
    "            sc_vals.append(sil_coeff)\n",
    "            # print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))\n",
    "\n",
    "            percent_change = (sil_coeff-previous_sil_coeff)*100/previous_sil_coeff\n",
    "\n",
    "            # return when below a threshold of 1%\n",
    "            if percent_change<1:\n",
    "                selected_cluster = n_cluster-1\n",
    "\n",
    "            previous_sil_coeff = sil_coeff\n",
    "\n",
    "        return selected_cluster or _max, sc_vals\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    X_dataframe = pd.read_csv(input_dataset_2.path)\n",
    "    X = X_dataframe.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    col_names = list(X_dataframe.columns.values)\n",
    "\n",
    "    X_trimmed_features = np.zeros((X.shape[0],1))\n",
    "\n",
    "    logging.warning(\"Importing Feature Importances...\")\n",
    "\n",
    "    #feature_path = f'gs://{bucket_id}/{folder_id}/feature_importances.csv'\n",
    "    features = pd.read_csv(input_dataset_3.path)\n",
    "\n",
    "    logging.warning(\"Selecting Top 13 Features for CLustering...\")\n",
    "\n",
    "    top_13 = features.iloc[:13, 0].tolist()\n",
    "\n",
    "    logging.warning(\"Top 13 Features stored in List\")\n",
    "\n",
    "    for feature in top_13:\n",
    "        X_trimmed_features = np.concatenate((X_trimmed_features,np.expand_dims(X_dataframe[feature],axis=1)),axis=1)\n",
    "    X_trimmed_features = X_trimmed_features[:,1:]\n",
    "\n",
    "    logging.warning(\"Choosing Best Number Of Clusters...\")\n",
    "\n",
    "    min_value = 2\n",
    "    max_value = 10\n",
    "    true_k, sc_vals = getBestCluster(X_trimmed_features,_min=min_value,_max=max_value)\n",
    "    true_k = 5\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores...\")\n",
    "\n",
    "\n",
    "    sil_score = [[i, sc_vals[i-min_value]] for i in range(min_value, max_value)]\n",
    "    sil = pd.DataFrame(sil_score, columns=[\"no_of_clusters\", \"silhoutte_score\"])\n",
    "    sil.to_csv(silhoutte_scores.path, index=False)\n",
    "    #sil.to_csv(f'gs://{bucket_id}/{folder_id}/silhoutte_scores.csv', index=False)\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores Done\")\n",
    "\n",
    "    logging.warning(\"Creating Clusters with Best No Of Clusters...\")\n",
    "\n",
    "    prediction = getClusterPredictions(X_trimmed_features, true_k)\n",
    "    seg_dict = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        seg_dict[X[i,0]] = prediction[i]\n",
    "\n",
    "    logging.warning(\"Inputing Filtered Data 2 Dataset...\")\n",
    "\n",
    "    X_dataframe_pri = pd.read_csv(input_dataset_1.path)\n",
    "    X_pri = X_dataframe_pri.to_numpy()\n",
    "    col_names = list(X_dataframe_pri.columns.values)\n",
    "\n",
    "    logging.warning(\"Read Filtered 2 Data\")\n",
    "\n",
    "    logging.warning(\"Creating Final Dataset with segments...\")\n",
    "\n",
    "    X_with_segments = []\n",
    "    for i in range(X_pri.shape[0]):\n",
    "        X_with_segments.append(np.concatenate(([[seg_dict[X_pri[i,3]]]],np.expand_dims(X_pri[i,:],axis=0)),axis=1)[0])\n",
    "\n",
    "    segmented_columns = ['segment','step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "                         'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_segmented = pd.DataFrame(X_with_segments, columns = segmented_columns)\n",
    "    data_segmented = data_segmented.drop('isFlaggedFraud', axis=1)\n",
    "    data_segmented.to_csv(final_dataset_output.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Final Dataset Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\", \"catboost\"]\n",
    ")\n",
    "def training(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str,\n",
    "        input_dataset : Input[Dataset],\n",
    "        model_path: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset: fina_dataset.csv from segment-generation\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pickle\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"MODEL CREATION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Final Dataset...\")\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_id)\n",
    "\n",
    "    dataMat = pd.read_csv(input_dataset.path)\n",
    "    data = dataMat.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Final Dataset\")\n",
    "\n",
    "    logging.warning(\"Checking Categorical Features...\")\n",
    "\n",
    "    cat_feat = [i for i in dataMat.columns if dataMat[i].dtypes == 'O']\n",
    "\n",
    "    logging.warning(\"Checking Missing Values...\")\n",
    "\n",
    "    a = dict(dataMat.isnull().sum())\n",
    "    b = [[i, a[i]] for i in a.keys()]\n",
    "    missing = pd.DataFrame(b, columns=['features', 'null_values_count'])\n",
    "\n",
    "    logging.warning(\"Storing Missing Values...\")\n",
    "\n",
    "    missing.to_csv(\"missing_values.csv\", index=False)\n",
    "\n",
    "    logging.warning(\"Storing Missing Values Done\")\n",
    "\n",
    "    logging.warning(\"Encoding Categorical Features...\")\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    for i in cat_feat:\n",
    "        dataMat[i] = encoder.fit_transform(dataMat[i])\n",
    "\n",
    "    blob = bucket.blob(f\"{bucket_folder}/label_encoder.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    logging.warning(\"Features Encoding Done\")\n",
    "\n",
    "    logging.warning(\"Creating X and y variables ...\")\n",
    "\n",
    "    X = dataMat.iloc[:, :-1]\n",
    "    y = dataMat['isFraud']\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of y: {y.shape}\")\n",
    "\n",
    "    logging.warning(\"Splitting Dataset...\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    logging.warning(\"Instantiating Model...\")\n",
    "\n",
    "    model = CatBoostClassifier(random_state=42, class_weights={0:1, 1:12}, silent=True)\n",
    "\n",
    "    logging.warning(\"Fitting Model...\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_cat = model.predict(X_test)\n",
    "\n",
    "    logging.warning(\"Saving Model...\")\n",
    "\n",
    "    #model_path = \"model.pkl\"\n",
    "    blob = bucket.blob(f\"{bucket_folder}/model.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    logging.warning(\"Saving Model Metrics...\")\n",
    "\n",
    "    metric_file_path = \"performance.json\"\n",
    "    # with open(metric_file_path, \"r\") as f:\n",
    "    #     data = json.load(f)\n",
    "\n",
    "    model_metric = {\n",
    "        \"time_stamp\": datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_cat).tolist(),\n",
    "        \"precision\": precision_score(y_test, y_pred_cat),\n",
    "        \"recall\": recall_score(y_test, y_pred_cat),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_cat)\n",
    "    }\n",
    "\n",
    "    # data['model_metric'].append(model_metric)\n",
    "    # with open(metric_file_path, \"w\") as f:\n",
    "    #     json.dump(data, f, indent=4)\n",
    "\n",
    "    logging.warning(\"Model Metrics Stored\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "\n",
    "@pipeline(name=\"money_laundering_detection\")\n",
    "def pipeline(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str\n",
    "):\n",
    "    preproces_job_1 = data_preprocess_stage_1(raw_dataset=raw_data_full_path)\n",
    "    preprocess_job_2 = data_preprocess_stage_2(input_dataset=preproces_job_1.outputs[\"output_dataset_two\"])\n",
    "    feature_selection_job = feature_selection(input_dataset=preprocess_job_2.outputs[\"output_dataset_three\"])\n",
    "    segment_generation_job = segment_generation(\n",
    "        input_dataset_1=preproces_job_1.outputs[\"output_dataset_two\"],\n",
    "        input_dataset_2=preprocess_job_2.outputs[\"output_dataset_three\"],\n",
    "        input_dataset_3=feature_selection_job.outputs[\"output_dataset\"]\n",
    "    )\n",
    "    training_job = training(\n",
    "        project_id = project_id,\n",
    "        bucket_id = bucket_id,\n",
    "        bucket_folder = bucket_folder,\n",
    "        input_dataset=segment_generation_job.outputs[\"final_dataset_output\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='money_laundering_detection.yaml'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=project_id,\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"money_laundering_detection\",\n",
    "    template_path=\"money_laundering_detection.yaml\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,\n",
    "        'bucket_id': bucket_id,\n",
    "        'bucket_folder': bucket_folder_name\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = storage.Client(project=\"jesusarguelles-sandbox\").bucket(\"jesusarguelles-datasets-public\")\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/model.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/label_encoder.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    encoder = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [1],\n",
    "    \"trans_type\": [\"DEBIT\"],\n",
    "    \"amount\": [181.00],\n",
    "    \"nameOrig\": [\"C1900366749\"],\n",
    "    \"oldbalanceOrg\": [4465.0],\n",
    "    \"nameDest\": [\"C997608398\"],\n",
    "    \"oldbalanceDest\": [\"10845.0\"],\n",
    "    \"accountType\": [\"DOMESTIC\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.isFraud == 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "to_predict_df = df[df.isFraud == 1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.isFraud == 1].iloc[7,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df = df[df.isFraud == 1].iloc[:,:-1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df.reset_index(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df = df[df.isFraud == 1].iloc[:,:-1]\n",
    "to_predict_df.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "print(to_predict_df.head(9))\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])\n",
    "print(model.predict(to_predict_df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.fit_transform(to_predict_df[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_predict_df 0     2   WIRE_OUT    18627.02  C1375503918       18627.02#%%\n",
    "from kfp import compiler\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp.dsl import component, Output, Input, Dataset, pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "project_id = \"jesusarguelles-sandbox\"\n",
    "pipeline_root_path = \"gs://jesusarguelles-staging/\"\n",
    "bucket_id = \"jesusarguelles-datasets-public\"\n",
    "bucket_folder_name = \"money_laundering_detection\"\n",
    "raw_file_name = \"paysim_dataset.csv\"\n",
    "raw_data_full_path = f\"gs://{bucket_id}/{bucket_folder_name}/{raw_file_name}\"\n",
    "filtered_data_1 = \"filtered_data_1.csv\"\n",
    "filtered_data_2 = \"filtered_data_2.csv\"\n",
    "filtered_data_3 = \"filtered_data_3.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"google-cloud-storage\", \"gcsfs\", \"pandas\"]\n",
    ")\n",
    "def data_preprocess_stage_1(\n",
    "        raw_dataset: str,\n",
    "        output_dataset_one : Output[Dataset],\n",
    "        output_dataset_two : Output[Dataset]\n",
    "):\n",
    "    import os\n",
    "    import csv\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from random import randint\n",
    "\n",
    "\n",
    "    logging.warning(\"DATA PREPROCESSING 1 STAGE\")\n",
    "    logging.warning(\"Reading Dataset...\")\n",
    "\n",
    "    X = pd.read_csv(raw_dataset)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Dataset\")\n",
    "\n",
    "    nameOrigCol = 3\n",
    "    nameDestCol = 6\n",
    "    nameOrig = []\n",
    "    nameDest = []\n",
    "    nameCount = {}\n",
    "    namesWithMoreThanOneOccurrence = []\n",
    "\n",
    "    logging.warning(\"Checking Each Person's Transactions Count...\")\n",
    "\n",
    "    for name in X[:, nameOrigCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameOrig.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    for name in X[:, nameDestCol] :\n",
    "        if nameCount.get(name, -1) == -1 :\n",
    "            nameDest.append(name)\n",
    "\n",
    "            nameCount[name] = 1\n",
    "\n",
    "        else :\n",
    "            nameCount[name] += 1\n",
    "            namesWithMoreThanOneOccurrence.append(name)\n",
    "\n",
    "    logging.warning(\"Count Identification Done\")\n",
    "\n",
    "    logging.warning(\"Calculating Median ...\")\n",
    "\n",
    "    countArr = []\n",
    "    count = 0\n",
    "    for attr, value in nameCount.items() :\n",
    "        if value > 40 :\n",
    "            countArr.append(value)\n",
    "            count += 1\n",
    "    median = np.median(countArr)\n",
    "\n",
    "    logging.warning(f\"Median : {median}\")\n",
    "\n",
    "    logging.warning(\"Filtering Data Based on Transactions Count...\")\n",
    "    csv_golden_data = []\n",
    "\n",
    "    for i in range(X.shape[0]) :\n",
    "        if nameCount.get(X[i, 3], -1) > 40 or nameCount.get(X[i, 6], -1) > 40 :\n",
    "            csv_golden_data.append(X[i, :])\n",
    "\n",
    "    logging.warning(\"Filtering Done\")\n",
    "\n",
    "    logging.warning(\"Storing Filtered Data in data_processed folder...\")\n",
    "\n",
    "    new_file_name = \"filtered_data.csv\"\n",
    "\n",
    "    with open(output_dataset_one.path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(csv_golden_data)\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 2 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 1 dataset...\")\n",
    "\n",
    "    X = pd.DataFrame(csv_golden_data)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 1 dataset...\")\n",
    "\n",
    "    csv_dataset_primary = []\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 6\n",
    "    oldbalanceDest = 7\n",
    "    accountType = 8\n",
    "    isFraud = 9\n",
    "    isFlaggedFraud = 10\n",
    "\n",
    "    logging.warning(\"Changing Labels of Type Column ...\")\n",
    "\n",
    "    transfer = [\"WIRE_IN\", \"WIRE_OUT\"]\n",
    "    for i in range(X.shape[0]):\n",
    "        arr = []\n",
    "        arr.append(X[i,step])\n",
    "        if X[i,trans_type] ==\"PAYMENT\":\n",
    "            arr.append(\"CREDIT\")\n",
    "        elif X[i,trans_type] ==\"TRANSFER\":\n",
    "            arr.append(transfer[randint(0,1)])\n",
    "        else:\n",
    "            arr.append(X[i,trans_type])\n",
    "        arr.append(X[i,amount])\n",
    "        arr.append(X[i,nameOrig])\n",
    "        arr.append(X[i,oldbalanceOrg])\n",
    "        arr.append(X[i,nameDest])\n",
    "        arr.append(X[i,oldbalanceDest])\n",
    "        if X[i,trans_type] == \"TRANSFER\":\n",
    "            arr.append(\"FOREIGN\")\n",
    "        else:\n",
    "            arr.append(\"DOMESTIC\")\n",
    "\n",
    "        arr.append(X[i,isFraud])\n",
    "        arr.append(X[i,isFlaggedFraud])\n",
    "\n",
    "        csv_dataset_primary.append(arr)\n",
    "\n",
    "    logging.warning(\"Changing Labels Done\")\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "\n",
    "    columns=['step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "             'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_primary = pd.DataFrame(csv_dataset_primary, columns=columns)\n",
    "\n",
    "    data_primary.to_csv(output_dataset_two.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\"]\n",
    ")\n",
    "def data_preprocess_stage_2(\n",
    "        input_dataset: Input[Dataset],\n",
    "        output_dataset_three: Output[Dataset]\n",
    "):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    # data_path = f'gs://{bucket_id}/{folder_id}/filtered_data_2.csv'\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"DATA PREPROCESSING 3 STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Preprocessed 2 dataset...\")\n",
    "\n",
    "    X = pd.read_csv(input_dataset.path)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Preprocessed 2 dataset\")\n",
    "\n",
    "    #col\n",
    "    step = 0\n",
    "    trans_type = 1\n",
    "    amount = 2\n",
    "    nameOrig = 3\n",
    "    oldbalanceOrg = 4\n",
    "    nameDest = 5\n",
    "    oldbalanceDest = 6\n",
    "    accountType = 7\n",
    "    isFraud = 8\n",
    "\n",
    "    #col\n",
    "    entity = 0\n",
    "    incommingDomestic30 = 1\n",
    "    incommingDomestic60 = 2\n",
    "    incommingDomestic90 = 3\n",
    "    outgoingDomestic30 = 4\n",
    "    outgoingDomestic60 = 5\n",
    "    outgoingDomestic90 = 6\n",
    "    incommingForeign30 = 7\n",
    "    incommingForeign60 = 8\n",
    "    incommingForeign90 = 9\n",
    "    outgoingForeign30 = 10\n",
    "    outgoingForeign60 = 11\n",
    "    outgoingForeign90 = 12\n",
    "    incoming_domestic_count_30 = 13\n",
    "    incoming_domestic_count_60 = 14\n",
    "    incoming_domestic_count_90 = 15\n",
    "    outgoing_domestic_count_30 = 16\n",
    "    outgoing_domestic_count_60 = 17\n",
    "    outgoing_domestic_count_90 = 18\n",
    "    incoming_foreign_count_30 = 19\n",
    "    incoming_foreign_count_60 = 20\n",
    "    incoming_foreign_count_90 = 21\n",
    "    outgoing_foreign_count_30 = 22\n",
    "    outgoing_foreign_count_60 = 23\n",
    "    outgoing_foreign_count_90 = 24\n",
    "    balance_difference_30 = 25\n",
    "    balance_difference_60 = 26\n",
    "    balance_difference_90 = 27\n",
    "    isFraudSec = 28\n",
    "\n",
    "    csv_dataset_secondary = []\n",
    "    entities_pos = {}\n",
    "    enititesDict = {}\n",
    "\n",
    "    logging.warning(\"Creating New Features Using Transaction History...\")\n",
    "\n",
    "    def getSecRow(entity):\n",
    "        return [entity,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        source_entity = X[i,nameOrig]\n",
    "        dest_entity = X[i,nameDest]\n",
    "\n",
    "        source_pos = entities_pos.get(source_entity,-1)\n",
    "        if source_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[source_entity] = pos\n",
    "            source_pos = pos\n",
    "\n",
    "            row = getSecRow(source_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        dest_pos = entities_pos.get(dest_entity,-1)\n",
    "        if dest_pos == -1:\n",
    "            pos = len(csv_dataset_secondary)\n",
    "            entities_pos[dest_entity] = pos\n",
    "            dest_pos = pos\n",
    "\n",
    "            row = getSecRow(dest_entity)\n",
    "\n",
    "            csv_dataset_secondary.append(row)\n",
    "\n",
    "        transferAmountSource = 0\n",
    "        transferAmountDest = 0\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_IN\" or X[i,trans_type] == \"CREDIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"CASH_OUT\" or X[i,trans_type] == \"DEBIT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingDomestic90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingDomestic90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_IN\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][incommingForeign90] += X[i,amount]\n",
    "                # print(dest_pos,outgoingForeign90,i,amount)\n",
    "                csv_dataset_secondary[dest_pos][outgoingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = X[i,amount]\n",
    "            transferAmountDest = -1*X[i,amount]\n",
    "\n",
    "        if X[i,trans_type] == \"WIRE_OUT\":\n",
    "            if X[i,step] <=30:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign30] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign30] += X[i,amount]\n",
    "            if X[i,step] <=60:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign60] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign60] += X[i,amount]\n",
    "            if X[i,step] <=90:\n",
    "                csv_dataset_secondary[source_pos][outgoingForeign90] += X[i,amount]\n",
    "                csv_dataset_secondary[dest_pos][incommingForeign90] += X[i,amount]\n",
    "\n",
    "            transferAmountSource = -1*X[i,amount]\n",
    "            transferAmountDest = X[i,amount]\n",
    "\n",
    "        if enititesDict.get(source_entity,-1) == -1:\n",
    "            enititesDict[source_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceOrg],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        if enititesDict.get(dest_entity,-1) == -1:\n",
    "            enititesDict[dest_entity] = {\n",
    "                'day1Bal': X[i,oldbalanceDest],\n",
    "                'day30Bal': 0,\n",
    "                'day60Bal': 0,\n",
    "                'day90Bal': 0,\n",
    "                'countIncomingDomestic30': 0,\n",
    "                'countOutgoingDomestic30': 0,\n",
    "                'countIncomingDomestic60': 0,\n",
    "                'countOutgoingDomestic60': 0,\n",
    "                'countIncomingDomestic90': 0,\n",
    "                'countOutgoingDomestic90': 0,\n",
    "                'countIncomingForeign30': 0,\n",
    "                'countOutgoingForeign30': 0,\n",
    "                'countIncomingForeign60': 0,\n",
    "                'countOutgoingForeign60': 0,\n",
    "                'countIncomingForeign90': 0,\n",
    "                'countOutgoingForeign90': 0\n",
    "            }\n",
    "\n",
    "        incomingForSource = [\"CASH_IN\",\"CREDIT\",\"WIRE_IN\"]\n",
    "        incomingForDest = [\"CASH_OUT\",\"DEBIT\",\"WIRE_OUT\"]\n",
    "        outgoingForDest = incomingForSource\n",
    "        outgoingForSource = incomingForDest\n",
    "\n",
    "        if X[i,step]<=30:\n",
    "            enititesDict[source_entity]['day30Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day30Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign30'] += 1\n",
    "                else:\n",
    "\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign30'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic30'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic30'] += 1\n",
    "\n",
    "        if X[i,step]<=60:\n",
    "            enititesDict[source_entity]['day60Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day60Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign60'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic60'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic60'] += 1\n",
    "\n",
    "        if X[i,step]<=90:\n",
    "            enititesDict[source_entity]['day90Bal'] = transferAmountSource+X[i,oldbalanceOrg]\n",
    "            enititesDict[dest_entity]['day90Bal'] = transferAmountDest+X[i,oldbalanceDest]\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForSource:\n",
    "                    enititesDict[source_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[source_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[source_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "            if X[i,accountType] == \"FOREIGN\":\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingForeign90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingForeign90'] += 1\n",
    "            else:\n",
    "                if X[i,trans_type] in incomingForDest:\n",
    "                    enititesDict[dest_entity]['countIncomingDomestic90'] += 1\n",
    "                else:\n",
    "                    enititesDict[dest_entity]['countOutgoingDomestic90'] += 1\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][balance_difference_30] = enititesDict[source_entity]['day30Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_60] = enititesDict[source_entity]['day60Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "        csv_dataset_secondary[source_pos][balance_difference_90] = enititesDict[source_entity]['day90Bal'] - enititesDict[source_entity]['day1Bal']\n",
    "\n",
    "\n",
    "\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_30] = enititesDict[source_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_30] = enititesDict[source_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_60] = enititesDict[source_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_60] = enititesDict[source_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[source_pos][incoming_domestic_count_90] = enititesDict[source_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_domestic_count_90] = enititesDict[source_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_30] = enititesDict[source_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_30] = enititesDict[source_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_60] = enititesDict[source_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_60] = enititesDict[source_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[source_pos][incoming_foreign_count_90] = enititesDict[source_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[source_pos][outgoing_foreign_count_90] = enititesDict[source_entity]['countOutgoingForeign90']\n",
    "\n",
    "        csv_dataset_secondary[source_pos][isFraudSec] = csv_dataset_secondary[source_pos][isFraudSec] or X[i,isFraud]\n",
    "\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_30] = enititesDict[dest_entity]['countIncomingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_30] = enititesDict[dest_entity]['countOutgoingDomestic30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_60] = enititesDict[dest_entity]['countIncomingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_60] = enititesDict[dest_entity]['countOutgoingDomestic60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_domestic_count_90] = enititesDict[dest_entity]['countIncomingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_domestic_count_90] = enititesDict[dest_entity]['countOutgoingDomestic90']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_30] = enititesDict[dest_entity]['countIncomingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_30] = enititesDict[dest_entity]['countOutgoingForeign30']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_60] = enititesDict[dest_entity]['countIncomingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_60] = enititesDict[dest_entity]['countOutgoingForeign60']\n",
    "        csv_dataset_secondary[dest_pos][incoming_foreign_count_90] = enititesDict[dest_entity]['countIncomingForeign90']\n",
    "        csv_dataset_secondary[dest_pos][outgoing_foreign_count_90] = enititesDict[dest_entity]['countOutgoingForeign90']\n",
    "\n",
    "\n",
    "    columns = ['entity','incoming_domestic_amount_30','incoming_domestic_amount_60','incoming_domestic_amount_90',\n",
    "               'outgoing_domestic_amount_30','outgoing_domestic_amount_60','outgoing_domestic_amount_90',\n",
    "               'incoming_foreign_amount_30','incoming_foreign_amount_60','incoming_foreign_amount_90',\n",
    "               'outgoing_foreign_amount_30','outgoing_foreign_amount_60','outgoing_foreign_amount_90',\n",
    "               'incoming_domestic_count_30','incoming_domestic_count_60','incoming_domestic_count_90',\n",
    "               'outgoing_domestic_count_30','outgoing_domestic_count_60','outgoing_domestic_count_90',\n",
    "               'incoming_foreign_count_30','incoming_foreign_count_60','incoming_foreign_count_90',\n",
    "               'outgoing_foreign_count_30','outgoing_foreign_count_60','outgoing_foreign_count_90',\n",
    "               'balance_difference_30','balance_difference_60','balance_difference_90','isFraud']\n",
    "\n",
    "    logging.warning(\"Creating New Features Done\")\n",
    "\n",
    "    logging.warning(\"Storing Data in Data_processed Folder...\")\n",
    "\n",
    "    # filtered_data_3.csv\n",
    "    data_secondary = pd.DataFrame(csv_dataset_secondary, columns=columns)\n",
    "    data_secondary.to_csv(output_dataset_three.path,index=False)\n",
    "    logging.warning(\"Storing Data Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def feature_selection(\n",
    "        input_dataset : Input[Dataset],\n",
    "        output_dataset : Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1: filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param output_dataset: feature_importances.csv\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"FEATURE SELECTION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    dataframeX = pd.read_csv(input_dataset.path)\n",
    "    col_names = list(dataframeX.columns.values)\n",
    "    dataMat = dataframeX.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    logging.warning(\"Creating X and Y Variables...\")\n",
    "\n",
    "    X = dataMat[:,1:-2]\n",
    "    Y = dataMat[:,-1]\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of Y: {Y.shape}\")\n",
    "\n",
    "    logging.warning(\"Instiantiating Random Forest Model...\")\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    logging.warning(\"Fitting Data...\")\n",
    "\n",
    "    model.fit(X, Y.astype(int))\n",
    "\n",
    "    logging.warning(\"Checking Feature Importances...\")\n",
    "\n",
    "    feature_imp = model.feature_importances_\n",
    "\n",
    "    sorted_feature_vals = np.sort(feature_imp)\n",
    "    sorted_feature_indexes = np.argsort(feature_imp)\n",
    "\n",
    "    logging.warning(\"Significant Features in decreasing order of importance: \")\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances in reports...\")\n",
    "\n",
    "    fea_imp = [[col_names[i+2], feature_imp[i]] for i in reversed(sorted_feature_indexes)]\n",
    "    features = pd.DataFrame(fea_imp, columns=[\"features\", \"importance_score\"])\n",
    "    # feature_importances.csv\n",
    "    features.to_csv(output_dataset.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Feature Importances Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def segment_generation(\n",
    "        input_dataset_1 : Input[Dataset],\n",
    "        input_dataset_2 : Input[Dataset],\n",
    "        input_dataset_3 : Input[Dataset],\n",
    "        silhoutte_scores : Output[Dataset],\n",
    "        final_dataset_output: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset_1:  filtered_data_2.csv from data-preprocess-stage-1\n",
    "    :param input_dataset_2:  filtered_data_3.csv from data-preprocess-stage-2\n",
    "    :param input_dataset_3:  feature_importances.csv from features_election\n",
    "    :param silhoutte_scores: Metrics\n",
    "    :param final_dataset_output: final_dataset\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Segment Generation\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    logging.basicConfig(filename='logs/model_development.txt',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s %(message)s',\n",
    "                        datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"SEGMENT GENERATOR STAGE\")\n",
    "\n",
    "    def getClusterPredictions(data, true_k):\n",
    "        model = KMeans(n_clusters=true_k)\n",
    "        model.fit(data)\n",
    "        prediction = model.predict(data)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def getBestCluster(X,_min=2,_max=10):\n",
    "        selected_cluster = 0\n",
    "        previous_sil_coeff = 0.001 #some random small number not 0\n",
    "        sc_vals = []\n",
    "        for n_cluster in range(_min, _max):\n",
    "            kmeans = KMeans(n_clusters=n_cluster).fit(X)\n",
    "            label = kmeans.labels_\n",
    "\n",
    "            sil_coeff = silhouette_score(X, label, metric='euclidean', sample_size=1000)\n",
    "            sc_vals.append(sil_coeff)\n",
    "            # print(\"For n_clusters={}, The Silhouette Coefficient is {}\".format(n_cluster, sil_coeff))\n",
    "\n",
    "            percent_change = (sil_coeff-previous_sil_coeff)*100/previous_sil_coeff\n",
    "\n",
    "            # return when below a threshold of 1%\n",
    "            if percent_change<1:\n",
    "                selected_cluster = n_cluster-1\n",
    "\n",
    "            previous_sil_coeff = sil_coeff\n",
    "\n",
    "        return selected_cluster or _max, sc_vals\n",
    "\n",
    "    logging.warning(\"Reading Filtered Data 3 ...\")\n",
    "\n",
    "    X_dataframe = pd.read_csv(input_dataset_2.path)\n",
    "    X = X_dataframe.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Filtered Data 3\")\n",
    "\n",
    "    col_names = list(X_dataframe.columns.values)\n",
    "\n",
    "    X_trimmed_features = np.zeros((X.shape[0],1))\n",
    "\n",
    "    logging.warning(\"Importing Feature Importances...\")\n",
    "\n",
    "    #feature_path = f'gs://{bucket_id}/{folder_id}/feature_importances.csv'\n",
    "    features = pd.read_csv(input_dataset_3.path)\n",
    "\n",
    "    logging.warning(\"Selecting Top 13 Features for CLustering...\")\n",
    "\n",
    "    top_13 = features.iloc[:13, 0].tolist()\n",
    "\n",
    "    logging.warning(\"Top 13 Features stored in List\")\n",
    "\n",
    "    for feature in top_13:\n",
    "        X_trimmed_features = np.concatenate((X_trimmed_features,np.expand_dims(X_dataframe[feature],axis=1)),axis=1)\n",
    "    X_trimmed_features = X_trimmed_features[:,1:]\n",
    "\n",
    "    logging.warning(\"Choosing Best Number Of Clusters...\")\n",
    "\n",
    "    min_value = 2\n",
    "    max_value = 10\n",
    "    true_k, sc_vals = getBestCluster(X_trimmed_features,_min=min_value,_max=max_value)\n",
    "    true_k = 5\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores...\")\n",
    "\n",
    "\n",
    "    sil_score = [[i, sc_vals[i-min_value]] for i in range(min_value, max_value)]\n",
    "    sil = pd.DataFrame(sil_score, columns=[\"no_of_clusters\", \"silhoutte_score\"])\n",
    "    sil.to_csv(silhoutte_scores.path, index=False)\n",
    "    #sil.to_csv(f'gs://{bucket_id}/{folder_id}/silhoutte_scores.csv', index=False)\n",
    "\n",
    "    logging.warning(\"Storing Silhoutte Scores Done\")\n",
    "\n",
    "    logging.warning(\"Creating Clusters with Best No Of Clusters...\")\n",
    "\n",
    "    prediction = getClusterPredictions(X_trimmed_features, true_k)\n",
    "    seg_dict = {}\n",
    "    for i in range(X.shape[0]):\n",
    "        seg_dict[X[i,0]] = prediction[i]\n",
    "\n",
    "    logging.warning(\"Inputing Filtered Data 2 Dataset...\")\n",
    "\n",
    "    X_dataframe_pri = pd.read_csv(input_dataset_1.path)\n",
    "    X_pri = X_dataframe_pri.to_numpy()\n",
    "    col_names = list(X_dataframe_pri.columns.values)\n",
    "\n",
    "    logging.warning(\"Read Filtered 2 Data\")\n",
    "\n",
    "    logging.warning(\"Creating Final Dataset with segments...\")\n",
    "\n",
    "    X_with_segments = []\n",
    "    for i in range(X_pri.shape[0]):\n",
    "        X_with_segments.append(np.concatenate(([[seg_dict[X_pri[i,3]]]],np.expand_dims(X_pri[i,:],axis=0)),axis=1)[0])\n",
    "\n",
    "    segmented_columns = ['segment','step','trans_type','amount','nameOrig','oldbalanceOrg',\n",
    "                         'nameDest','oldbalanceDest','accountType','isFraud','isFlaggedFraud']\n",
    "\n",
    "    data_segmented = pd.DataFrame(X_with_segments, columns = segmented_columns)\n",
    "    data_segmented = data_segmented.drop('isFlaggedFraud', axis=1)\n",
    "    data_segmented.to_csv(final_dataset_output.path, index=False)\n",
    "\n",
    "    logging.warning(\"Storing Final Dataset Done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.10\",\n",
    "    packages_to_install=[\"pandas\", \"gcsfs\", \"scikit-learn\", \"catboost\"]\n",
    ")\n",
    "def training(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str,\n",
    "        input_dataset : Input[Dataset],\n",
    "        model_path: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param input_dataset: fina_dataset.csv from segment-generation\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import pickle\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    from google.cloud import storage\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "\n",
    "    logging.warning(\"----------\")\n",
    "    logging.warning(\"MODEL CREATION STAGE\")\n",
    "\n",
    "    logging.warning(\"Reading Final Dataset...\")\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(bucket_id)\n",
    "\n",
    "    dataMat = pd.read_csv(input_dataset.path)\n",
    "    data = dataMat.to_numpy()\n",
    "\n",
    "    logging.warning(\"Read Final Dataset\")\n",
    "\n",
    "    logging.warning(\"Checking Categorical Features...\")\n",
    "\n",
    "    cat_feat = [i for i in dataMat.columns if dataMat[i].dtypes == 'O']\n",
    "\n",
    "    logging.warning(\"Checking Missing Values...\")\n",
    "\n",
    "    a = dict(dataMat.isnull().sum())\n",
    "    b = [[i, a[i]] for i in a.keys()]\n",
    "    missing = pd.DataFrame(b, columns=['features', 'null_values_count'])\n",
    "\n",
    "    logging.warning(\"Storing Missing Values...\")\n",
    "\n",
    "    missing.to_csv(\"missing_values.csv\", index=False)\n",
    "\n",
    "    logging.warning(\"Storing Missing Values Done\")\n",
    "\n",
    "    logging.warning(\"Encoding Categorical Features...\")\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    for i in cat_feat:\n",
    "        dataMat[i] = encoder.fit_transform(dataMat[i])\n",
    "\n",
    "    blob = bucket.blob(f\"{bucket_folder}/label_encoder.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    logging.warning(\"Features Encoding Done\")\n",
    "\n",
    "    logging.warning(\"Creating X and y variables ...\")\n",
    "\n",
    "    X = dataMat.iloc[:, :-1]\n",
    "    y = dataMat['isFraud']\n",
    "\n",
    "    logging.warning(f\"Shape of X: {X.shape} and Shape of y: {y.shape}\")\n",
    "\n",
    "    logging.warning(\"Splitting Dataset...\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    logging.warning(\"Instantiating Model...\")\n",
    "\n",
    "    model = CatBoostClassifier(random_state=42, class_weights={0:1, 1:12}, silent=True)\n",
    "\n",
    "    logging.warning(\"Fitting Model...\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_cat = model.predict(X_test)\n",
    "\n",
    "    logging.warning(\"Saving Model...\")\n",
    "\n",
    "    #model_path = \"model.pkl\"\n",
    "    blob = bucket.blob(f\"{bucket_folder}/model.pkl\")\n",
    "    # pickle.dump(model, open(blob, 'wb'))\n",
    "\n",
    "    with blob.open(\"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    logging.warning(\"Saving Model Metrics...\")\n",
    "\n",
    "    metric_file_path = \"performance.json\"\n",
    "    # with open(metric_file_path, \"r\") as f:\n",
    "    #     data = json.load(f)\n",
    "\n",
    "    model_metric = {\n",
    "        \"time_stamp\": datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\"),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_cat).tolist(),\n",
    "        \"precision\": precision_score(y_test, y_pred_cat),\n",
    "        \"recall\": recall_score(y_test, y_pred_cat),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_cat)\n",
    "    }\n",
    "\n",
    "    # data['model_metric'].append(model_metric)\n",
    "    # with open(metric_file_path, \"w\") as f:\n",
    "    #     json.dump(data, f, indent=4)\n",
    "\n",
    "    logging.warning(\"Model Metrics Stored\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "\n",
    "@pipeline(name=\"money_laundering_detection\")\n",
    "def pipeline(\n",
    "        project_id: str,\n",
    "        bucket_id: str,\n",
    "        bucket_folder: str\n",
    "):\n",
    "    preproces_job_1 = data_preprocess_stage_1(raw_dataset=raw_data_full_path)\n",
    "    preprocess_job_2 = data_preprocess_stage_2(input_dataset=preproces_job_1.outputs[\"output_dataset_two\"])\n",
    "    feature_selection_job = feature_selection(input_dataset=preprocess_job_2.outputs[\"output_dataset_three\"])\n",
    "    segment_generation_job = segment_generation(\n",
    "        input_dataset_1=preproces_job_1.outputs[\"output_dataset_two\"],\n",
    "        input_dataset_2=preprocess_job_2.outputs[\"output_dataset_three\"],\n",
    "        input_dataset_3=feature_selection_job.outputs[\"output_dataset\"]\n",
    "    )\n",
    "    training_job = training(\n",
    "        project_id = project_id,\n",
    "        bucket_id = bucket_id,\n",
    "        bucket_folder = bucket_folder,\n",
    "        input_dataset=segment_generation_job.outputs[\"final_dataset_output\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='money_laundering_detection.yaml'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=project_id,\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"money_laundering_detection\",\n",
    "    template_path=\"money_laundering_detection.yaml\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id,\n",
    "        'bucket_id': bucket_id,\n",
    "        'bucket_folder': bucket_folder_name\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jesusarguelles/IdeaProjects/demos/venv/lib/python3.11/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.4.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = storage.Client(project=\"jesusarguelles-sandbox\").bucket(\"jesusarguelles-datasets-public\")\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/model.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "blob = bucket.blob(\"money_laundering_detection/label_encoder.pkl\")\n",
    "\n",
    "with blob.open(\"rb\") as f:\n",
    "    encoder = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [1],\n",
    "    \"trans_type\": [\"DEBIT\"],\n",
    "    \"amount\": [181.00],\n",
    "    \"nameOrig\": [\"C1900366749\"],\n",
    "    \"oldbalanceOrg\": [4465.0],\n",
    "    \"nameDest\": [\"C997608398\"],\n",
    "    \"oldbalanceDest\": [\"10845.0\"],\n",
    "    \"accountType\": [\"DOMESTIC\"]\n",
    "}\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "{'trans_type': {'CASH_IN': 0,\n  'CASH_OUT': 1,\n  'CREDIT': 2,\n  'DEBIT': 3,\n  'WIRE_IN': 4,\n  'WIRE_OUT': 5},\n 'nameOrig': {'C1000008975': 0,\n  'C1000012640': 1,\n  'C1000028246': 2,\n  'C1000044196': 3,\n  'C1000053329': 4,\n  'C1000073191': 5,\n  'C1000078727': 6,\n  'C1000079132': 7,\n  'C1000093176': 8,\n  'C1000094896': 9,\n  'C1000097327': 10,\n  'C1000103904': 11,\n  'C1000109302': 12,\n  'C1000111897': 13,\n  'C1000119553': 14,\n  'C1000119807': 15,\n  'C1000121623': 16,\n  'C1000123015': 17,\n  'C1000148923': 18,\n  'C1000160958': 19,\n  'C1000169021': 20,\n  'C10001825': 21,\n  'C1000185314': 22,\n  'C1000198697': 23,\n  'C1000202702': 24,\n  'C1000222753': 25,\n  'C100022590': 26,\n  'C1000229380': 27,\n  'C1000231597': 28,\n  'C1000232082': 29,\n  'C1000236204': 30,\n  'C1000264406': 31,\n  'C1000264972': 32,\n  'C1000275423': 33,\n  'C100028125': 34,\n  'C1000286893': 35,\n  'C1000292364': 36,\n  'C1000340261': 37,\n  'C1000372884': 38,\n  'C1000398935': 39,\n  'C1000411139': 40,\n  'C1000413708': 41,\n  'C1000428751': 42,\n  'C1000429687': 43,\n  'C1000449928': 44,\n  'C1000450067': 45,\n  'C1000453441': 46,\n  'C1000461005': 47,\n  'C1000486051': 48,\n  'C1000505510': 49,\n  'C1000512970': 50,\n  'C1000534621': 51,\n  'C1000546575': 52,\n  'C1000555739': 53,\n  'C1000564196': 54,\n  'C1000565333': 55,\n  'C1000569455': 56,\n  'C1000581906': 57,\n  'C1000582817': 58,\n  'C1000600190': 59,\n  'C1000600979': 60,\n  'C1000607301': 61,\n  'C1000607784': 62,\n  'C1000619549': 63,\n  'C1000621742': 64,\n  'C1000628778': 65,\n  'C1000631767': 66,\n  'C1000632655': 67,\n  'C1000638448': 68,\n  'C1000657654': 69,\n  'C1000662292': 70,\n  'C1000666653': 71,\n  'C1000676226': 72,\n  'C1000677787': 73,\n  'C1000678289': 74,\n  'C100068693': 75,\n  'C1000701211': 76,\n  'C1000703087': 77,\n  'C1000719581': 78,\n  'C1000720528': 79,\n  'C100072186': 80,\n  'C1000726647': 81,\n  'C1000734187': 82,\n  'C1000740725': 83,\n  'C1000750947': 84,\n  'C1000754903': 85,\n  'C1000772154': 86,\n  'C1000776450': 87,\n  'C1000782655': 88,\n  'C1000784017': 89,\n  'C1000789920': 90,\n  'C1000813338': 91,\n  'C1000815030': 92,\n  'C1000820773': 93,\n  'C10008266': 94,\n  'C1000831273': 95,\n  'C1000834747': 96,\n  'C1000835226': 97,\n  'C1000839177': 98,\n  'C1000839468': 99,\n  'C100084739': 100,\n  'C1000848098': 101,\n  'C1000850851': 102,\n  'C1000856851': 103,\n  'C1000868340': 104,\n  'C1000873613': 105,\n  'C1000875943': 106,\n  'C1000886182': 107,\n  'C1000897951': 108,\n  'C1000901490': 109,\n  'C1000915206': 110,\n  'C1000920056': 111,\n  'C1000928556': 112,\n  'C1000941003': 113,\n  'C1000941567': 114,\n  'C1000957916': 115,\n  'C1000959847': 116,\n  'C1000960957': 117,\n  'C1000973094': 118,\n  'C1000975429': 119,\n  'C1000987363': 120,\n  'C1000989558': 121,\n  'C1000997134': 122,\n  'C1001007846': 123,\n  'C1001015651': 124,\n  'C1001031275': 125,\n  'C1001068982': 126,\n  'C1001070092': 127,\n  'C1001074701': 128,\n  'C1001075922': 129,\n  'C1001080513': 130,\n  'C1001085950': 131,\n  'C1001086871': 132,\n  'C1001088736': 133,\n  'C1001107989': 134,\n  'C100111146': 135,\n  'C1001113093': 136,\n  'C1001124945': 137,\n  'C100112624': 138,\n  'C1001127805': 139,\n  'C1001156371': 140,\n  'C1001169347': 141,\n  'C1001170465': 142,\n  'C1001193407': 143,\n  'C1001196167': 144,\n  'C1001196828': 145,\n  'C1001206767': 146,\n  'C1001210058': 147,\n  'C1001212483': 148,\n  'C1001220052': 149,\n  'C1001221076': 150,\n  'C1001227463': 151,\n  'C1001231507': 152,\n  'C1001256830': 153,\n  'C1001257401': 154,\n  'C1001262344': 155,\n  'C1001262479': 156,\n  'C1001281925': 157,\n  'C1001294495': 158,\n  'C1001304485': 159,\n  'C1001305936': 160,\n  'C100132264': 161,\n  'C1001322855': 162,\n  'C100134585': 163,\n  'C1001363802': 164,\n  'C1001369865': 165,\n  'C1001383277': 166,\n  'C1001389710': 167,\n  'C1001390107': 168,\n  'C1001392992': 169,\n  'C1001401870': 170,\n  'C1001406201': 171,\n  'C1001417510': 172,\n  'C100142394': 173,\n  'C1001433436': 174,\n  'C1001439137': 175,\n  'C1001443469': 176,\n  'C100144436': 177,\n  'C1001444586': 178,\n  'C1001451547': 179,\n  'C1001454057': 180,\n  'C1001464837': 181,\n  'C1001476563': 182,\n  'C1001488342': 183,\n  'C1001494565': 184,\n  'C100150061': 185,\n  'C1001502183': 186,\n  'C1001505601': 187,\n  'C100151969': 188,\n  'C1001535922': 189,\n  'C1001539307': 190,\n  'C1001550233': 191,\n  'C1001571869': 192,\n  'C1001580639': 193,\n  'C1001594369': 194,\n  'C1001600900': 195,\n  'C1001630601': 196,\n  'C1001638963': 197,\n  'C1001647229': 198,\n  'C1001647768': 199,\n  'C1001650062': 200,\n  'C1001650678': 201,\n  'C1001658373': 202,\n  'C1001658507': 203,\n  'C1001669794': 204,\n  'C1001674053': 205,\n  'C1001680636': 206,\n  'C1001686222': 207,\n  'C100168689': 208,\n  'C1001696739': 209,\n  'C1001704228': 210,\n  'C1001710860': 211,\n  'C1001714072': 212,\n  'C1001717851': 213,\n  'C1001726526': 214,\n  'C1001726585': 215,\n  'C1001738606': 216,\n  'C10017400': 217,\n  'C1001742979': 218,\n  'C100175082': 219,\n  'C1001762102': 220,\n  'C1001767630': 221,\n  'C1001770327': 222,\n  'C1001775172': 223,\n  'C1001779603': 224,\n  'C1001784791': 225,\n  'C1001791481': 226,\n  'C1001803924': 227,\n  'C1001817166': 228,\n  'C1001817818': 229,\n  'C1001817825': 230,\n  'C1001831443': 231,\n  'C1001832768': 232,\n  'C1001841462': 233,\n  'C1001844551': 234,\n  'C1001861162': 235,\n  'C1001876102': 236,\n  'C1001887173': 237,\n  'C1001926305': 238,\n  'C100193539': 239,\n  'C1001939515': 240,\n  'C1001959540': 241,\n  'C1001962456': 242,\n  'C1001977988': 243,\n  'C1001979516': 244,\n  'C1002004721': 245,\n  'C1002023146': 246,\n  'C1002025760': 247,\n  'C1002027935': 248,\n  'C1002043823': 249,\n  'C1002048214': 250,\n  'C1002055170': 251,\n  'C1002062193': 252,\n  'C1002069812': 253,\n  'C1002076367': 254,\n  'C1002091428': 255,\n  'C1002118763': 256,\n  'C1002125514': 257,\n  'C1002127572': 258,\n  'C1002134900': 259,\n  'C1002146498': 260,\n  'C1002151235': 261,\n  'C1002165221': 262,\n  'C1002174268': 263,\n  'C1002175850': 264,\n  'C1002185434': 265,\n  'C1002214200': 266,\n  'C1002216592': 267,\n  'C1002217352': 268,\n  'C1002240678': 269,\n  'C1002268530': 270,\n  'C1002269345': 271,\n  'C1002271955': 272,\n  'C1002309063': 273,\n  'C1002310051': 274,\n  'C1002310608': 275,\n  'C1002313856': 276,\n  'C10023385': 277,\n  'C1002348639': 278,\n  'C1002351399': 279,\n  'C1002398876': 280,\n  'C1002417864': 281,\n  'C1002427544': 282,\n  'C1002446735': 283,\n  'C1002472149': 284,\n  'C1002483990': 285,\n  'C1002512063': 286,\n  'C1002530733': 287,\n  'C1002567387': 288,\n  'C1002578318': 289,\n  'C1002582492': 290,\n  'C1002587535': 291,\n  'C1002591828': 292,\n  'C1002595497': 293,\n  'C1002604886': 294,\n  'C100261245': 295,\n  'C1002613480': 296,\n  'C1002619436': 297,\n  'C100262405': 298,\n  'C1002679466': 299,\n  'C100268313': 300,\n  'C1002703476': 301,\n  'C1002705280': 302,\n  'C1002707743': 303,\n  'C1002708978': 304,\n  'C1002713153': 305,\n  'C1002716109': 306,\n  'C1002719237': 307,\n  'C1002730716': 308,\n  'C1002749719': 309,\n  'C1002750213': 310,\n  'C1002754166': 311,\n  'C1002755066': 312,\n  'C1002757189': 313,\n  'C100275759': 314,\n  'C1002771240': 315,\n  'C1002781680': 316,\n  'C1002795777': 317,\n  'C1002799211': 318,\n  'C1002802357': 319,\n  'C1002805074': 320,\n  'C1002810315': 321,\n  'C1002846319': 322,\n  'C1002860386': 323,\n  'C1002867421': 324,\n  'C1002877756': 325,\n  'C1002881349': 326,\n  'C1002911516': 327,\n  'C1002914748': 328,\n  'C1002922063': 329,\n  'C1002925765': 330,\n  'C1002955694': 331,\n  'C1002983148': 332,\n  'C1003000306': 333,\n  'C1003019443': 334,\n  'C100302692': 335,\n  'C1003038107': 336,\n  'C1003040412': 337,\n  'C1003041577': 338,\n  'C1003045664': 339,\n  'C1003070623': 340,\n  'C1003071810': 341,\n  'C1003081125': 342,\n  'C1003089370': 343,\n  'C1003090424': 344,\n  'C1003112869': 345,\n  'C1003126291': 346,\n  'C1003138612': 347,\n  'C1003153699': 348,\n  'C1003164154': 349,\n  'C1003176796': 350,\n  'C1003205881': 351,\n  'C1003209187': 352,\n  'C1003212161': 353,\n  'C1003217031': 354,\n  'C1003221970': 355,\n  'C1003236329': 356,\n  'C1003240711': 357,\n  'C100324249': 358,\n  'C1003271241': 359,\n  'C1003298482': 360,\n  'C1003311950': 361,\n  'C100331403': 362,\n  'C1003318121': 363,\n  'C1003320664': 364,\n  'C1003331183': 365,\n  'C1003344296': 366,\n  'C1003353224': 367,\n  'C1003365311': 368,\n  'C1003387276': 369,\n  'C1003398136': 370,\n  'C1003406065': 371,\n  'C1003447086': 372,\n  'C1003455117': 373,\n  'C1003458249': 374,\n  'C1003460104': 375,\n  'C1003467354': 376,\n  'C1003473612': 377,\n  'C1003473996': 378,\n  'C100348161': 379,\n  'C1003484185': 380,\n  'C1003492162': 381,\n  'C1003492346': 382,\n  'C1003515547': 383,\n  'C1003519153': 384,\n  'C1003526443': 385,\n  'C1003547261': 386,\n  'C1003558438': 387,\n  'C1003564540': 388,\n  'C1003567878': 389,\n  'C1003580529': 390,\n  'C1003587698': 391,\n  'C100359050': 392,\n  'C100359053': 393,\n  'C1003598110': 394,\n  'C1003621132': 395,\n  'C1003621812': 396,\n  'C1003636300': 397,\n  'C1003636872': 398,\n  'C1003660232': 399,\n  'C1003663195': 400,\n  'C100367067': 401,\n  'C100367356': 402,\n  'C1003681306': 403,\n  'C1003687186': 404,\n  'C1003690321': 405,\n  'C1003714013': 406,\n  'C100376420': 407,\n  'C1003773256': 408,\n  'C1003774966': 409,\n  'C1003775387': 410,\n  'C1003792263': 411,\n  'C1003818246': 412,\n  'C1003824894': 413,\n  'C100382841': 414,\n  'C1003836532': 415,\n  'C1003856745': 416,\n  'C1003864855': 417,\n  'C1003872442': 418,\n  'C1003877438': 419,\n  'C1003897032': 420,\n  'C1003911496': 421,\n  'C1003922709': 422,\n  'C1003933154': 423,\n  'C100394411': 424,\n  'C1003953949': 425,\n  'C1003957987': 426,\n  'C100396780': 427,\n  'C1003968105': 428,\n  'C1003974051': 429,\n  'C1003978460': 430,\n  'C1003983159': 431,\n  'C1003995695': 432,\n  'C1003995829': 433,\n  'C1003996225': 434,\n  'C1004000324': 435,\n  'C1004011295': 436,\n  'C1004012931': 437,\n  'C1004021392': 438,\n  'C1004022387': 439,\n  'C100403028': 440,\n  'C1004040256': 441,\n  'C100405132': 442,\n  'C1004055060': 443,\n  'C1004056696': 444,\n  'C1004056775': 445,\n  'C1004084012': 446,\n  'C1004086035': 447,\n  'C1004095747': 448,\n  'C1004097549': 449,\n  'C1004111031': 450,\n  'C1004122739': 451,\n  'C1004125202': 452,\n  'C1004134824': 453,\n  'C1004136768': 454,\n  'C1004149342': 455,\n  'C1004151218': 456,\n  'C1004159141': 457,\n  'C1004167913': 458,\n  'C1004176028': 459,\n  'C1004179723': 460,\n  'C1004193860': 461,\n  'C1004202342': 462,\n  'C1004217254': 463,\n  'C1004228080': 464,\n  'C100422846': 465,\n  'C1004229504': 466,\n  'C100423305': 467,\n  'C1004238357': 468,\n  'C1004250570': 469,\n  'C1004255461': 470,\n  'C10042589': 471,\n  'C1004270885': 472,\n  'C1004277129': 473,\n  'C1004289701': 474,\n  'C1004289837': 475,\n  'C100429411': 476,\n  'C1004310754': 477,\n  'C1004311816': 478,\n  'C1004323868': 479,\n  'C1004337001': 480,\n  'C1004353928': 481,\n  'C1004355628': 482,\n  'C1004369063': 483,\n  'C1004369098': 484,\n  'C1004378179': 485,\n  'C1004380133': 486,\n  'C1004394192': 487,\n  'C1004415789': 488,\n  'C1004420962': 489,\n  'C1004421693': 490,\n  'C1004422278': 491,\n  'C1004436072': 492,\n  'C1004452292': 493,\n  'C100445376': 494,\n  'C1004454148': 495,\n  'C1004480063': 496,\n  'C1004515785': 497,\n  'C1004533006': 498,\n  'C1004546361': 499,\n  'C1004547200': 500,\n  'C1004549468': 501,\n  'C1004553597': 502,\n  'C1004558325': 503,\n  'C1004563423': 504,\n  'C1004567166': 505,\n  'C1004577240': 506,\n  'C1004579718': 507,\n  'C1004580428': 508,\n  'C1004580876': 509,\n  'C1004595493': 510,\n  'C1004608755': 511,\n  'C1004614887': 512,\n  'C1004620689': 513,\n  'C1004631566': 514,\n  'C1004645690': 515,\n  'C1004655737': 516,\n  'C1004657449': 517,\n  'C1004660750': 518,\n  'C1004676293': 519,\n  'C1004706555': 520,\n  'C1004711430': 521,\n  'C1004770470': 522,\n  'C1004798661': 523,\n  'C1004800155': 524,\n  'C100481529': 525,\n  'C1004817699': 526,\n  'C1004825230': 527,\n  'C1004826105': 528,\n  'C1004827377': 529,\n  'C1004855238': 530,\n  'C1004868833': 531,\n  'C100487907': 532,\n  'C1004885373': 533,\n  'C1004893348': 534,\n  'C1004901619': 535,\n  'C1004908875': 536,\n  'C1004909110': 537,\n  'C1004913397': 538,\n  'C1004918013': 539,\n  'C1004918494': 540,\n  'C1004932344': 541,\n  'C1004938892': 542,\n  'C1004939913': 543,\n  'C1004945791': 544,\n  'C1004977757': 545,\n  'C1004996631': 546,\n  'C1005012898': 547,\n  'C100501836': 548,\n  'C1005030765': 549,\n  'C1005053968': 550,\n  'C1005059958': 551,\n  'C1005063886': 552,\n  'C100507640': 553,\n  'C1005077585': 554,\n  'C1005094757': 555,\n  'C1005102426': 556,\n  'C1005115644': 557,\n  'C1005116050': 558,\n  'C1005129306': 559,\n  'C1005143630': 560,\n  'C1005146274': 561,\n  'C1005151980': 562,\n  'C100515516': 563,\n  'C1005180298': 564,\n  'C1005182852': 565,\n  'C1005189741': 566,\n  'C1005196622': 567,\n  'C1005219673': 568,\n  'C1005241602': 569,\n  'C1005253090': 570,\n  'C1005253861': 571,\n  'C100526541': 572,\n  'C1005269345': 573,\n  'C1005289956': 574,\n  'C1005296443': 575,\n  'C1005297051': 576,\n  'C100529875': 577,\n  'C1005300183': 578,\n  'C1005303441': 579,\n  'C1005314492': 580,\n  'C1005338603': 581,\n  'C1005338989': 582,\n  'C1005348551': 583,\n  'C100536026': 584,\n  'C1005412973': 585,\n  'C1005428411': 586,\n  'C1005445087': 587,\n  'C1005448569': 588,\n  'C1005451321': 589,\n  'C1005453653': 590,\n  'C1005456277': 591,\n  'C1005462448': 592,\n  'C1005481824': 593,\n  'C1005484470': 594,\n  'C1005503033': 595,\n  'C1005520354': 596,\n  'C1005521069': 597,\n  'C1005533484': 598,\n  'C1005537171': 599,\n  'C1005540454': 600,\n  'C1005543801': 601,\n  'C1005545775': 602,\n  'C1005563862': 603,\n  'C1005583272': 604,\n  'C1005590911': 605,\n  'C1005595723': 606,\n  'C1005603340': 607,\n  'C1005606993': 608,\n  'C1005609790': 609,\n  'C1005611537': 610,\n  'C1005634351': 611,\n  'C1005637444': 612,\n  'C1005664537': 613,\n  'C1005673518': 614,\n  'C1005680858': 615,\n  'C1005697943': 616,\n  'C1005698645': 617,\n  'C1005700772': 618,\n  'C1005707360': 619,\n  'C1005714117': 620,\n  'C1005727445': 621,\n  'C1005728765': 622,\n  'C1005730847': 623,\n  'C1005734680': 624,\n  'C1005747008': 625,\n  'C1005752870': 626,\n  'C1005754692': 627,\n  'C100576082': 628,\n  'C1005786942': 629,\n  'C1005790348': 630,\n  'C1005791203': 631,\n  'C1005803625': 632,\n  'C1005806243': 633,\n  'C1005818743': 634,\n  'C1005821920': 635,\n  'C1005822018': 636,\n  'C1005835213': 637,\n  'C1005846800': 638,\n  'C1005850764': 639,\n  'C1005865597': 640,\n  'C1005867143': 641,\n  'C1005879813': 642,\n  'C1005884508': 643,\n  'C1005891608': 644,\n  'C100589889': 645,\n  'C1005901099': 646,\n  'C1005905103': 647,\n  'C1005912053': 648,\n  'C1005914543': 649,\n  'C1005915411': 650,\n  'C1005922405': 651,\n  'C1005922901': 652,\n  'C1005926429': 653,\n  'C1005928079': 654,\n  'C1005941852': 655,\n  'C1005957422': 656,\n  'C1005965922': 657,\n  'C1005966070': 658,\n  'C1005974695': 659,\n  'C1005985034': 660,\n  'C1005998888': 661,\n  'C100600294': 662,\n  'C1006003281': 663,\n  'C100601124': 664,\n  'C1006059470': 665,\n  'C1006078766': 666,\n  'C1006096352': 667,\n  'C1006097731': 668,\n  'C1006117979': 669,\n  'C100612036': 670,\n  'C1006120932': 671,\n  'C1006139090': 672,\n  'C1006151221': 673,\n  'C1006159593': 674,\n  'C1006161559': 675,\n  'C1006164596': 676,\n  'C1006168115': 677,\n  'C1006173042': 678,\n  'C1006177641': 679,\n  'C1006205770': 680,\n  'C1006209497': 681,\n  'C1006209692': 682,\n  'C1006212635': 683,\n  'C100621883': 684,\n  'C1006228077': 685,\n  'C1006230813': 686,\n  'C1006235741': 687,\n  'C100626027': 688,\n  'C1006268835': 689,\n  'C1006273572': 690,\n  'C10062821': 691,\n  'C100628485': 692,\n  'C1006286853': 693,\n  'C1006297128': 694,\n  'C1006303769': 695,\n  'C1006309827': 696,\n  'C1006310339': 697,\n  'C1006313056': 698,\n  'C1006325206': 699,\n  'C1006332201': 700,\n  'C1006333450': 701,\n  'C1006339761': 702,\n  'C1006350278': 703,\n  'C100637425': 704,\n  'C1006388223': 705,\n  'C1006391632': 706,\n  'C1006404644': 707,\n  'C1006432135': 708,\n  'C1006433548': 709,\n  'C1006440985': 710,\n  'C1006443202': 711,\n  'C1006445456': 712,\n  'C100645440': 713,\n  'C1006458735': 714,\n  'C1006468733': 715,\n  'C1006473056': 716,\n  'C100647312': 717,\n  'C1006513229': 718,\n  'C1006513913': 719,\n  'C1006523518': 720,\n  'C1006523599': 721,\n  'C1006552098': 722,\n  'C1006582130': 723,\n  'C1006601938': 724,\n  'C1006602357': 725,\n  'C1006613977': 726,\n  'C1006616689': 727,\n  'C1006617350': 728,\n  'C1006617648': 729,\n  'C1006619175': 730,\n  'C1006630440': 731,\n  'C1006643723': 732,\n  'C1006654205': 733,\n  'C1006659861': 734,\n  'C100666037': 735,\n  'C1006662258': 736,\n  'C1006662934': 737,\n  'C1006672393': 738,\n  'C1006683173': 739,\n  'C1006708342': 740,\n  'C1006740795': 741,\n  'C1006750298': 742,\n  'C1006756215': 743,\n  'C1006778810': 744,\n  'C1006783057': 745,\n  'C1006792838': 746,\n  'C1006795941': 747,\n  'C1006815926': 748,\n  'C1006825791': 749,\n  'C1006860227': 750,\n  'C1006862741': 751,\n  'C1006867559': 752,\n  'C100687952': 753,\n  'C1006879660': 754,\n  'C1006880837': 755,\n  'C1006905449': 756,\n  'C1006922911': 757,\n  'C1006936913': 758,\n  'C1006948417': 759,\n  'C1006950425': 760,\n  'C1006963795': 761,\n  'C1006969841': 762,\n  'C1006970176': 763,\n  'C1006981867': 764,\n  'C1006988878': 765,\n  'C1006993133': 766,\n  'C100699501': 767,\n  'C1007001043': 768,\n  'C1007001064': 769,\n  'C1007032391': 770,\n  'C1007034987': 771,\n  'C1007040437': 772,\n  'C100704629': 773,\n  'C1007076085': 774,\n  'C1007085054': 775,\n  'C100708535': 776,\n  'C1007091734': 777,\n  'C1007095837': 778,\n  'C1007100433': 779,\n  'C1007112297': 780,\n  'C1007128813': 781,\n  'C1007140534': 782,\n  'C1007146756': 783,\n  'C1007157968': 784,\n  'C1007168092': 785,\n  'C1007179679': 786,\n  'C100719742': 787,\n  'C1007201380': 788,\n  'C1007203044': 789,\n  'C100721644': 790,\n  'C1007232905': 791,\n  'C1007243195': 792,\n  'C100724789': 793,\n  'C1007248258': 794,\n  'C100726539': 795,\n  'C1007277227': 796,\n  'C1007279494': 797,\n  'C1007280972': 798,\n  'C1007286566': 799,\n  'C1007293196': 800,\n  'C1007299730': 801,\n  'C1007301880': 802,\n  'C1007303474': 803,\n  'C1007324506': 804,\n  'C1007346242': 805,\n  'C1007347887': 806,\n  'C1007364726': 807,\n  'C1007367027': 808,\n  'C1007380383': 809,\n  'C1007383086': 810,\n  'C1007388024': 811,\n  'C1007390210': 812,\n  'C1007399494': 813,\n  'C1007400187': 814,\n  'C1007418207': 815,\n  'C1007432371': 816,\n  'C1007454057': 817,\n  'C1007456949': 818,\n  'C1007470969': 819,\n  'C100748532': 820,\n  'C100749807': 821,\n  'C1007505417': 822,\n  'C1007536835': 823,\n  'C1007550632': 824,\n  'C1007573247': 825,\n  'C1007576454': 826,\n  'C1007579203': 827,\n  'C100758408': 828,\n  'C1007586156': 829,\n  'C1007595120': 830,\n  'C1007600599': 831,\n  'C1007609515': 832,\n  'C1007619190': 833,\n  'C1007619728': 834,\n  'C1007623444': 835,\n  'C1007624175': 836,\n  'C100762598': 837,\n  'C1007627621': 838,\n  'C1007648069': 839,\n  'C1007658612': 840,\n  'C1007679575': 841,\n  'C1007681293': 842,\n  'C1007691462': 843,\n  'C1007704346': 844,\n  'C1007709089': 845,\n  'C1007714612': 846,\n  'C100771609': 847,\n  'C1007761757': 848,\n  'C1007766556': 849,\n  'C1007811396': 850,\n  'C1007814798': 851,\n  'C1007831627': 852,\n  'C1007841065': 853,\n  'C1007849046': 854,\n  'C1007853419': 855,\n  'C1007871906': 856,\n  'C1007874697': 857,\n  'C1007881394': 858,\n  'C1007890238': 859,\n  'C1007897586': 860,\n  'C1007901765': 861,\n  'C1007904558': 862,\n  'C1007908515': 863,\n  'C10079180': 864,\n  'C10079202': 865,\n  'C1007931178': 866,\n  'C1007954252': 867,\n  'C1007955511': 868,\n  'C1007987149': 869,\n  'C1007990431': 870,\n  'C100799947': 871,\n  'C1008000249': 872,\n  'C1008001570': 873,\n  'C1008003049': 874,\n  'C1008023118': 875,\n  'C1008028088': 876,\n  'C1008030455': 877,\n  'C1008036274': 878,\n  'C1008038842': 879,\n  'C1008050051': 880,\n  'C1008063280': 881,\n  'C1008065001': 882,\n  'C1008082682': 883,\n  'C1008083406': 884,\n  'C1008087421': 885,\n  'C1008099285': 886,\n  'C1008128874': 887,\n  'C1008132000': 888,\n  'C1008132020': 889,\n  'C1008167549': 890,\n  'C1008184214': 891,\n  'C1008196827': 892,\n  'C1008214127': 893,\n  'C100822247': 894,\n  'C1008224411': 895,\n  'C1008233409': 896,\n  'C100823517': 897,\n  'C1008238339': 898,\n  'C1008244697': 899,\n  'C1008246127': 900,\n  'C100826646': 901,\n  'C1008266688': 902,\n  'C1008268270': 903,\n  'C10082714': 904,\n  'C1008275159': 905,\n  'C1008300811': 906,\n  'C1008304914': 907,\n  'C1008310164': 908,\n  'C1008329124': 909,\n  'C1008345188': 910,\n  'C1008349458': 911,\n  'C1008349726': 912,\n  'C1008352930': 913,\n  'C1008360215': 914,\n  'C1008371317': 915,\n  'C1008387034': 916,\n  'C1008392765': 917,\n  'C1008405229': 918,\n  'C1008442738': 919,\n  'C1008457718': 920,\n  'C100848334': 921,\n  'C1008488265': 922,\n  'C1008494164': 923,\n  'C1008529280': 924,\n  'C1008529978': 925,\n  'C1008553016': 926,\n  'C1008567280': 927,\n  'C1008613971': 928,\n  'C1008620068': 929,\n  'C1008638789': 930,\n  'C1008650607': 931,\n  'C1008666538': 932,\n  'C1008675540': 933,\n  'C1008686078': 934,\n  'C100868649': 935,\n  'C1008689646': 936,\n  'C1008689858': 937,\n  'C1008711240': 938,\n  'C1008722052': 939,\n  'C1008733343': 940,\n  'C1008739479': 941,\n  'C1008774102': 942,\n  'C1008783574': 943,\n  'C1008817077': 944,\n  'C1008822454': 945,\n  'C1008844750': 946,\n  'C1008846589': 947,\n  'C1008847853': 948,\n  'C1008866578': 949,\n  'C1008868866': 950,\n  'C1008897663': 951,\n  'C1008899848': 952,\n  'C1008904622': 953,\n  'C1008907293': 954,\n  'C100894001': 955,\n  'C1008946833': 956,\n  'C1008947638': 957,\n  'C1008947948': 958,\n  'C1008967322': 959,\n  'C1008968999': 960,\n  'C1008972502': 961,\n  'C1008977493': 962,\n  'C1009038984': 963,\n  'C1009041289': 964,\n  'C1009058174': 965,\n  'C1009058306': 966,\n  'C100906666': 967,\n  'C1009075749': 968,\n  'C1009082041': 969,\n  'C1009092974': 970,\n  'C1009101830': 971,\n  'C1009101840': 972,\n  'C1009104790': 973,\n  'C1009121523': 974,\n  'C1009135088': 975,\n  'C1009144250': 976,\n  'C1009146459': 977,\n  'C100915456': 978,\n  'C100915781': 979,\n  'C1009157811': 980,\n  'C1009163362': 981,\n  'C1009164061': 982,\n  'C1009182079': 983,\n  'C100920554': 984,\n  'C1009205686': 985,\n  'C1009218508': 986,\n  'C1009223866': 987,\n  'C1009225672': 988,\n  'C1009233673': 989,\n  'C1009244385': 990,\n  'C1009245870': 991,\n  'C1009253801': 992,\n  'C1009256905': 993,\n  'C100925801': 994,\n  'C1009267441': 995,\n  'C1009284386': 996,\n  'C1009287659': 997,\n  'C1009299627': 998,\n  'C1009300235': 999,\n  ...},\n 'nameDest': {'C1000910565': 0,\n  'C1001367899': 1,\n  'C1001449890': 2,\n  'C1001611622': 3,\n  'C1002407179': 4,\n  'C1002545847': 5,\n  'C1002973288': 6,\n  'C1003872245': 7,\n  'C1004780164': 8,\n  'C1004962268': 9,\n  'C100555887': 10,\n  'C1005928818': 11,\n  'C1005941472': 12,\n  'C1006094903': 13,\n  'C1006605786': 14,\n  'C1007717381': 15,\n  'C1009195400': 16,\n  'C1009631303': 17,\n  'C1010306008': 18,\n  'C1010765614': 19,\n  'C1011395618': 20,\n  'C1012785532': 21,\n  'C1013296540': 22,\n  'C1013331488': 23,\n  'C1013561013': 24,\n  'C1013700132': 25,\n  'C1013875739': 26,\n  'C1014154376': 27,\n  'C1014255081': 28,\n  'C1014607150': 29,\n  'C1014923261': 30,\n  'C1014935920': 31,\n  'C1016689071': 32,\n  'C1017113849': 33,\n  'C1017767725': 34,\n  'C1017770930': 35,\n  'C1018394275': 36,\n  'C1018546641': 37,\n  'C1018972658': 38,\n  'C1019670582': 39,\n  'C1020160156': 40,\n  'C1020613921': 41,\n  'C1021790512': 42,\n  'C1021853943': 43,\n  'C1022023283': 44,\n  'C1022269511': 45,\n  'C1022680775': 46,\n  'C1022896347': 47,\n  'C1023535667': 48,\n  'C1023714065': 49,\n  'C1024208640': 50,\n  'C1024346126': 51,\n  'C102537461': 52,\n  'C1025978207': 53,\n  'C1026973279': 54,\n  'C1027017168': 55,\n  'C1027043112': 56,\n  'C1027296777': 57,\n  'C1028574311': 58,\n  'C1029125570': 59,\n  'C1029211742': 60,\n  'C102932163': 61,\n  'C1030386925': 62,\n  'C1030564396': 63,\n  'C1030706332': 64,\n  'C1031244205': 65,\n  'C1031639563': 66,\n  'C1031948877': 67,\n  'C1032442702': 68,\n  'C1033049515': 69,\n  'C1033425869': 70,\n  'C103357744': 71,\n  'C1033905669': 72,\n  'C1034339016': 73,\n  'C1034382650': 74,\n  'C1034584766': 75,\n  'C1034902119': 76,\n  'C1035585513': 77,\n  'C1035781898': 78,\n  'C1036138670': 79,\n  'C1036331914': 80,\n  'C1037802667': 81,\n  'C1038434218': 82,\n  'C1038678108': 83,\n  'C1038715421': 84,\n  'C1039162432': 85,\n  'C1039302738': 86,\n  'C1039375562': 87,\n  'C1039411368': 88,\n  'C1039865382': 89,\n  'C1039935737': 90,\n  'C1040240917': 91,\n  'C1040847151': 92,\n  'C1041397550': 93,\n  'C1041664164': 94,\n  'C1042118330': 95,\n  'C1043870161': 96,\n  'C1044100176': 97,\n  'C1044117371': 98,\n  'C1044743847': 99,\n  'C1045258317': 100,\n  'C1046102247': 101,\n  'C1046160944': 102,\n  'C1046190725': 103,\n  'C1046459309': 104,\n  'C1047108779': 105,\n  'C1048785778': 106,\n  'C1048855692': 107,\n  'C1049228732': 108,\n  'C1049312121': 109,\n  'C1049373115': 110,\n  'C1049795740': 111,\n  'C1049804269': 112,\n  'C105002686': 113,\n  'C1050196275': 114,\n  'C1050374066': 115,\n  'C1050406642': 116,\n  'C1051187510': 117,\n  'C105129740': 118,\n  'C1052117080': 119,\n  'C1052801037': 120,\n  'C1052951038': 121,\n  'C1054410157': 122,\n  'C1054868486': 123,\n  'C1054909747': 124,\n  'C10550299': 125,\n  'C1055337060': 126,\n  'C1055680194': 127,\n  'C1058634310': 128,\n  'C1059072014': 129,\n  'C1059781259': 130,\n  'C1060041730': 131,\n  'C1060191389': 132,\n  'C1060563153': 133,\n  'C1060830840': 134,\n  'C1060951315': 135,\n  'C1061034243': 136,\n  'C1061401264': 137,\n  'C1062254208': 138,\n  'C1062650008': 139,\n  'C1062714827': 140,\n  'C1062791125': 141,\n  'C1063299937': 142,\n  'C1063303686': 143,\n  'C1063418068': 144,\n  'C1063484926': 145,\n  'C1063777023': 146,\n  'C1065349988': 147,\n  'C1065470233': 148,\n  'C1065774928': 149,\n  'C1065997164': 150,\n  'C1066166621': 151,\n  'C1066948549': 152,\n  'C1067155425': 153,\n  'C1067360446': 154,\n  'C1067936865': 155,\n  'C1068824137': 156,\n  'C1069131391': 157,\n  'C1069568462': 158,\n  'C1069873258': 159,\n  'C107095723': 160,\n  'C1071364078': 161,\n  'C1071381336': 162,\n  'C1071713375': 163,\n  'C1072086300': 164,\n  'C107300817': 165,\n  'C1073161843': 166,\n  'C1073540419': 167,\n  'C1074821415': 168,\n  'C1075238398': 169,\n  'C1075447932': 170,\n  'C1075564985': 171,\n  'C1076496284': 172,\n  'C1076835071': 173,\n  'C1077045505': 174,\n  'C1077219957': 175,\n  'C1078584667': 176,\n  'C1078814158': 177,\n  'C1079088036': 178,\n  'C1079343872': 179,\n  'C1079491260': 180,\n  'C1079554968': 181,\n  'C107993010': 182,\n  'C1079930466': 183,\n  'C108005819': 184,\n  'C1080077694': 185,\n  'C108034745': 186,\n  'C1082580319': 187,\n  'C1082992891': 188,\n  'C1083142315': 189,\n  'C1083694367': 190,\n  'C1084435493': 191,\n  'C1084995130': 192,\n  'C1085479446': 193,\n  'C1086158899': 194,\n  'C1086206575': 195,\n  'C108763013': 196,\n  'C1089101011': 197,\n  'C1089177065': 198,\n  'C1089993541': 199,\n  'C1090263069': 200,\n  'C1090825814': 201,\n  'C109195229': 202,\n  'C1092318644': 203,\n  'C1092417574': 204,\n  'C1092465037': 205,\n  'C1092987200': 206,\n  'C1093054320': 207,\n  'C1094224274': 208,\n  'C1094712572': 209,\n  'C1095446005': 210,\n  'C1095587424': 211,\n  'C1095666481': 212,\n  'C109599906': 213,\n  'C1096040876': 214,\n  'C1096283470': 215,\n  'C1096524110': 216,\n  'C1096984287': 217,\n  'C1097476560': 218,\n  'C1099073452': 219,\n  'C1099915107': 220,\n  'C11003494': 221,\n  'C1100439041': 222,\n  'C1100767002': 223,\n  'C1101174554': 224,\n  'C1102715869': 225,\n  'C1103170060': 226,\n  'C1103621993': 227,\n  'C110416257': 228,\n  'C1104197405': 229,\n  'C1104676061': 230,\n  'C1104693947': 231,\n  'C1104746610': 232,\n  'C110491570': 233,\n  'C1105063017': 234,\n  'C1105400940': 235,\n  'C1105527690': 236,\n  'C1105901057': 237,\n  'C1106744673': 238,\n  'C1107146140': 239,\n  'C110721357': 240,\n  'C1107228492': 241,\n  'C1107367383': 242,\n  'C1109421585': 243,\n  'C1109485117': 244,\n  'C1109959414': 245,\n  'C1110072010': 246,\n  'C1110630179': 247,\n  'C111150518': 248,\n  'C111210112': 249,\n  'C111232025': 250,\n  'C1112414583': 251,\n  'C1112466060': 252,\n  'C1113265902': 253,\n  'C111391237': 254,\n  'C11139703': 255,\n  'C1114190780': 256,\n  'C1114708103': 257,\n  'C1114881449': 258,\n  'C1115113238': 259,\n  'C1115240385': 260,\n  'C1115404972': 261,\n  'C1116137715': 262,\n  'C1116148899': 263,\n  'C1116292004': 264,\n  'C111636207': 265,\n  'C1116915569': 266,\n  'C1117816135': 267,\n  'C1118052446': 268,\n  'C1118423959': 269,\n  'C1118750465': 270,\n  'C1119870116': 271,\n  'C1120487810': 272,\n  'C1120702661': 273,\n  'C1121384286': 274,\n  'C1121453612': 275,\n  'C1121863378': 276,\n  'C1122524668': 277,\n  'C1123313084': 278,\n  'C1124597091': 279,\n  'C1126404645': 280,\n  'C1126553623': 281,\n  'C1127049440': 282,\n  'C1128238842': 283,\n  'C1128407310': 284,\n  'C1129451699': 285,\n  'C1129670968': 286,\n  'C1130053335': 287,\n  'C1130075212': 288,\n  'C1130179838': 289,\n  'C1130444401': 290,\n  'C1130568252': 291,\n  'C1130803130': 292,\n  'C113096815': 293,\n  'C1131308631': 294,\n  'C1132709076': 295,\n  'C1132795252': 296,\n  'C1133007712': 297,\n  'C1134266682': 298,\n  'C1134395461': 299,\n  'C1134701218': 300,\n  'C1134776066': 301,\n  'C1134883945': 302,\n  'C1134976854': 303,\n  'C1136050999': 304,\n  'C1136186114': 305,\n  'C1137247436': 306,\n  'C1137320932': 307,\n  'C1137350304': 308,\n  'C1137371968': 309,\n  'C1137805002': 310,\n  'C1137877526': 311,\n  'C1139127799': 312,\n  'C1139636566': 313,\n  'C1140028291': 314,\n  'C1140223380': 315,\n  'C1141049797': 316,\n  'C1141137903': 317,\n  'C1142098871': 318,\n  'C1142653119': 319,\n  'C1143894549': 320,\n  'C1144070975': 321,\n  'C1145835418': 322,\n  'C1146577120': 323,\n  'C1146698143': 324,\n  'C1146719991': 325,\n  'C114730398': 326,\n  'C1147717854': 327,\n  'C1147977782': 328,\n  'C1148052483': 329,\n  'C1148564657': 330,\n  'C1149273771': 331,\n  'C1149788645': 332,\n  'C1150224259': 333,\n  'C1150575867': 334,\n  'C1150610696': 335,\n  'C1150630865': 336,\n  'C1150936132': 337,\n  'C115095474': 338,\n  'C115141847': 339,\n  'C1151464555': 340,\n  'C1151706143': 341,\n  'C1151925006': 342,\n  'C1152194738': 343,\n  'C1152378087': 344,\n  'C1153867509': 345,\n  'C1153885173': 346,\n  'C1154065495': 347,\n  'C115457323': 348,\n  'C1154607874': 349,\n  'C1155838976': 350,\n  'C1156317584': 351,\n  'C1156584815': 352,\n  'C1157464567': 353,\n  'C115809998': 354,\n  'C115885991': 355,\n  'C1160708664': 356,\n  'C1161865950': 357,\n  'C1162171909': 358,\n  'C116251606': 359,\n  'C1162775738': 360,\n  'C116300945': 361,\n  'C1163056956': 362,\n  'C1163088488': 363,\n  'C1163480574': 364,\n  'C1163572928': 365,\n  'C1165398731': 366,\n  'C1166175379': 367,\n  'C1166654566': 368,\n  'C1167066016': 369,\n  'C1167432890': 370,\n  'C1167813715': 371,\n  'C1168422649': 372,\n  'C1168881368': 373,\n  'C116956744': 374,\n  'C1169903663': 375,\n  'C1170794006': 376,\n  'C117108994': 377,\n  'C117162427': 378,\n  'C1171758755': 379,\n  'C1171948877': 380,\n  'C1172009958': 381,\n  'C1172704377': 382,\n  'C1172716014': 383,\n  'C1172814480': 384,\n  'C1173662257': 385,\n  'C117403789': 386,\n  'C1174072748': 387,\n  'C1174496884': 388,\n  'C1174600239': 389,\n  'C1174935745': 390,\n  'C1175199409': 391,\n  'C1175389102': 392,\n  'C1175399276': 393,\n  'C1175707090': 394,\n  'C1176045015': 395,\n  'C1178471274': 396,\n  'C1179975227': 397,\n  'C1180014238': 398,\n  'C118136684': 399,\n  'C1181579840': 400,\n  'C1181708889': 401,\n  'C1181733275': 402,\n  'C1181943270': 403,\n  'C1182461167': 404,\n  'C1182908789': 405,\n  'C1183075634': 406,\n  'C1183266411': 407,\n  'C1185298063': 408,\n  'C1185502384': 409,\n  'C1185525558': 410,\n  'C1186157294': 411,\n  'C1186254299': 412,\n  'C1186256103': 413,\n  'C1186285732': 414,\n  'C1186449563': 415,\n  'C118648358': 416,\n  'C1186819207': 417,\n  'C1187639680': 418,\n  'C1187717597': 419,\n  'C1187871127': 420,\n  'C1188427220': 421,\n  'C1188916648': 422,\n  'C1189861498': 423,\n  'C1190136960': 424,\n  'C1190215254': 425,\n  'C1191364717': 426,\n  'C1191544932': 427,\n  'C1191985342': 428,\n  'C1192240143': 429,\n  'C1192446625': 430,\n  'C1192937442': 431,\n  'C119306016': 432,\n  'C1193329690': 433,\n  'C1193495878': 434,\n  'C1193690398': 435,\n  'C1194066750': 436,\n  'C1194834028': 437,\n  'C1195194841': 438,\n  'C1195378184': 439,\n  'C1196307684': 440,\n  'C1196384757': 441,\n  'C1196571825': 442,\n  'C1196590236': 443,\n  'C1196694338': 444,\n  'C1197114389': 445,\n  'C1197382632': 446,\n  'C1197482041': 447,\n  'C1197606915': 448,\n  'C1198167820': 449,\n  'C1200233967': 450,\n  'C1200314947': 451,\n  'C1200417755': 452,\n  'C1200612373': 453,\n  'C1201057368': 454,\n  'C1202306289': 455,\n  'C1202769194': 456,\n  'C1203711061': 457,\n  'C1204684191': 458,\n  'C1205778254': 459,\n  'C1205844899': 460,\n  'C1206178620': 461,\n  'C1206447754': 462,\n  'C1207178960': 463,\n  'C1207946845': 464,\n  'C1208635528': 465,\n  'C1208702024': 466,\n  'C1209271652': 467,\n  'C1209613575': 468,\n  'C1210002123': 469,\n  'C1210180003': 470,\n  'C1210315704': 471,\n  'C1210337654': 472,\n  'C1210434315': 473,\n  'C1210879412': 474,\n  'C1212151219': 475,\n  'C1212974498': 476,\n  'C1213359698': 477,\n  'C1213485528': 478,\n  'C121404705': 479,\n  'C121488263': 480,\n  'C1215084319': 481,\n  'C121523915': 482,\n  'C121569473': 483,\n  'C1215697988': 484,\n  'C121628080': 485,\n  'C1216745400': 486,\n  'C1217206140': 487,\n  'C121729362': 488,\n  'C1217303067': 489,\n  'C1217400138': 490,\n  'C121751903': 491,\n  'C1218231243': 492,\n  'C1218365329': 493,\n  'C1218584230': 494,\n  'C1219169266': 495,\n  'C1219241944': 496,\n  'C1219248641': 497,\n  'C1219273867': 498,\n  'C1219369531': 499,\n  'C1219457970': 500,\n  'C1220278931': 501,\n  'C122086708': 502,\n  'C1220897602': 503,\n  'C1220911229': 504,\n  'C1221832908': 505,\n  'C1222250580': 506,\n  'C1222590577': 507,\n  'C1223591088': 508,\n  'C1224130196': 509,\n  'C122501657': 510,\n  'C1225152713': 511,\n  'C1225309817': 512,\n  'C1225366033': 513,\n  'C1225390660': 514,\n  'C1225406756': 515,\n  'C1225616405': 516,\n  'C1225979740': 517,\n  'C122631871': 518,\n  'C1226372269': 519,\n  'C1227603646': 520,\n  'C1227688628': 521,\n  'C1228316980': 522,\n  'C1230082414': 523,\n  'C1230583314': 524,\n  'C1231540556': 525,\n  'C1231985446': 526,\n  'C1232058643': 527,\n  'C1232088827': 528,\n  'C1233454027': 529,\n  'C123366619': 530,\n  'C1234059684': 531,\n  'C1234612952': 532,\n  'C1234776885': 533,\n  'C1235151206': 534,\n  'C1235725243': 535,\n  'C1237463271': 536,\n  'C1238707698': 537,\n  'C12387149': 538,\n  'C1238739669': 539,\n  'C1238904328': 540,\n  'C1239059271': 541,\n  'C1239707538': 542,\n  'C1239826343': 543,\n  'C1240024476': 544,\n  'C1240091493': 545,\n  'C1240730624': 546,\n  'C1240760303': 547,\n  'C1241039373': 548,\n  'C1241261398': 549,\n  'C1241878571': 550,\n  'C1242090949': 551,\n  'C1242586395': 552,\n  'C124285763': 553,\n  'C1242861033': 554,\n  'C1243393970': 555,\n  'C1243712710': 556,\n  'C1244093207': 557,\n  'C1244185833': 558,\n  'C1244521461': 559,\n  'C1244762702': 560,\n  'C1244839062': 561,\n  'C1244848793': 562,\n  'C1244947864': 563,\n  'C124540047': 564,\n  'C1245442940': 565,\n  'C1245472829': 566,\n  'C1245478626': 567,\n  'C1245609030': 568,\n  'C1245689449': 569,\n  'C124573125': 570,\n  'C1246074457': 571,\n  'C1246134024': 572,\n  'C1246142195': 573,\n  'C1246792604': 574,\n  'C1247051677': 575,\n  'C1247394840': 576,\n  'C1247600089': 577,\n  'C1247658278': 578,\n  'C1248272338': 579,\n  'C1248539702': 580,\n  'C1248645795': 581,\n  'C1248712846': 582,\n  'C1249000159': 583,\n  'C1250635817': 584,\n  'C1251218116': 585,\n  'C1251686742': 586,\n  'C1251991550': 587,\n  'C1252122735': 588,\n  'C1252466498': 589,\n  'C1253793047': 590,\n  'C1254211980': 591,\n  'C1254526270': 592,\n  'C1254893565': 593,\n  'C1255024717': 594,\n  'C1255395942': 595,\n  'C1255980822': 596,\n  'C1256198009': 597,\n  'C1256392652': 598,\n  'C1256732367': 599,\n  'C125722707': 600,\n  'C1258643950': 601,\n  'C1258662321': 602,\n  'C1258702925': 603,\n  'C1259769769': 604,\n  'C1260889084': 605,\n  'C1261031890': 606,\n  'C126129101': 607,\n  'C1261329785': 608,\n  'C1261940766': 609,\n  'C1261940778': 610,\n  'C1261983139': 611,\n  'C1262016786': 612,\n  'C1262147530': 613,\n  'C12625920': 614,\n  'C1262822392': 615,\n  'C126351739': 616,\n  'C1263665389': 617,\n  'C1263738407': 618,\n  'C1263845074': 619,\n  'C1264088262': 620,\n  'C1264851251': 621,\n  'C1265284050': 622,\n  'C1265847217': 623,\n  'C1266035080': 624,\n  'C1266435158': 625,\n  'C1267159812': 626,\n  'C1268075707': 627,\n  'C1269283129': 628,\n  'C1269336757': 629,\n  'C1269605689': 630,\n  'C126978053': 631,\n  'C1270516': 632,\n  'C1270744516': 633,\n  'C1271057169': 634,\n  'C1271098857': 635,\n  'C1271835801': 636,\n  'C1272040757': 637,\n  'C1272179353': 638,\n  'C1272528369': 639,\n  'C1272982746': 640,\n  'C127385698': 641,\n  'C1273939873': 642,\n  'C1274971943': 643,\n  'C127509847': 644,\n  'C1275353661': 645,\n  'C1275435326': 646,\n  'C1275777205': 647,\n  'C1276160402': 648,\n  'C1276319779': 649,\n  'C1276424637': 650,\n  'C1276892569': 651,\n  'C1277694634': 652,\n  'C12793273': 653,\n  'C1279610437': 654,\n  'C1279831179': 655,\n  'C1280512370': 656,\n  'C1281229383': 657,\n  'C1281855264': 658,\n  'C1281898359': 659,\n  'C1281912848': 660,\n  'C1282155635': 661,\n  'C1282235415': 662,\n  'C1282745381': 663,\n  'C1283595719': 664,\n  'C1284149862': 665,\n  'C1284449291': 666,\n  'C128558966': 667,\n  'C1286084959': 668,\n  'C1286783796': 669,\n  'C1286885448': 670,\n  'C1287328228': 671,\n  'C128846374': 672,\n  'C128870695': 673,\n  'C1288708473': 674,\n  'C128931614': 675,\n  'C1289384798': 676,\n  'C1289453529': 677,\n  'C1289493889': 678,\n  'C1289716559': 679,\n  'C1289906124': 680,\n  'C1290599201': 681,\n  'C1290601415': 682,\n  'C1291253720': 683,\n  'C1291664589': 684,\n  'C1292067471': 685,\n  'C1292435170': 686,\n  'C1292827974': 687,\n  'C1292838001': 688,\n  'C1292966728': 689,\n  'C1293499473': 690,\n  'C1295301356': 691,\n  'C1296656022': 692,\n  'C1297685781': 693,\n  'C1298147262': 694,\n  'C1298177219': 695,\n  'C1298242208': 696,\n  'C1298314970': 697,\n  'C1298316171': 698,\n  'C1298333001': 699,\n  'C1298737967': 700,\n  'C1300519073': 701,\n  'C1301525222': 702,\n  'C1302931951': 703,\n  'C1303184051': 704,\n  'C1303218675': 705,\n  'C1303868418': 706,\n  'C1304345230': 707,\n  'C1305039609': 708,\n  'C1305075214': 709,\n  'C1305830291': 710,\n  'C1306228515': 711,\n  'C1306274803': 712,\n  'C1306439078': 713,\n  'C1307933246': 714,\n  'C1308392840': 715,\n  'C1308497108': 716,\n  'C1308625658': 717,\n  'C1309189719': 718,\n  'C1310685539': 719,\n  'C1310864170': 720,\n  'C1311325237': 721,\n  'C131307582': 722,\n  'C1313471918': 723,\n  'C1313534832': 724,\n  'C1313685064': 725,\n  'C1314618182': 726,\n  'C1314961752': 727,\n  'C1315729466': 728,\n  'C1316560467': 729,\n  'C1317067915': 730,\n  'C1317731757': 731,\n  'C1318038292': 732,\n  'C131837504': 733,\n  'C1318491599': 734,\n  'C131871009': 735,\n  'C1318822808': 736,\n  'C1320217924': 737,\n  'C1320515149': 738,\n  'C1321128627': 739,\n  'C1321139761': 740,\n  'C1321586454': 741,\n  'C1321640594': 742,\n  'C1321802524': 743,\n  'C1322942971': 744,\n  'C1323741703': 745,\n  'C132379888': 746,\n  'C1324196239': 747,\n  'C1324548103': 748,\n  'C1324885855': 749,\n  'C1325038895': 750,\n  'C1325349830': 751,\n  'C1326121635': 752,\n  'C1326243087': 753,\n  'C1326645274': 754,\n  'C1327282178': 755,\n  'C1327761328': 756,\n  'C1327907419': 757,\n  'C1328129815': 758,\n  'C1328263070': 759,\n  'C1328442807': 760,\n  'C1328991385': 761,\n  'C1329589315': 762,\n  'C1329688547': 763,\n  'C1329956939': 764,\n  'C1331052438': 765,\n  'C1332133484': 766,\n  'C1333366140': 767,\n  'C1334240794': 768,\n  'C1334272276': 769,\n  'C1334620206': 770,\n  'C1334669858': 771,\n  'C1334679764': 772,\n  'C1334973394': 773,\n  'C1335050193': 774,\n  'C1335946878': 775,\n  'C1336996537': 776,\n  'C1337062880': 777,\n  'C1337306552': 778,\n  'C1337416387': 779,\n  'C1337563431': 780,\n  'C1337720486': 781,\n  'C1337741679': 782,\n  'C1339179775': 783,\n  'C1339257929': 784,\n  'C1339678445': 785,\n  'C1339765646': 786,\n  'C1339798857': 787,\n  'C1339858954': 788,\n  'C1340123681': 789,\n  'C134012626': 790,\n  'C1340707077': 791,\n  'C1340872163': 792,\n  'C1341043251': 793,\n  'C1341616763': 794,\n  'C1343108348': 795,\n  'C1343134043': 796,\n  'C1343686543': 797,\n  'C134398945': 798,\n  'C134426098': 799,\n  'C1344555666': 800,\n  'C1345063038': 801,\n  'C1345463476': 802,\n  'C1345585961': 803,\n  'C13459888': 804,\n  'C1346052990': 805,\n  'C134623499': 806,\n  'C1346317033': 807,\n  'C1346667529': 808,\n  'C1347294617': 809,\n  'C1347890180': 810,\n  'C1348373832': 811,\n  'C1348502440': 812,\n  'C1348937989': 813,\n  'C1349402229': 814,\n  'C1349767414': 815,\n  'C1349923341': 816,\n  'C1350789239': 817,\n  'C1351164037': 818,\n  'C1353512285': 819,\n  'C1354155703': 820,\n  'C1354725727': 821,\n  'C1354774829': 822,\n  'C1354952534': 823,\n  'C1356054329': 824,\n  'C1356710075': 825,\n  'C135718722': 826,\n  'C1357670903': 827,\n  'C1357984916': 828,\n  'C1359255073': 829,\n  'C1359484306': 830,\n  'C1359634401': 831,\n  'C1360092022': 832,\n  'C1360211577': 833,\n  'C1360243494': 834,\n  'C1360767589': 835,\n  'C1360913629': 836,\n  'C1361396394': 837,\n  'C1361864867': 838,\n  'C1361885449': 839,\n  'C1362041447': 840,\n  'C1362232282': 841,\n  'C13627188': 842,\n  'C1362883144': 843,\n  'C1363208344': 844,\n  'C1363504147': 845,\n  'C136383086': 846,\n  'C13639604': 847,\n  'C1365765031': 848,\n  'C1367871114': 849,\n  'C1367904567': 850,\n  'C1368612562': 851,\n  'C136892321': 852,\n  'C1369248292': 853,\n  'C1369348004': 854,\n  'C1369695156': 855,\n  'C1370269326': 856,\n  'C1370787297': 857,\n  'C1371191175': 858,\n  'C1371304585': 859,\n  'C1371929292': 860,\n  'C1372520401': 861,\n  'C13726579': 862,\n  'C1372941293': 863,\n  'C1373172466': 864,\n  'C1373467808': 865,\n  'C1374063565': 866,\n  'C1374126829': 867,\n  'C1375280409': 868,\n  'C1375815098': 869,\n  'C137643455': 870,\n  'C137655601': 871,\n  'C1377194794': 872,\n  'C1378631566': 873,\n  'C1379031360': 874,\n  'C1379289710': 875,\n  'C1380361139': 876,\n  'C1380720615': 877,\n  'C1380922009': 878,\n  'C1381129323': 879,\n  'C1381376705': 880,\n  'C1382919334': 881,\n  'C1383273534': 882,\n  'C1383446722': 883,\n  'C1383454473': 884,\n  'C1383605612': 885,\n  'C1383831706': 886,\n  'C1383942363': 887,\n  'C1384149246': 888,\n  'C1384492050': 889,\n  'C1385126783': 890,\n  'C1386732390': 891,\n  'C1387674302': 892,\n  'C1387713471': 893,\n  'C1387750261': 894,\n  'C1388052079': 895,\n  'C1388182036': 896,\n  'C1388298612': 897,\n  'C1388365204': 898,\n  'C1389405694': 899,\n  'C1390121014': 900,\n  'C1390272479': 901,\n  'C1390295291': 902,\n  'C1390358265': 903,\n  'C1390814796': 904,\n  'C1390913202': 905,\n  'C1391397365': 906,\n  'C1393612077': 907,\n  'C1394526584': 908,\n  'C1394878885': 909,\n  'C1395487968': 910,\n  'C1395724466': 911,\n  'C1395746279': 912,\n  'C1395957862': 913,\n  'C1395959888': 914,\n  'C1396297158': 915,\n  'C1396618865': 916,\n  'C139688944': 917,\n  'C139708138': 918,\n  'C1397244538': 919,\n  'C1398304707': 920,\n  'C1398453045': 921,\n  'C1398530689': 922,\n  'C1398683883': 923,\n  'C13998805': 924,\n  'C1400085107': 925,\n  'C1401316767': 926,\n  'C1401721023': 927,\n  'C1402042164': 928,\n  'C1403770809': 929,\n  'C1404112538': 930,\n  'C140572150': 931,\n  'C14057327': 932,\n  'C1405906887': 933,\n  'C1406193485': 934,\n  'C1406333164': 935,\n  'C1406749291': 936,\n  'C1406968003': 937,\n  'C14070792': 938,\n  'C1407083386': 939,\n  'C1407472437': 940,\n  'C1407928233': 941,\n  'C1408337297': 942,\n  'C1410301341': 943,\n  'C1410669831': 944,\n  'C1410782806': 945,\n  'C1410936008': 946,\n  'C1411863742': 947,\n  'C1413055366': 948,\n  'C141316671': 949,\n  'C1413651097': 950,\n  'C1413677222': 951,\n  'C1413871034': 952,\n  'C141458554': 953,\n  'C1414763289': 954,\n  'C1414803985': 955,\n  'C1414919255': 956,\n  'C1415082627': 957,\n  'C141535961': 958,\n  'C1415380443': 959,\n  'C1416272227': 960,\n  'C141706120': 961,\n  'C1419771524': 962,\n  'C1419794148': 963,\n  'C1419857464': 964,\n  'C1420119812': 965,\n  'C1420356843': 966,\n  'C1421039625': 967,\n  'C142113813': 968,\n  'C142211154': 969,\n  'C1422999923': 970,\n  'C1423125072': 971,\n  'C1423550457': 972,\n  'C1423769609': 973,\n  'C1423847048': 974,\n  'C1423956695': 975,\n  'C1423983240': 976,\n  'C1423992712': 977,\n  'C1424082571': 978,\n  'C1424731356': 979,\n  'C1425435004': 980,\n  'C142585591': 981,\n  'C1426317832': 982,\n  'C1426442542': 983,\n  'C1426855526': 984,\n  'C1426908073': 985,\n  'C1427068633': 986,\n  'C1427825258': 987,\n  'C1428171686': 988,\n  'C1428539340': 989,\n  'C1428874286': 990,\n  'C1429167598': 991,\n  'C1429623854': 992,\n  'C1430654561': 993,\n  'C1431689012': 994,\n  'C1432083914': 995,\n  'C1432387181': 996,\n  'C1432469355': 997,\n  'C1433166426': 998,\n  'C1433313209': 999,\n  ...},\n 'accountType': {'DOMESTIC': 0, 'FOREIGN': 1}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [\"0.0\"],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "#to_predict_df = df[df.isFraud == 1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = encoder.transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df = df[df.isFraud == 1]\n",
    "#to_predict_df = df[df.isFraud == 1]\n",
    "cat_feat = [i for i in pre_df.columns if pre_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    pre_df[i] = encoder.transform(pre_df[i])\n",
    "\n",
    "model.predict(pre_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "\n",
    "logging.warning(\"----------\")\n",
    "logging.warning(\"MODEL CREATION STAGE\")\n",
    "\n",
    "logging.warning(\"Reading Final Dataset...\")\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(bucket_id)\n",
    "\n",
    "dataMat = pd.read_csv(\"gs://jesusarguelles-datasets-public/money_laundering_detection/final_data.csv\")\n",
    "data = dataMat.to_numpy()\n",
    "\n",
    "logging.warning(\"Read Final Dataset\")\n",
    "\n",
    "logging.warning(\"Checking Categorical Features...\")\n",
    "\n",
    "cat_feat = [i for i in dataMat.columns if dataMat[i].dtypes == 'O']\n",
    "\n",
    "logging.warning(\"Checking Missing Values...\")\n",
    "\n",
    "a = dict(dataMat.isnull().sum())\n",
    "b = [[i, a[i]] for i in a.keys()]\n",
    "missing = pd.DataFrame(b, columns=['features', 'null_values_count'])\n",
    "\n",
    "logging.warning(\"Storing Missing Values...\")\n",
    "\n",
    "missing.to_csv(\"missing_values.csv\", index=False)\n",
    "\n",
    "logging.warning(\"Storing Missing Values Done\")\n",
    "\n",
    "logging.warning(\"Encoding Categorical Features...\")\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "print(\"------\")\n",
    "print(cat_feat)\n",
    "print(\"------\")\n",
    "\n",
    "label_encoders = {}\n",
    "label_mappings = {}\n",
    "\n",
    "for i in cat_feat:\n",
    "    encoder.fit(dataMat[i])\n",
    "    dataMat[i] = encoder.transform(dataMat[i])\n",
    "\n",
    "    label_encoders[i] = encoder\n",
    "    label_mappings[i] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "\n",
    "logging.warning(\"Features Encoding Done\")\n",
    "\n",
    "logging.warning(\"Creating X and y variables ...\")\n",
    "\n",
    "X = dataMat.iloc[:, :-1]\n",
    "y = dataMat['isFraud']\n",
    "\n",
    "logging.warning(f\"Shape of X: {X.shape} and Shape of y: {y.shape}\")\n",
    "\n",
    "logging.warning(\"Splitting Dataset...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "logging.warning(\"Instantiating Model...\")\n",
    "\n",
    "model = CatBoostClassifier(random_state=42, class_weights={0:1, 1:12}, silent=True)\n",
    "\n",
    "logging.warning(\"Fitting Model...\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_cat = model.predict(X_test)\n",
    "\n",
    "\n",
    "logging.warning(\"Model Metrics Stored\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = {\n",
    "    \"segment\": [0],\n",
    "    \"step\": [2],\n",
    "    \"trans_type\": [\"WIRE_OUT\"],\n",
    "    \"amount\": [18627.02],\n",
    "    \"nameOrig\": [\"C1375503918\"],\n",
    "    \"oldbalanceOrg\": [18627.02],\n",
    "    \"nameDest\": [\"C234430897\"],\n",
    "    \"oldbalanceDest\": [0.0],\n",
    "    \"accountType\": [\"FOREIGN\"]\n",
    "}\n",
    "\n",
    "to_predict_df = pd.DataFrame(predict)\n",
    "#to_predict_df = df[df.isFraud == 1]\n",
    "cat_feat = [i for i in to_predict_df.columns if to_predict_df[i].dtypes == 'O']\n",
    "\n",
    "for i in cat_feat:\n",
    "    to_predict_df[i] = to_predict_df[i].map(label_mappings[i])\n",
    "    #to_predict_df[i] = encoder.transform(to_predict_df[i])\n",
    "\n",
    "model.predict(to_predict_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
